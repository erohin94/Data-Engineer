# Про HDFS

Это распределенная файловая система.

Например у нас есть excel файл на компьютере, который хранится в какой то папке на рабочем столе.

В мире больших данных, один файл делят на несколько блоков.

![image](https://github.com/user-attachments/assets/6e8e6402-0023-4fb6-81a7-7186dc9ccaae)

![image](https://github.com/user-attachments/assets/e14d931f-9de4-47f6-a44f-d76d4b5d6cac)

Размер блока в HDFS можно сделать любым, в основном делят на блоки по 128 мб. 

То есть у нас есть HDD диск, с красной магнитной головкой. Например мы пишем на диск файл размером 100мб одним блоком. Диск напишет одну длинную непрерывную дорожку. 

И когда будем считывать этот файл, то эта магнитная головка найдет эту дорожку с файлом и будет считывать эту дорожку с файлом непрерывно за 1 сек.

Если поделить файл на несколько блоков, то магнитная головка будет бегать с одной дорожки на другую. В результате чего чтение будет происходить за 2 сек. И время затраченное на перемещение магнитной головки будет иметь накопительный эффект.

Поэтому делить файл на большое количество блоков не разумно. Точно так же не разумно оставлять один блок, так как не всегда читаем весь файл целиком. Поэтому оптимально размер блока делать 128 мб.

![image](https://github.com/user-attachments/assets/b8944f41-0654-4b0f-a608-0f02c4e4a6fd)

**Как делить на блоки?**

![image](https://github.com/user-attachments/assets/46040598-5ae7-4d98-af5e-17d3c1f9481b)

![image](https://github.com/user-attachments/assets/f95b95cf-8534-411e-9176-f95b536253e8)

Если файл меньше размера блока HDFS, то блок записывается размером как сам файл.

![image](https://github.com/user-attachments/assets/e56350f7-2464-45f9-bac4-6fb759ee87f3)

**Репликация**

Например, хотим загрузить файл размером 150 Мбайт.

Будем учитывать, что по умолчанию в нашей сборке HDFS стоит фактор репликации - 3,  а размер каждого блока 64 Мб.

1. Разделим наш файл 150 Мбайт на размер блока 64 Мб = получится 2.34375 файлов. Так как количество файлов не может быть неровным, то получится что 2 файла будут с одинаковыми размерами в 64 Мб, а третий файл будет остатком от деления, то есть 22 Мбайт.

2. Исходя из того, что файл был разделен на 3 части, очевидно, что необходимо иметь хотя бы 3 дата ноды. Таким образом 2 файла по 64 Мбайт будут загружены на НОДУ1 и НОДУ2, а на НОДУ3 положится файл размером 22 МБайт.

3. Не будем забывать о том, что HDFS - отказоустойчивая система, и, в случае выхода одной дата ноды, информация все равно должна быть доступна на другой ноде. То есть сейчас оперируем 3 нодами, 3 кусочками файла. То есть схема будет выглядеть следующим образом:

![image](https://github.com/user-attachments/assets/b8ec00f6-ecda-42ae-adec-0af485e23704)

В файловой системе в терминале это выглядит вот так:

![image](https://github.com/user-attachments/assets/871e4473-1c5a-4e32-8de8-3a120c6cec11)

**Архитектура Hadoop Distributed File System (HDFS)**

Архитектура Hadoop Distributed File System (HDFS) состоит из нескольких ключевых компонентов, которые работают вместе для обеспечения распределенного хранения и обработки данных. Вот основные компоненты и принципы архитектуры HDFS:

1. NameNode (узел имени):

NameNode является главным узлом метаданных в HDFS.

Он хранит информацию о том, где расположены блоки данных в файловой системе, и о состоянии файлов и каталогов.

NameNode также отслеживает работу DataNode и координирует операции записи, чтения и удаления файлов.

Вся метаинформация о файлах и блоках данных хранится в оперативной памяти NameNode и, поэтому, она должна быть достаточно емкой для обработки больших объемов данных.

2. DataNode (узел данных):

DataNode представляет собой узел хранения фактических блоков данных.

Он хранит блоки данных на локальных дисках и отвечает за чтение и запись данных по запросу NameNode и клиентов.

DataNode периодически отправляет отчеты о своем состоянии NameNode, сообщая ему о доступности блоков данных.

3. Secondary NameNode (вторичный узел имени):

Secondary NameNode предназначен для обслуживания NameNode и выполнения операций по резервному копированию и слиянию журналов транзакций.

Вторичный узел имени не является резервным узлом для NameNode. Он просто помогает восстановлению системы после сбоев и выполняет процедуры слияния и архивирования, чтобы уменьшить время восстановления в случае необходимости.

4. Client (клиент):
   
Клиенты представляют собой приложения, которые используют HDFS для чтения, записи и обработки данных.

Клиенты отправляют запросы на операции файловой системы (например, чтение файла) к NameNode и затем взаимодействуют с DataNode для фактического доступа к данным.

5. Block (блок):
   
Данные в HDFS разделены на блоки фиксированного размера (обычно 64 МБ по умолчанию).

Блоки данных реплицируются на разные узлы данных для обеспечения отказоустойчивости и достижения высокой доступности данных.

![image](https://github.com/user-attachments/assets/fbe87fd5-279f-4a2e-b8e8-cd7da6e61f89)

![image](https://github.com/user-attachments/assets/9c567b64-dea2-464f-98c2-483baf6019aa)

**Простыми словами:**

NameNode напрямую к DataNode никогда не обращается, все идет через ClientNode.

![image](https://github.com/user-attachments/assets/55531b6c-3d1a-4358-bd7d-98b36b4b1802)

*На клиентской ноде хотим прочитать файл.*

Обращаемся к NameNode. 

NameNode смотрит в свой справочник и отдает адреса для всех реплик сразу, то есть говорит на на первой DataNode, второй DataNode, третьей DataNode такие то блоки от такого то файла которые тебе нужны. Вот держи апишник "шкафчика-стойки-сервака".

ClientNode после того как получила список идет к DataNode и забирает данные с дисков.

![image](https://github.com/user-attachments/assets/62ad90fb-391b-4a44-b0b5-f88db2233f55)

*Запись происходит немного по другому*

Обращаемся к NameNode. Говорим хотим записать файлик размером гигабайт, фактор репликации такой то, размер блока такой то. Приходим на NameNode. NameNode смотрит где есть оптимально близкое расположение к ClientNode, чтобы не писать в соседний город в Дата Центр, ищет место максимально близкое с ClientNode, например соседний сервер. NameNode расчитывает и выдает список адресов куда можно записать блок. ClientNode получает эту инфу(метаданные) и дальше записывает только первую реплику блока. Первую реплику записали, далее ClientNode идет в NameNode и говорит что все ок. Далее DataNode сами реплицируют все оставшиеся блоки и отсылают всю инфу в NameNode через ClientNode.

![image](https://github.com/user-attachments/assets/5d554057-59b3-4cc0-8f6a-2a2eaaa8376a)

В профессиональном сообществе часто используют термин «узел», или нода (от англ. node — «узел»). Эти понятия идентичны и обозначают серверный компьютер с вычислительной мощностью, которую можно использовать для выполнения полезной работы.

Много узлов — нод, компьютеров — можно связать в сеть, которую называют кластер.

Классический кластер в дата-центре выглядит как высокая стойка с большим количеством серверных компьютеров — узлов.

Кластер намного мощнее одиночного компьютера. Соответственно, может выполнять более сложные задачи, например обрабатывать массивы информации и файлы, которые не помещаются в оперативную память одиночного компьютера.

В кластере (группе компьютеров, соединённых в единую сеть), на котором выполняются вычисления, есть два вида узлов, то есть серверов, компьютеров:

Главный узел, который также называют master node или master.

Рабочий узел, он же worker node, или просто worker.

# Установка HDFS через Docker

**Склонировать проект:**

`git clone https://github.com/big-data-europe/docker-hive.git`

![image](https://github.com/user-attachments/assets/68d67b3e-174d-4693-af64-0cef1fbbab1c)

**Перейти в папку с проектом:**

`cd docker-hive`

![image](https://github.com/user-attachments/assets/ff6600cd-4d78-4bd7-a6b3-9156505b13aa)

**Запустить Docker образ:**

`docker-compose up -d`

После успешного запуска, получим сообщение:

![image](https://github.com/user-attachments/assets/5de27163-1df4-4d8e-9e03-2b804689e7e3)

Далее необходимо залезть в контейнер и поработать с hdfs.

Посмотрим список процессов командой `docker ps`:

![image](https://github.com/user-attachments/assets/1b790337-9029-4fe2-8264-12087cba8a60)

Далее ищем контейнер с названием docker-hive-namenode-1. Вводим следующую команду, чтобы туда провалиться:

`docker exec -it docker-hive-namenode-1 /bin/bash`

![image](https://github.com/user-attachments/assets/dea99366-f130-49ef-a1b5-a6769a1f8427)

![image](https://github.com/user-attachments/assets/dfe55daf-d23c-45a7-9fe3-1cb1968eaf8f)

Теперь мы в HDFS. Чтобы проверить это, вводим команду:

`hadoop version`

![image](https://github.com/user-attachments/assets/a4152183-b6c0-478e-bb95-2a043d0867b1)

Так же если ввести команду:

`hdfs dfsadmin -report`

Увидим:

![image](https://github.com/user-attachments/assets/086259dd-5a27-4f5e-a82c-6302f59b3c35)

Команда `hdfs dfsadmin -report` используется в Hadoop для получения сводной информации о состоянии кластера HDFS (Hadoop Distributed File System). Она позволяет администраторам системы видеть статистику и состояние файловой системы.

# Команды 

`hdfs dfs` может применяться только в HDFS, а команды `hadoop fs` — это универсальная команда файловой системы. Она может работать как с HDFS, так и с другими файловыми системами, поддерживаемыми Hadoop (например, локальной файловой системой или S3).

**Команды**

Создать папки:

`hdfs dfs -mkdir /test1`

![image](https://github.com/user-attachments/assets/c9a40bb5-173a-4e9d-adbc-ec530c246df1)

Посмотреть что создали:

`hdfs dfs -ls /`

![image](https://github.com/user-attachments/assets/7daa56e9-f9e6-4fdb-8ede-6203ab1e152a)

Команды в HDFS предназначены для выполнения одной операции за один раз (например, ls, put, get). Эти команды подключаются к HDFS, выполняют операцию и завершаются. 

В HDFS нет команды, аналогичной touch в локальных файловых системах, для создания пустого файла. В HDFS нельзя просто создать пустой файл, как это делается с помощью touch в Unix-подобных системах. HDFS ориентирован на хранение больших объемов данных, и концепция пустых файлов не так важна для распределенной файловой системы.

Но, поскольку HDFS у нас располагается в Docker, то алгоритм будет выглядеть следующим образом:

-Создать файл локально (на своем рабочем компьютере)

-Перенести его в файловую систему докера (ничего перезапускать не нужно будет)

-Перенести из файловой системы докера в HDFS

Создадим рандомный файл в файловой системе и заполним его любой информацией. Сделать это очень просто и можно использовать UI. В моем случае это будет VsCode.

![image](https://github.com/user-attachments/assets/e3e07f48-19b1-4bfb-9e0b-13ce2062073b)

Открыть еще один терминал и ввести команду (В первом терминале запущен HDFS):

Сначала перейти в папку с проектом `cd Desktop\hdfs`

Посмотерть контейнеры: `docker ps`

Ввести команду: `docker cp localfile.txt ed770a67d0c6:/tmp/localfile.txt`

Откуда я взял ed770a67d0c6? Это нейм нода Hadoop.

![image](https://github.com/user-attachments/assets/f3c5b102-56e0-481a-8a1c-f9007ad4bcef)

Вернемся в другой терминал в котором открыт HDFS. 

Вводим следующую команду. Команда `put` копирует файл или директорию из локальной файловой системы в HDFS. Она похожа на команду `cp` в Unix-подобных системах, но работает между локальной файловой системой и HDFS.

`hdfs dfs -put /tmp/localfile.txt /test1/localfile.txt`

![image](https://github.com/user-attachments/assets/c2774927-8a55-41d0-8806-518b917cbfe4)

Проверим, а действительно ли файл оказался в HDFS:

`hdfs dfs -ls /test1/`

![image](https://github.com/user-attachments/assets/34f9e0c1-2691-4f7d-9b4a-8065fa6d20c6)

Размер не нулевой, это уже радует. Как посмотреть содержимое файла? 

Используем cat и указываем тот файл, который хотели бы посмотреть.

`hdfs dfs -cat /test1/localfile.txt`

![image](https://github.com/user-attachments/assets/625ae5b9-310b-4c53-bb27-b7d47fafc75a)

*Чтобы не вбивать команды в ручную а копировать и вставлять в терминал, использовать сочетание клавиш CTRL+SHIFT+V*

Ровно также, как и в Linux файл можно перемещать и копировать. Например введем следующую команду.

Чтобы выйти из строки `Тестовый текст для HDFSroot@ed770a67d0c6:/#` Нажать ENTER и вводить команды ниже:

`hdfs dfs -cp /test1/localfile.txt /test1/localfile_copy.txt`

`hdfs dfs -ls /test1/`

![image](https://github.com/user-attachments/assets/279875dc-822a-4b0c-88bf-4900728975c3)

Ровно также будут работать и `mv`, как с точки зрения перемещения, так и с точки зрения переименовывания. Видим, что благодаря примеру, ниже файл поменял свое название.

`hdfs dfs -mv /test1/localfile.txt /test1/localfile_renamed.txt`

`hdfs dfs -ls /test1/`

![image](https://github.com/user-attachments/assets/3e264e26-8839-4ecb-b8a1-f9a8d5907faf)

Аналогично Linux будет работать и команда `rm`: 

`hdfs dfs -rm /test1/localfile_copy.txt`

![image](https://github.com/user-attachments/assets/9bceac92-b86e-43a7-a550-d9e7f14ba43d)

![image](https://github.com/user-attachments/assets/723a212d-d9ca-48d0-b102-5b12394248b5)

Но... Что с репликами? Познакомимся с очень важной командой в HDFS - `stat`. Команда `hdfs dfs -stat` в HDFS используется для получения информации о файле или директории. Выглядит вот так в общем случае:

`hdfs dfs -stat [формат] [путь_к_файлу]`

`%b` — размер файла в байтах.

`%y` — время последней модификации файла (формат даты).

`%n` — имя файла.

`%o` — права доступа к файлу (в формате rwx).

`%r` — количество реплик файла.

`%u` — владелец файла.

`%g` — группа файла.

Ставятся в формате, как в примере ниже. В данном случае у нас 3 реплики файла (такая конфигурация), а также пользователь - root.

`hdfs dfs -stat "%r %u" /test1/localfile_renamed.txt`

![image](https://github.com/user-attachments/assets/70313c51-7193-4cfa-bfe6-b06a5a250722)

Можно ли изменять количество реплик? Можно. Но не нужно. Потому что HDFS неспроста называют отказоустойчивой. Ведь благодаря фактору репликации мы можем делать копии файла. Но, поскольку команда такая есть - мы на нее смотрим:

`hdfs dfs -setrep 1 /test1/localfile_renamed.txt`

`hdfs dfs -stat "%r %u" /test1/localfile_renamed.txt`

![image](https://github.com/user-attachments/assets/9b81b9e8-5f3e-4b65-9bec-bf7f80f5ced6)

Команда `hdfs dfsadmin -report` в HDFS используется для получения отчёта о состоянии распределённой файловой системы Hadoop (HDFS). Она выводит информацию о всём кластере HDFS, включая статистику использования пространства, состояние DataNode, а также метаданные о блоках и репликации.

![image](https://github.com/user-attachments/assets/b650486c-b164-4aa3-9663-e8b3a6e75f45)

Cверху вниз пояснение.

Configured Capacity — общая ёмкость всех DataNode, доступная в HDFS (сумма всех дисковых пространств).

Present Capacity — доступная ёмкость HDFS с учетом зарезервированного пространства.

DFS Remaining — оставшееся свободное пространство на всех DataNode.

DFS Used — пространство, которое используется для хранения данных в HDFS.

DFS Used% — процент использования пространства в HDFS.

Under replicated blocks — количество блоков, у которых недостаточно реплик.

Blocks with corrupt replicas — количество блоков с повреждёнными репликами.

Missing blocks — количество блоков, которые отсутствуют в кластере (потерянные данные).

Live datanodes — количество активных (доступных) DataNode.

Dead datanodes — количество DataNode, которые не в сети или не отвечают.

Name и Hostname — IP-адрес и имя хоста DataNode.

Decommission Status — статус DataNode (например, Normal или Decommissioned, если DataNode выведен из эксплуатации).

Configured Capacity, DFS Used, DFS Remaining — ёмкость, используемое и оставшееся пространство на DataNode.

DFS Used%, DFS Remaining% — процентное использование и оставшееся пространство.

Last contact — время последнего контакта с DataNode.














