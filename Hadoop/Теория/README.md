# Содержание

-[Архитектура экосистемы Hadoop](#Архитектура-экосистемы-Hadoop)

-[Архитектура HDFS](#Архитектура-HDFS)

**Hadoop** — это популярная платформа с открытым исходным кодом, которая используется для хранения и обработки больших объемов данных.
Она была разработана для того, чтобы работать на кластерах серверов, обеспечивая эффективное распределение и обработку данных между множеством машин. 
Hadoop играет ключевую роль в экосистеме Big Data и используется для анализа огромных наборов данных в различных областях, таких как анализ логов, машинное обучение и бизнес-аналитика.

<img width="2000" height="473" alt="image" src="https://github.com/user-attachments/assets/f0fa8bbd-6ef1-4a36-a691-e00a8c0598d3" />


## Архитектура экосистемы Hadoop

**Экосистема состоит из четырёх ключевых компонентов:** `HDFS`, `YARN`, `MapReduce` и `Common`. 
В дополнение к ним выпущено несколько десятков инструментов, используемых для расширения функциональности платформы.

**1. Hadoop Distributed File System (HDFS):**

HDFS — это распределённая файловая система, которая позволяет хранить большие объемы данных на множестве машин. 
Она разбивает данные на блоки и распределяет их между узлами кластера, обеспечивая как высокую надёжность (данные дублируются), так и масштабируемость.

**2. MapReduce:**

Это фреймворк, программная модель для распределённой обработки данных хранящихся в `HDFS`. `MapReduce` состоит из двух этапов:
 
 - **Map:** данные разбиваются на небольшие части и обрабатываются параллельно.

 - **Reduce:** результаты обработки собираются и объединяются.

**3. YARN (Yet Another Resource Negotiator):**

YARN — это система управления ресурсами Hadoop. Она распределяет ресурсы между различными приложениями и задачами, что делает Hadoop более гибким и способным поддерживать несколько рабочих нагрузок одновременно.

YARN позволяет разным приложениям (не только MapReduce) использовать ресурсы Hadoop.

**4. Hadoop Common:**

Это набор общих библиотек и утилит, которые поддерживают работу всех компонентов Hadoop. Они обеспечивают взаимодействие между различными частями системы.

<p align="center"><img width="500" height="490" alt="Без названия" src="https://github.com/user-attachments/assets/4cf69dee-62dc-400a-ac2d-c48d09c9c464" /></p>

## Архитектура HDFS

**«Узел», или нода (от англ. node — «узел»)** - эти понятия идентичны и обозначают серверный компьютер с вычислительной мощностью, которую можно использовать для выполнения полезной работы.

Много узлов — нод, компьютеров — можно связать в сеть, которую называют **кластер**.

Классический кластер в дата-центре выглядит как высокая стойка с большим количеством серверных компьютеров — узлов.

Кластер намного мощнее одиночного компьютера. Соответственно, может выполнять более сложные задачи, например обрабатывать массивы информации и файлы, которые не помещаются в оперативную память одиночного компьютера.

В кластере (группе компьютеров, соединённых в единую сеть), на котором выполняются вычисления, есть два вида узлов, то есть серверов, компьютеров:

- **Главный узел**, который также называют `master node` или `master`.
- **Рабочий узел**, он же `worker node`, или просто `worker`.

**Архитектура `Hadoop Distributed File System (HDFS)`** состоит из нескольких ключевых компонентов, которые работают вместе для обеспечения распределенного хранения и обработки данных в кластере `Hadoop`. Вот основные компоненты и принципы архитектуры `HDFS`:

**1. NameNode (узел имени):**

- `NameNode` является главным узлом метаданных в `HDFS`.
- Не хранит сами данные (только карта).
- Он хранит информацию о том, где расположены блоки данных в файловой системе, и о состоянии файлов и каталогов.
- 'NameNode' также отслеживает работу 'DataNode' и координирует операции записи, чтения и удаления файлов.
- Вся метаинформация о файлах и блоках данных хранится в оперативной памяти 'NameNode` и, поэтому, она должна быть достаточно емкой для обработки больших объемов данных.
- `NameNode` напрямую к `DataNode` никогда не обращается, все идет через `ClientNode`.
- 
**2. DataNode (узел данных):**

- `DataNode` представляет собой узел хранения фактических блоков данных.
- Каждый блок обычно имеет несколько копий (репликация, стандартно = 3).
- Он хранит блоки данных на локальных дисках и отвечает за чтение и запись данных по запросу `NameNode` и клиентов.
- `DataNode` периодически отправляет отчеты о своем состоянии `NameNode`, сообщая ему о доступности блоков данных.

**3. Secondary NameNode (вторичный узел имени):**

- `Secondary NameNode` предназначен для обслуживания `NameNode` и выполнения операций по резервному копированию и слиянию журналов транзакций.
- Вторичный узел имени не является резервным узлом для `NameNode`. Он просто помогает восстановлению системы после сбоев и выполняет процедуры слияния и архивирования, чтобы уменьшить время восстановления в случае необходимости.
- Вторичный узел — как и `NameNode`, это отдельный компьютер в кластере. Вторичный узел копирует образ `HDFS` и лог транзакций (операций с файловыми блоками) во временную папку, применяет изменения, накопленные в логе транзакций к образу `HDFS`, а также записывает его на узел `NameNode` и очищает лог транзакций. `Secondary NameNode` необходим, чтобы вручную быстро восстановить `NameNode` в случае его выхода из строя.

**4. Client (клиент):**

- Клиенты представляют собой приложения, которые используют `HDFS` для чтения, записи и обработки данных.
- Клиенты отправляют запросы на операции файловой системы (например, чтение файла) к `NameNode` и затем взаимодействуют с `DataNode` для фактического доступа к данным.

**5. Block (блок):**

- Данные в `HDFS` разделены на блоки фиксированного размера (обычно 64 МБ по умолчанию).
- Блоки данных реплицируются на разные узлы данных для обеспечения отказоустойчивости и достижения высокой доступности данных.

**Кратко:**

Сервер имён, или *NameNode*, оперирует всей метаинформацией о хранимых данных.

*Secondary NameNode* - Вторичный узел помогает восстановить *NameNode*.

*DataNode* - Именно этот узел создаёт, копирует и удаляет блоки, обрабатывает запросы на чтение и запись, а также сообщает *NameNode* о своём состоянии.

Клиент хочет прочитать файл. Обращается к `NameNode` → получает список `DataNode`, где лежат блоки.

Клиент читает блоки напрямую с `DataNode` (минует `NameNode`, чтобы не перегружать его).

Если `DataNode` падает → клиент переключается на другую реплику.

<p align="center"><img width="500" height="490" alt="Без названия (1)" src="https://github.com/user-attachments/assets/b0116f10-f306-406f-b0d2-0add9e8aeb43" /></p>

**Пример**

Например у нас есть excel файл на компьютере, который хранится в какой то папке на рабочем столе. В мире больших данных, один файл делят на несколько блоков.
Размер блока в HDFS можно сделать любым, в основном делят на блоки по 128 мб.

<p align="center"><img width="600" height="300" alt="image" src="https://github.com/user-attachments/assets/2ebed442-52fe-4680-8019-aa121137e23f" /></p>

То есть у нас есть HDD диск, с красной магнитной головкой. Например мы пишем на диск файл размером 100мб одним блоком. Диск напишет одну длинную непрерывную дорожку.
И когда будем считывать этот файл, то эта магнитная головка найдет эту дорожку с файлом и будет считывать эту дорожку с файлом непрерывно за 1 сек.
Если поделить файл на несколько блоков, то магнитная головка будет бегать с одной дорожки на другую. В результате чего чтение будет происходить за 2 сек. И время затраченное на перемещение магнитной головки будет иметь накопительный эффект. Поэтому делить файл на большое количество блоков не разумно. Точно так же не разумно оставлять один блок, так как не всегда читаем весь файл целиком. 
Поэтому оптимально размер блока делать 128 мб.

<p align="center"><img width="600" height="300" alt="image" src="https://github.com/user-attachments/assets/9ce7510e-a716-4fb0-9103-ac8d1445033e" /></p>

**Как делить на блоки?**

Если файл меньше размера блока HDFS, то блок записывается размером как сам файл.

<p align="center"><img width="730" height="769" alt="image" src="https://github.com/user-attachments/assets/db7c7f16-9e22-4e08-9287-f09113582f7e" /></p>

**Пример добавления нового файла в HDFS.**

Например, хотим загрузить файл размером `150 Мбайт`.

Будем учитывать, что по умолчанию в нашей сборке `HDFS` стоит фактор репликации - `3`,  а размер каждого блока `64 Мб`.

1. Разделим наш файл `150 Мбайт` на размер блока `64 Мб` = получится `2.34375` файлов. Так как количество файлов не может быть неровным, то получится что 2 файла будут с одинаковыми размерами в `64 Мб`, а третий файл будет остатком от деления, то есть `22 Мбайт`.

2. Исходя из того, что файл был разделен на 3 части, очевидно, что необходимо иметь хотя бы 3 дата ноды. Таким образом 2 файла по `64 Мбайт` будут загружены на `НОДУ1` и `НОДУ2`, а на `НОДУ3` положится файл размером `22 МБайт`.

3. Не будем забывать о том, что `HDFS` - отказоустойчивая система, и, в случае выхода одной дата ноды, информация все равно должна быть доступна на другой ноде. То есть сейчас оперируем 3 нодами, 3 кусочками файла. То есть схема будет выглядеть следующим образом:

<p align="center"><img width="609" height="189" alt="image" src="https://github.com/user-attachments/assets/f1a65ca9-61cf-4279-8440-e5e771308164" /></p>

**В файловой системе (в терминале) файлы будут лежать так:**

```
/user/your_username/example.txt - метаданные файла, включая информацию о блоках.
/user/your_username/example.txt/_block_0 - первый блок размером 64 МБ.
/user/your_username/example.txt/_block_1 - второй блок размером 64 МБ.
/user/your_username/example.txt/_block_2 - третий блок размером 22 МБ.
```

**Но не так :**

```
/user/your_username/copy_example.txt - метаданные файла, включая информацию о блоках.
/user/your_username/example.txt/_block_0 - первый блок размером 64 МБ.
/user/your_username/example.txt/_block_1 - второй блок размером 64 МБ.
/user/your_username/example.txt/_block_2 - третий блок размером 22 МБ.
/user/your_username/copy_example.txt - метаданные файла, включая информацию о блоках.
/user/your_username/example.txt/_block_0_copy - первый блок размером 64 МБ.
/user/your_username/example.txt/_block_1_copy - второй блок размером 64 МБ.
/user/your_username/example.txt/_block_2_copy - третий блок размером 22 МБ.
/user/your_username/copy_example.txt - метаданные файла, включая информацию о блоках.
/user/your_username/example.txt/_block_0_copy - первый блок размером 64 МБ.
/user/your_username/example.txt/_block_1_copy - второй блок размером 64 МБ.
/user/your_username/example.txt/_block_2_copy - третий блок размером 22 МБ.
```
Потому что репликация - это внутреннее устройство `HDFS` и нагружать `UI` интерфейс бессмысленно. В случае удаления части файла из `HDFS` - он автоматически заберет его копию с другой ноды и отразит в `UI`. И так будет происходить ровно столько раз, сколько стоит фактор репликации. В случае удаления метаинформации о файле, удалится весь хранимый файл и его части!

Таким образом, `HDFS` будет создавать еще 2 копии наших файлов, но на других узлах. Чтобы в случае выхода из строя какой-либо ноды, все равно можно будет получить данные. 

Теперь давайте рассмотрим более сложный пример, с файлом размером `1380 Мб`. Пройдем по тому же алгоритму и сразу же появятся подводные камни.

1. Также разделим его на блоки в `64 Мб`, получим 21 файлик по `64 Мб` и один `36 Мб`. 

Исходя из этого получаем 22 ноды, что выглядит неразумно. Ведь так как поведение namenode очень похоже на поведение матери в реальной жизни, то проведем аналогию, что чем больше детей, тем сложнее за ними следить.

Получается, что 22 ноды создавать нелогично, тем более если остальные файлы у нас будут маленькими, а оставшиеся ноды будут простаивать. Таким образом, необходимо увеличивать размер хранимого блока до `128 Мб` или до `256 Мб`. Решение о том, какой размер блока лучше ставить, принимается исходя из размеров хранимых файлов и частоты обращения к ним. 

Внимание! Фактор репликации всегда должен быть минимум 2, не отключайте его, так как файлы могут потеряться. Более высокую цифру необходимо ставить только при ситуации, если у вас есть свободное место. 
