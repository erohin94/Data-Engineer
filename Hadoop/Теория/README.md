# Содержание

-[Архитектура экосистемы Hadoop](#Архитектура-экосистемы-Hadoop)

**Hadoop** — это популярная платформа с открытым исходным кодом, которая используется для хранения и обработки больших объемов данных.
Она была разработана для того, чтобы работать на кластерах серверов, обеспечивая эффективное распределение и обработку данных между множеством машин. 
Hadoop играет ключевую роль в экосистеме Big Data и используется для анализа огромных наборов данных в различных областях, таких как анализ логов, машинное обучение и бизнес-аналитика.

<img width="2000" height="473" alt="image" src="https://github.com/user-attachments/assets/f0fa8bbd-6ef1-4a36-a691-e00a8c0598d3" />


## Архитектура экосистемы Hadoop

**Экосистема состоит из четырёх ключевых компонентов:** `HDFS`, `YARN`, `MapReduce` и `Common`. 
В дополнение к ним выпущено несколько десятков инструментов, используемых для расширения функциональности платформы.

**1. Hadoop Distributed File System (HDFS):**

HDFS — это распределённая файловая система, которая позволяет хранить большие объемы данных на множестве машин. 
Она разбивает данные на блоки и распределяет их между узлами кластера, обеспечивая как высокую надёжность (данные дублируются), так и масштабируемость.

**2. MapReduce:**

Это программная модель для распределённой обработки данных. `MapReduce` состоит из двух этапов:
 
 - **Map:** данные разбиваются на небольшие части и обрабатываются параллельно.

 - **Reduce:** результаты обработки собираются и объединяются.

**3. YARN (Yet Another Resource Negotiator):**

YARN — это система управления ресурсами Hadoop. Она распределяет ресурсы между различными приложениями и задачами, что делает Hadoop более гибким и способным поддерживать несколько рабочих нагрузок одновременно.

YARN позволяет разным приложениям (не только MapReduce) использовать ресурсы Hadoop.

**4. Hadoop Common:**

Это набор общих библиотек и утилит, которые поддерживают работу всех компонентов Hadoop. Они обеспечивают взаимодействие между различными частями системы.
