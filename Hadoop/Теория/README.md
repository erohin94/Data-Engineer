# Содержание

-[Архитектура экосистемы Hadoop](#Архитектура-экосистемы-Hadoop)

-[Архитектура HDFS](#Архитектура-HDFS)

-[Пример блочного деления HDFS](#Пример-блочного-деления-HDFS)

-[Пример добавления нового файла в HDFS](#Пример-добавления-нового-файла-в-HDFS)

-[Пример выгрузки файла из хранилища](#Пример-выгрузки-файла-из-хранилища)

-[Основные форматы файлов для хранения](#Основные-форматы-файлов-для-хранения)

**Hadoop** — это популярная платформа с открытым исходным кодом, которая используется для хранения и обработки больших объемов данных.
Она была разработана для того, чтобы работать на кластерах серверов, обеспечивая эффективное распределение и обработку данных между множеством машин. 
Hadoop играет ключевую роль в экосистеме Big Data и используется для анализа огромных наборов данных в различных областях, таких как анализ логов, машинное обучение и бизнес-аналитика.

<img width="2000" height="473" alt="image" src="https://github.com/user-attachments/assets/f0fa8bbd-6ef1-4a36-a691-e00a8c0598d3" />


## Архитектура экосистемы Hadoop

**Экосистема состоит из четырёх ключевых компонентов:** `HDFS`, `YARN`, `MapReduce` и `Common`. 
В дополнение к ним выпущено несколько десятков инструментов, используемых для расширения функциональности платформы.

**1. Hadoop Distributed File System (HDFS):**

HDFS — это распределённая файловая система, которая позволяет хранить большие объемы данных на множестве машин. 
Она разбивает данные на блоки и распределяет их между узлами кластера, обеспечивая как высокую надёжность (данные дублируются), так и масштабируемость.

**2. MapReduce:**

Это фреймворк, программная модель для распределённой обработки данных хранящихся в `HDFS`. `MapReduce` состоит из двух этапов:
 
 - **Map:** данные разбиваются на небольшие части и обрабатываются параллельно.

 - **Reduce:** результаты обработки собираются и объединяются.

**3. YARN (Yet Another Resource Negotiator):**

YARN — это система управления ресурсами Hadoop. Она распределяет ресурсы между различными приложениями и задачами, что делает Hadoop более гибким и способным поддерживать несколько рабочих нагрузок одновременно.

YARN позволяет разным приложениям (не только MapReduce) использовать ресурсы Hadoop.

**4. Hadoop Common:**

Это набор общих библиотек и утилит, которые поддерживают работу всех компонентов Hadoop. Они обеспечивают взаимодействие между различными частями системы.

<p align="center"><img width="500" height="490" alt="Без названия" src="https://github.com/user-attachments/assets/4cf69dee-62dc-400a-ac2d-c48d09c9c464" /></p>

## Архитектура HDFS

**«Узел», или нода (от англ. node — «узел»)** - эти понятия идентичны и обозначают серверный компьютер с вычислительной мощностью, которую можно использовать для выполнения полезной работы.

Много узлов — нод, компьютеров — можно связать в сеть, которую называют **кластер**.

Классический кластер в дата-центре выглядит как высокая стойка с большим количеством серверных компьютеров — узлов.

Кластер намного мощнее одиночного компьютера. Соответственно, может выполнять более сложные задачи, например обрабатывать массивы информации и файлы, которые не помещаются в оперативную память одиночного компьютера.

В кластере (группе компьютеров, соединённых в единую сеть), на котором выполняются вычисления, есть два вида узлов, то есть серверов, компьютеров:

- **Главный узел**, который также называют `master node` или `master`.
- **Рабочий узел**, он же `worker node`, или просто `worker`.

**Архитектура `Hadoop Distributed File System (HDFS)`** состоит из нескольких ключевых компонентов, которые работают вместе для обеспечения распределенного хранения и обработки данных в кластере `Hadoop`. Вот основные компоненты и принципы архитектуры `HDFS`:

**1. NameNode (узел имени):**

- `NameNode` является главным узлом метаданных в `HDFS`.
- Не хранит сами данные (только карта).
- Он хранит информацию о том, где расположены блоки данных в файловой системе, и о состоянии файлов и каталогов.
- 'NameNode' также отслеживает работу 'DataNode' и координирует операции записи, чтения и удаления файлов.
- Вся метаинформация о файлах и блоках данных хранится в оперативной памяти 'NameNode` и, поэтому, она должна быть достаточно емкой для обработки больших объемов данных.
- `NameNode` напрямую к `DataNode` никогда не обращается, все идет через `ClientNode`.

**2. DataNode (узел данных):**

- `DataNode` представляет собой узел хранения фактических блоков данных.
- Каждый блок обычно имеет несколько копий (репликация, стандартно = 3).
- Он хранит блоки данных на локальных дисках и отвечает за чтение и запись данных по запросу `NameNode` и клиентов.
- `DataNode` периодически отправляет отчеты о своем состоянии `NameNode`, сообщая ему о доступности блоков данных.

**3. Secondary NameNode (вторичный узел имени):**

- `Secondary NameNode` предназначен для обслуживания `NameNode` и выполнения операций по резервному копированию и слиянию журналов транзакций.
- Вторичный узел имени не является резервным узлом для `NameNode`. Он просто помогает восстановлению системы после сбоев и выполняет процедуры слияния и архивирования, чтобы уменьшить время восстановления в случае необходимости.
- Вторичный узел — как и `NameNode`, это отдельный компьютер в кластере. Вторичный узел копирует образ `HDFS` и лог транзакций (операций с файловыми блоками) во временную папку, применяет изменения, накопленные в логе транзакций к образу `HDFS`, а также записывает его на узел `NameNode` и очищает лог транзакций. `Secondary NameNode` необходим, чтобы вручную быстро восстановить `NameNode` в случае его выхода из строя.

**4. Client (клиент):**

- Клиенты представляют собой приложения, которые используют `HDFS` для чтения, записи и обработки данных.
- Клиенты отправляют запросы на операции файловой системы (например, чтение файла) к `NameNode` и затем взаимодействуют с `DataNode` для фактического доступа к данным.

**5. Block (блок):**

- Данные в `HDFS` разделены на блоки фиксированного размера (обычно 64 МБ по умолчанию).
- Блоки данных реплицируются на разные узлы данных для обеспечения отказоустойчивости и достижения высокой доступности данных.

**Кратко:**

Сервер имён, или *NameNode*, оперирует всей метаинформацией о хранимых данных.

*Secondary NameNode* - Вторичный узел помогает восстановить *NameNode*.

*DataNode* - Именно этот узел создаёт, копирует и удаляет блоки, обрабатывает запросы на чтение и запись, а также сообщает *NameNode* о своём состоянии.

Клиент хочет прочитать файл. Обращается к `NameNode` → получает список `DataNode`, где лежат блоки.

Клиент читает блоки напрямую с `DataNode` (минует `NameNode`, чтобы не перегружать его).

Если `DataNode` падает → клиент переключается на другую реплику.

<p align="center"><img width="500" height="490" alt="Без названия (1)" src="https://github.com/user-attachments/assets/b0116f10-f306-406f-b0d2-0add9e8aeb43" /></p>

## Пример блочного деления HDFS

Например у нас есть excel файл на компьютере, который хранится в какой то папке на рабочем столе. В мире больших данных, один файл делят на несколько блоков.
Размер блока в HDFS можно сделать любым, в основном делят на блоки по 128 мб.

<p align="center"><img width="600" height="300" alt="image" src="https://github.com/user-attachments/assets/2ebed442-52fe-4680-8019-aa121137e23f" /></p>

То есть у нас есть HDD диск, с красной магнитной головкой. Например мы пишем на диск файл размером 100мб одним блоком. Диск напишет одну длинную непрерывную дорожку.
И когда будем считывать этот файл, то эта магнитная головка найдет эту дорожку с файлом и будет считывать эту дорожку с файлом непрерывно за 1 сек.
Если поделить файл на несколько блоков, то магнитная головка будет бегать с одной дорожки на другую. В результате чего чтение будет происходить за 2 сек. И время затраченное на перемещение магнитной головки будет иметь накопительный эффект. Поэтому делить файл на большое количество блоков не разумно. Точно так же не разумно оставлять один блок, так как не всегда читаем весь файл целиком. 
Поэтому оптимально размер блока делать 128 мб.

<p align="center"><img width="600" height="300" alt="image" src="https://github.com/user-attachments/assets/9ce7510e-a716-4fb0-9103-ac8d1445033e" /></p>

**Как делить на блоки?**

Если файл меньше размера блока HDFS, то блок записывается размером как сам файл.

<p align="center"><img width="730" height="769" alt="image" src="https://github.com/user-attachments/assets/db7c7f16-9e22-4e08-9287-f09113582f7e" /></p>

Блоки записываются в разные части кластера HDFS, то есть на разные жёсткие диски в DataNode. Каждый из блоков записывается столько раз, сколько указано в настройке, которая называется **фактором репликации**.

**Фактор репликации** — это количество копий файла, которое нужно сделать.

Дефолтное значение фактора репликации равно трём. Это значит, что если файл разбит на N частей, то каждая из них будет записана три раза на разные жёсткие диски DataNode. Чтобы узнать общее количество блоков, которое будет записано на HDFS, нужно количество блоков, на которые разбит файл, умножить на фактор репликации.

## Пример добавления нового файла в HDFS

Например, хотим загрузить файл размером `150 Мбайт`.

Будем учитывать, что по умолчанию в нашей сборке `HDFS` стоит фактор репликации - `3`,  а размер каждого блока `64 Мб`.

1. Разделим наш файл `150 Мбайт` на размер блока `64 Мб` = получится `2.34375` файлов. Так как количество файлов не может быть неровным, то получится что 2 файла будут с одинаковыми размерами в `64 Мб`, а третий файл будет остатком от деления, то есть `22 Мбайт`.

2. Исходя из того, что файл был разделен на 3 части, очевидно, что необходимо иметь хотя бы 3 дата ноды. Таким образом 2 файла по `64 Мбайт` будут загружены на `НОДУ1` и `НОДУ2`, а на `НОДУ3` положится файл размером `22 МБайт`.

3. Не будем забывать о том, что `HDFS` - отказоустойчивая система, и, в случае выхода одной дата ноды, информация все равно должна быть доступна на другой ноде. То есть сейчас оперируем 3 нодами, 3 кусочками файла. То есть схема будет выглядеть следующим образом:

<p align="center"><img width="609" height="189" alt="image" src="https://github.com/user-attachments/assets/f1a65ca9-61cf-4279-8440-e5e771308164" /></p>

**В файловой системе (в терминале) файлы будут лежать так:**

```
/user/your_username/example.txt - метаданные файла, включая информацию о блоках.
/user/your_username/example.txt/_block_0 - первый блок размером 64 МБ.
/user/your_username/example.txt/_block_1 - второй блок размером 64 МБ.
/user/your_username/example.txt/_block_2 - третий блок размером 22 МБ.
```

**Но не так :**

```
/user/your_username/copy_example.txt - метаданные файла, включая информацию о блоках.
/user/your_username/example.txt/_block_0 - первый блок размером 64 МБ.
/user/your_username/example.txt/_block_1 - второй блок размером 64 МБ.
/user/your_username/example.txt/_block_2 - третий блок размером 22 МБ.
/user/your_username/copy_example.txt - метаданные файла, включая информацию о блоках.
/user/your_username/example.txt/_block_0_copy - первый блок размером 64 МБ.
/user/your_username/example.txt/_block_1_copy - второй блок размером 64 МБ.
/user/your_username/example.txt/_block_2_copy - третий блок размером 22 МБ.
/user/your_username/copy_example.txt - метаданные файла, включая информацию о блоках.
/user/your_username/example.txt/_block_0_copy - первый блок размером 64 МБ.
/user/your_username/example.txt/_block_1_copy - второй блок размером 64 МБ.
/user/your_username/example.txt/_block_2_copy - третий блок размером 22 МБ.
```
Потому что репликация - это внутреннее устройство `HDFS` и нагружать `UI` интерфейс бессмысленно. В случае удаления части файла из `HDFS` - он автоматически заберет его копию с другой ноды и отразит в `UI`. И так будет происходить ровно столько раз, сколько стоит фактор репликации. В случае удаления метаинформации о файле, удалится весь хранимый файл и его части!

Таким образом, `HDFS` будет создавать еще 2 копии наших файлов, но на других узлах. Чтобы в случае выхода из строя какой-либо ноды, все равно можно будет получить данные. 

Теперь давайте рассмотрим более сложный пример, с файлом размером `1380 Мб`. Пройдем по тому же алгоритму и сразу же появятся подводные камни.

1. Также разделим его на блоки в `64 Мб`, получим 21 файлик по `64 Мб` и один `36 Мб`. 

Исходя из этого получаем 22 ноды, что выглядит неразумно. Ведь так как поведение namenode очень похоже на поведение матери в реальной жизни, то проведем аналогию, что чем больше детей, тем сложнее за ними следить.

Получается, что 22 ноды создавать нелогично, тем более если остальные файлы у нас будут маленькими, а оставшиеся ноды будут простаивать. Таким образом, необходимо увеличивать размер хранимого блока до `128 Мб` или до `256 Мб`. Решение о том, какой размер блока лучше ставить, принимается исходя из размеров хранимых файлов и частоты обращения к ним. 

Внимание! Фактор репликации всегда должен быть минимум 2, не отключайте его, так как файлы могут потеряться. Более высокую цифру необходимо ставить только при ситуации, если у вас есть свободное место. 

## Пример выгрузки файла из хранилища

Рассмотрим ситуацию, что есть client, который запрашивает данные из нашего хранилища.

Например, файл, который мы загружали ранее, необходимо забрать. Его вес `150 Мб` и он отправился на хранение в 3 дата ноды.

В целом, алгоритм будет выглядеть следующим образом.

- **Инициация запроса на чтение файла:** Пользователь или приложение, запустившие команду чтения файла из HDFS, отправляют запрос на чтение файла.

- **Клиентский запрос к NameNode:** Команда чтения отправляет запрос на NameNode для получения информации о файле, включая список блоков данных и их расположение.

- **Получение информации о блоках данных:** NameNode отвечает клиенту, предоставляя информацию о блоках данных, из которых состоит файл, и их расположение на различных DataNode. В нашем случае это адреса датанод и блоки нашего файла. 

- **Запросы к DataNode:** Клиент отправляет параллельные запросы к каждому DataNode, на котором хранится блок данных, для получения копий этих блоков. Обратите внимание, что NameNode не отправляет запросы, а всего лишь говорят клиенту о том, где лежат блоки данных.

- **Параллельное чтение блоков данных:** Каждый DataNode отвечает на запрос клиента, передавая копии блоков данных через сеть. Эти запросы и ответы обрабатываются параллельно, что ускоряет процесс чтения.

- **Получение блоков данных:** Клиент получает копии всех блоков данных, которые были запрошены, из различных DataNode.

- **Сборка файла:** По мере получения копий всех блоков данных, клиент собирает их в единый файл в локальной файловой системе. Этот процесс может быть организован таким образом, чтобы данные из разных блоков собирались параллельно для ускорения операции.

- **Завершение процесса:** После завершения передачи всех блоков данных и сборки файла, клиент получает уведомление об успешном завершении операции. Файл становится доступным в локальной файловой системе для дальнейшего использования.


В данном разделе необходимо ответить на непростой вопрос - как HDFS, как файловая система, понимает высокоуровневый запрос от клиента? Ведь клиентом чаще всего выступает система или приложение, использующая языки программирования Java, Scala, R, Python.

И ведь действительно, если мы забираем весь файл, который например является документом, то вопросов не возникает. Да и в целом они не возникают, если мы берем уже разделенные блоки файла. Но ведь это не очень удобно. 

Рассмотрим пример - мы храним файл `Excel (45.000 строк)`, который весит `150 Мб`. Все по классике, 3 дата ноды, 3 файла. Но, что если нам необходимо забрать только первые 2 строки. Как послать запрос в HDFS, чтобы взять только эти данные? Неужели придется брать блок `64 Мб` и работать с ним?

И действительно, чтобы выполнить похожий запрос от клиента, необходимо трансформировать код в некую абстракцию, которую поймет HDFS. Этой абстракцией может служить инструмент Hive или Pig, который преобразует код от клиента в SQL запросы. Но ведь, уже зная SQL, возникает вопрос, как HDFS понимает SQL. И тут все более, чем просто. Благодаря Hive или Pig, строятся виртуальные таблицы, основанные на файлах в HDFS.

Таким образом, имеем ввиду, что с HDFS можно работать как через запросы от клиента, так и напрямую через HDFS DFS. Но команды, написанные в HDFS DFS могут вызывать трудности. 

1. Запросы к HDFS сложные, аналитик не сможет работать с таким интерфейсом (в терминале)

2. Чтобы хоть как-то писать запросы к HDFS, необходимо понимать его архитектуру, что опять же вызывает трудности  у всех, кроме дата инженера.

То есть, учитываем, что для задач взять целый файл или его часть - отлично подходит интерфейс терминала с командами HDFS DFS. Если задача более тонкая - нужен трансформатор для преобразования высокоуровневого кода в SQL-подобные запросы. Чаще всего конечно задачи всегда тонкие, потому что как минимум аналитику нужно работать с данными, а через HDFS DFS сделать ему это проблематично.
 
Взятие целого файла, либо его части (указывается в пути) - `hdfs dfs -get /путь_к_файлу_в_HDFS /локальный_путь`

## Основные форматы файлов для хранения

Какие файлы вообще возможно загружать в HDFS. Начнем с самого простого и популярного типа файлов.

**Текстовые файлы (Text Files)**

Текстовые файлы можно хранить в HDFS без изменений, как обычные текстовые документы. Они содержат данные в строковом формате и могут использоваться для хранения логов, CSV-файлов, JSON и XML данных.

Дефолтный csv файл выглядит следующим образом:

```
id,name,age
1,John Doe,29
2,Jane Smith,34
3,Emily White,25
```
               
**Sequence Files**

Sequence File — это бинарный формат, используемый для хранения данных в парах "ключ-значение". Применяется в системах Hadoop для хранения данных в сжатом формате. Sequence файлы менее читаемы для человека, поскольку данные закодированы в бинарном виде.

Каждая запись содержит ключ (например, идентификатор) и значение (данные пользователя).

Ключ: `1`, Значение: `John Doe,29`

Ключ: `2`, Значение: `Jane Smith,34`

**Avro Files**

Avro — это формат с поддержкой схем, где данные сохраняются с метаданными о своей структуре, что делает Avro файлы самодокументируемыми. Они выглядят как бинарные файлы, которые не предназначены для чтения человеком. Выглядят вот так

```
{"id": 1, "name": "John Doe", "age": 29}
{"id": 2, "name": "Jane Smith", "age": 34}
{"id": 3, "name": "Emily White", "age": 25}
```
                
**Parquet Files**

Самый популярный формат. Parquet — это столбцовой формат хранения, оптимизированный для аналитики. Это бинарный формат, в котором данные организованы по столбцам, что позволяет эффективно сжимать данные и ускорять доступ к ним.

```
id	     name	        age
1	      John Doe	    29
2	      Jane Smith	  34
3	      Emily White	 25
```

**ORC Files**
ORC (Optimized Row Columnar) — это формат столбцового хранения, разработанный для хранения больших наборов данных, преимущественно в Hive. Это бинарный формат, оптимизированный для использования с системами Big Data, такими как Apache Hive. Мы будем его касаться, но особо он не используется. 

