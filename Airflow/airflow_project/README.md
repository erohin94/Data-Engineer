# Airflow проект

[Инструкция по установке Airflow](https://github.com/erohin94/Data-Engineer/blob/main/Airflow/README.md)

**Про start_date и execution_date**

В Apache Airflow есть 2 ключевые даты, которые нужно понимать.

*Start Date* ```start_date```

*Execution Date* ```execution_date```

*Start Date* это дата начала от которой следует начинать запускать DAG согласно расписания schedule_interval.

*Execution Date* это дата выполнения конкретного запуска. В примере выше у меня было 18 запусков. 18 запусков, а значит 18 execution_date, а именно:

Execution date можно получить, обратившись к контексту выполнения. Контекст можно получить, вызвав функцию ```get_current_context```. Для примера работы с ```execution_date``` я создал новый [DAG](https://github.com/erohin94/Data-Engineer/blob/main/Airflow/airflow_project/dags/first_dag_execution_date.py) по аналогии с предыдущим, но немного модифицировал код оператора.

*Контекст — это информация о том, в каком состоянии находится выполнение задачи в момент её выполнения. Он включает в себя данные, такие как:*

Информация о DAG и задаче:

```dag_id``` — идентификатор DAG.

```task_id``` — идентификатор задачи.

```execution_date``` — дата и время выполнения задачи.

```task_instance``` — объект, представляющий текущую задачу в контексте её выполнения.

Параметры выполнения:

```ds``` — строковое представление даты выполнения задачи в формате YYYY-MM-DD.

```ts``` — временная метка (timestamp) даты и времени в формате YYYY-MM-DDTHH:MM:SS.

Информация о DAG Run:

```dag_run``` — объект, представляющий текущий запуск DAG, включая дату и время его выполнения.

Таски будут падать через день, т.е. каждый нечетный день.

![image](https://github.com/user-attachments/assets/f5d07f3e-a582-474a-9bca-0f70ec7194c5)

**Зависимость тасков**

Прелесть любого workflow менеджера в умении формировать граф зависимостей между тасками и успешно его выполнять. В предыдущем примере рассмотрели пайплайн всего лишь с одним таском PythonOperator even_only. Но в реальных пайплайнах количество тасков может достигать огромных значений, добавим ещё один таск, и сделаем его зависимым от первого.

Я буду использовать DummyOperator, который ничего не делает. [DAG](https://github.com/erohin94/Data-Engineer/blob/main/Airflow/airflow_project/dags/dag_with_two_tasks.py)

Обратите внимание на последнюю строку:

```even_only >> dummy```

Это способ указать направление выполнения тасков и их зависимость. Если в UI открыть детали DAG и посмотреть графическое представление зависимостей, то видно:

![image](https://github.com/user-attachments/assets/8af6c346-0abe-4291-a635-a64035baba80)

Что означает, что dummy_task зависит от выполнения even_only. Этот же код можно записать в других вариациях:

```# dummy.set_upstream(even_only)```

```# even_only.set_downstream(dummy)```

```# dummy << even_only```

Используем вариант со стрелочками, т.к. они интуитивно понятны, направление стрелок указывает на порядок выполнения.

Если стартовать новый DAG, то часть запусков провалится (в нечетный день), а другая часть будет успешно выполнена (в четный день). Я специально выбрал такой пример, чтобы была возможность посмотреть в интерфейсе как выглядят ситуации с неудачными запусками пайплайнов, изучить статусы операторов.

Пример выполнения в нечетный день (9 января 2025):

![image](https://github.com/user-attachments/assets/09e6e19c-c773-4974-b441-6ea7b9ecbc0a)

Обратите внимание, что Airflow подкрашивает операторы в цвет, соответствующий статусу в легенде. Красный означает, что оператор even_only вернул ошибку, а оранжевый сигнализирует о том, что предшествующий таск (для dummy_task это even_only) не был успешно выполнен (upstream failed).

Важно помнить, что успешное выполнение пайплайна возможно только при условии, что все операторы завершились без ошибок.

В четных днях вы должны увидеть следующую картину:

![image](https://github.com/user-attachments/assets/2dcdef95-a914-43ca-b5b1-812e5b15622e)

**TaskFlow API**

В Apache Airflow 2.0 появился новый способ описания DAG и PythonOperator — TaskFlow API. TaskFlow API это синтаксический сахар, который делает код чище и проще для чтения. Во второй версии Apache Airflow были введены 2 декоратора: dag и task.

Давайте посмотрим как выглядит предыдущий код, переписанный на TaskFlow API: [DAG](https://github.com/erohin94/Data-Engineer/blob/main/Airflow/airflow_project/dags/taskflow_dag_with_two_operators.py)

Когда вы добавляете новый DAG в папку с DAG'ами в Apache Airflow,нужно перезапустить планировщик (scheduler), чтобы новый DAG был загружен и появился в веб-интерфейсе.

Перезапустите контейнер с планировщиком: ```docker restart airflow_project-airflow-scheduler-1```

![image](https://github.com/user-attachments/assets/4e5d1120-f7f9-49ab-aa7c-b7ecdecdab09)

Чтобы сформировать объект DAG, необходимо обернуть функцию декоратором dag. Этот декоратор принимает те же аргументы. Чтобы превратить функцию в PythonOperator, её необходимо обернуть в декоратор task.

Обратите внимание, что инстанс DAG должен быть доступен в глобальном пространстве при импорте кода планировщиком. Именно поэтому я вызываю функцию, обернутую в декоратор dag и присваиваю переменной main_dag. Если этого не сделать, то планировщик не сможет найти DAG.

Заметьте, что явно не указываем task_id, он берётся из названия оборачиваемой функции.

Далее во всех примерах, где будет нужен PythonOperator я буду использовать TaskFlow API. Мы также рассмотрим как TaskFlow API позволяет "бесшовно" передавать возвращаемые значения из одного оператора в другой. До TaskFlow API необходимо было явно использовать XCom.

Подробный пост на тему TaskFlow API [TaskFlow API в Apache Airflow 2.0](https://startdatajourney.com/ru/course/apache-airflow-2/modules/11/36/1#:~:text=TaskFlow%20API%20%D0%B2%20Apache%20Airflow%202.0)
