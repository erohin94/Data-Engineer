# **Установка**

Дока https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html

Установить и запустить Airflow можно разными способами,но для удобства буду использовать docker compose из официальной документации Airflow.

Для развертывания Apache Airflow с использованием docker compose, нужно выполнить несколько подготовительных шагов, включая настройку папок, скачивание и конфигурирование файла docker-compose.yaml, а также инициализацию и запуск Airflow. Так вкратце выглядит путь, который предстоит пройти.

**Шаг 1: Подготовка структуры папок**

- Создать  главную папку для проекта `Airflow`. Например, `airflow_project`
- Внутри этой папки создать три подпапки: `dags`, `logs`, и `plugins`

**Шаг 2:  Скачивание docker-compose.yaml**

Необходимо скачать файл `docker-compose.yaml` с официального сайта Apache Airflow по ссылке :
https://airflow.apache.org/docs/apache-airflow/2.9.2/docker-compose.yaml и поместить его в папку `airflow_project`.

Копируем весь текст из этой ссылки и вставлем в блокнот который создали в папке.
Меняем расширение у блокнота c txt на yaml. Получаем файл `docker-compose.yaml`

Должно получиться вот так:

![image](https://github.com/user-attachments/assets/294896ad-6630-41f3-97e5-e9f4b063ad84)

Теперь открываем этот файл и находим 62 строку. Меняем в ней значение параметра с 'true' на 'false' и нажимаем кнопку Сохранить:

![image](https://github.com/user-attachments/assets/1902507f-b1fc-465c-9405-a3e7358209f9)

Этот параметр отвечает за загрузку разных примеров DAG при запуске Airflow. Это несомненно полезно, но только когда видишь Airflow не в первый раз. Всегда можно включить этот параметр и посмотреть на те примеры, которые сделали сами создатели Airflow.

**Шаг 3: Инициализация Airflow**
- Открываем Docker Desctop
- Заходим в папку  `airflow_project` кликаем правой кнопкой мыши и выбираем пункт меню Открыть в терминале и в открывшемся терминале выполняем команду:

Linux:

`sudo docker-compose up airflow-init`

Windows (выполняем в PowerShell либо Git Bash):

`docker-compose up airflow-init`

Это необходимо для инициализации, а так же, как пишут в официальной документации, "чтобы запустить миграцию базы данных и создать первую учетную запись пользователя".

- Дождитесь окончания процесса инициализации, который настроит служебную базу данных и создаст необходимые таблицы.(Кстати в качестве служебных БД в Airflow используется PostgreSQL и не только - это будет видно в процессе выполнения команды).

Успешное завершение выглядит так:

![image](https://github.com/user-attachments/assets/2a579f1d-aa96-4669-94cb-3c5fcedf7749)

**Шаг 4: Запуск Airflow**

- После успешной инициализации запустите Airflow выполнив в терминале команду:

Linux:

`sudo docker-compose up -d `

Windows:

`docker-compose up -d`

Подождите, пока все сервисы будут запущены.

Чтобы увидеть интерфейс Airflow откройте браузер и перейдите по адресу http://127.0.0.1:8080/

Если все прошло по плану, то появится окно для ввода имени пользователя и пароля. Вводим в оба окошка airflow и подтверждаем. Перед нами открывается основное окно инструмента:

![image](https://github.com/user-attachments/assets/aa337c6c-7bbe-4483-89f6-bf1bf83ef6d0)

На этом установка закончена - переходим к использованию!

Дополнительно, для тех кто все же установил Docker Desktop, обращаю внимание на то, как выглядит запущенный compose в Docker Desktop. Можно увидеть все запущенные контейнеры, описанные в файле:

![image](https://github.com/user-attachments/assets/76328688-c846-48b1-ab0d-e19b9b8da55f)

# **Заметки**

**Как подключится к PostgreSQL через Ui Airflow?**

В .yaml файле для airflow не был указан порт для postgresql (База для логов).

В результате чего я не мог подключится к БД из dbvear и ui airflow

Так как:

-Внутри контейнера PostgreSQL всегда работает на стандартном порту 5432 (если не изменено в конфигурации).

-На хосте (вашей машине) порт не проброшен (нет ports: в конфигурации).

Host (хост): postgres (имя сервиса в docker-compose.yml)

Port (порт): 5432 (стандартный порт внутри контейнера)

В .yaml для Airflow прописал порты и заработало

![1](https://github.com/user-attachments/assets/b8032d18-2b3e-42bc-a6f7-ebe0f05865aa)

В conection прописал следующее

![2](https://github.com/user-attachments/assets/64e435a7-b7a9-4a41-9e78-70432eb0a2f6)

dbvear

![image](https://github.com/user-attachments/assets/9d447bfe-4f90-480c-aa53-eb3358fbdc6c)

**Не мог так же подключится к БД postgre которая создана в другом .yaml файле из UI Airflow. Даг падал в ошибку**

Причины и решение

1. Разные сети Docker
   
По умолчанию каждый docker-compose создаёт свою собственную сеть. Контейнеры из разных файлов не видят друг друга.

Решение:

Создать общую сеть в обоих файлах. В обоих docker-compose файлах добавить:
```
networks:
  default:
    name: my_shared_network
    external: true
```

2. Использовать host.docker.internal (для Mac/Windows):

В конфиге my_postgres добавить:

```
extra_hosts:
  - "host.docker.internal:host-gateway"
```
Тогда в Airflow UI Connections указать Host: host.docker.internal, Port: 5423

Сделал как в пункте 2. 

![3](https://github.com/user-attachments/assets/2c82fc68-5c84-446f-9982-9ce6f6bb5a73)

![4](https://github.com/user-attachments/assets/6103acc3-beb5-464c-94eb-84c5800b7eee)


**Про хост и порт**

Как работают хост и порт в Docker на локальном компьютере

Когда вы запускаете PostgreSQL (или любой другой сервис) в Docker, важно понимать три уровня адресации:

Внутри контейнера

Сервис (например, PostgreSQL) всегда работает на своём стандартном порту (5432 для Postgres).

Хост внутри контейнера: localhost (но только для этого контейнера).

В сети Docker (между контейнерами)

Контейнеры могут общаться по имени сервиса (из docker-compose.yml) или имени контейнера.

Пример: если у вас есть сервис my_postgres, другие контейнеры могут подключиться к нему по адресу:

```
Host: my_postgres  
Port: 5432
Docker автоматически резолвит имена через внутренний DNS.
```

На локальном компьютере (host-машине)

Чтобы подключиться к контейнеру с вашего ПК (не из Docker), нужно:

Пробросить порт через ports: в docker-compose.yml.

Использовать localhost (или 127.0.0.1) и внешний порт.

Примеры из конфигов
```
1. Для логов Airflow (первый контейнер)
yaml
services:
  postgres:
    image: postgres:13
    ports:
      - "5432:5432"  # Проброс порта: хост_порт:контейнер_порт
Подключение из другого контейнера:
postgres:5432

Подключение с локального ПК:
localhost:5432
```

2. Для моей БД (второй контейнер)
```
yaml
services:
  my_postgres:
    image: postgres:15
    ports:
      - "5423:5432"  # Внутри контейнера — 5432, снаружи — 5423
Подключение из другого контейнера:
my_postgres:5432

Подключение с локального ПК:
localhost:5423
```

Почему Airflow не подключается к my_postgres?

Если Airflow в Docker (в другом docker-compose):

Контейнеры по умолчанию в разных сетях → не видят друг друга.

Решение:

Создать общую сеть (как в предыдущем ответе).

Либо использовать host.docker.internal (если ОС поддерживает).

Если Airflow на хосте:

Нужно использовать localhost:5423 (проброшенный порт).

Итог

Хост — это "адрес" сервиса:

В Docker-сети: имя сервиса (my_postgres).

На локальном ПК: localhost.

Порт:

Внутри контейнера — стандартный (5432).

Снаружи — тот, что указан в ports: (5423).

Если Airflow и PostgreSQL в одном docker-compose — подключение по my_postgres:5432.
Если Airflow на хосте — localhost:5423.

