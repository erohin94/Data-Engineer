# New York taxi data pipline

[Инстукция](https://github.com/erohin94/Data-Engineer/blob/main/Airflow/README.md) по установке Airflow

Датасет, содержащий поездки на такси по городу Нью-Йорк, пожалуй, один из самых распространённых примеров анализа данных. Данные открыто лежат на [сайте](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page).

Задача пайплайна — скачать сырые данные с сайта, трансформировать их в колоночный формат Parquet и загрузить в S3. Это одна из типичных задач дата инженеров, когда необходимо что-то откуда то выгрузить, сохранить и трансформировать.

В качестве хранилища можно использовать AWS S3. Это популярный сервис для хранения файлов от компании Amazon, также S3 можно рассматривать как распределенную файловую систему. Чтобы воспользоваться AWS S3 необходимо иметь аккаунт в облаке AWS, также хранение данных на S3 предполагает некоторые расходы. Для тех, кто не хочет платить, есть альтернатива, которую можно развернуть у себя на компьютере. Сервис [Minio](https://min.io/) полностью совместимый с S3 файловый сервер, поэтому Apache Airflow будет работать с ним как с AWS S3.

**Структура пайплайна**

Пайплайн (DAG) будет состоять из следующих операторов:

```- SimpleHttpOperator```, его будем использовать для проверки существования файла на сервере перед его загрузкой

```- 2 PythonOperator```:
```- download_file``` — загрузка файла с сайта и перекладывание на S3 в сжатом виде (gzip)

```to_parquet``` — оператор скачивает файл, загруженный предыдущим оператором, и трансформирует его в формат Parquet, сохраняя результат в S3.

Вот как выглядит это на диаграмме с зависимостями:

![image](https://github.com/user-attachments/assets/6b1b3b38-daa8-4378-ba64-f5a934b8aa95)

Выполнение последующего шага зависит от успешности предыдущего. 

Если необходимого файла на сервере нет (check_file), то выполнение загрузки нецелесообразно (download_file), за этим строго следит Airflow.

**Про формат Parquet**

Дата инженеру, предстоит неоднократно столкнуться с различными форматами файлов. Хранить данные открытым текстом дорого и неэффективно. Дорого потому что объём больше (а в S3 плата взымается в том числе за размер файла), а неэффективно потому что для того, чтобы найти в нём нужную информацию, его надо прочитать целиком (не говоря уже о времени передачи по сети). Умные головы за нас решили эту проблему, поэтому существует множество эффективных способов хранить и читать данные.

**Apache Parquet** — это бинарный формат колоночного хранения данных в сжатом виде (есть ряд поддерживаемых алгоритмов сжатия, включая lzo, gzip, snappy и т.д.). Идеально подходит для представления табличных данных. Parquet-файл можно представить в виде базы данных с одной таблицей. Преимущество этого формата в эффективной компрессии файла за счёт колоночного хранения (строка по сути содержит все данные одной конкретной колонки), а также в эффективном чтении. В аналитических запросах редко присутствуют выборки всех колонок сразу, обычно читают лишь часть. Если не вдаваться в детали реализации Parquet, а попробовать объяснить представление данных внутри максимально просто, то Parquet выглядит как небольшая файловая система, где значения каждой колонки лежат в отдельных файлах, а также присутствует дополнительный файл с метаданными, где хранится информация о типах колонок и их расположении. То есть чтобы получить значения заданных колонок нужно прочитать только файлы, содержащие данные этих колонок (а не всё целиком). Надеюсь у меня получилось внятно объяснить. Подробную информацию можно найти на официальном [сайте Apache Parquet](https://parquet.apache.org/). Также рекомендую взглянуть на наглядное [сравнение между Parquet и CSV в скорости обработки и стоимости](https://dzone.com/articles/how-to-be-a-hero-with-powerful-parquet-google-and).

# Разбор каждого оператора из DAG по отдельности

**check_file**

Первым оператором будет ```SimpleHttpOperator``` под названием ```check_file```. ```SimpleHttpOperator``` входит в стандартный набор операторов Apache Airflow. Его задача — выполнить HTTP запрос и вернуть ответ. Мы его будем использовать для проверки существования файла перед его загрузкой.

Чтобы не скачивать весь файл целиком я предлагаю использовать метод HEAD. Он идентичен стандартному GET с той лишь разницей, что ответ будет без тела, вернутся лишь заголовки и статус ответа.

Также ```SimpleHttpOperator``` отличный способ показать как работать с разделом ```Connections```.

При инициализации оператор принимает несколько аргументов:

```task_id``` — уникальное название оператора

```method``` — HTTP метод, мы будем использовать HEAD

```http_conn_id``` — название ключа соединения, которое мы создадим через раздел Connections. Оператору нельзя передать полную ссылку напрямую.

```endpoint``` — URI, т.е. это всё, что есть в ссылке после указания домена. Например, если полная ссылка на данные за декабрь 2020 года ```https://nyc-tlc.s3.amazonaws.com/trip+data/yellow_tripdata_2020-12.csv```, то endpoint здесь это ```/trip+data/yellow_tripdata_2020-12.csv``` или ```yellow_tripdata_2020-12.csv```. Он меняется в зависимости от года и месяца.

```dag``` — инстанс объекта DAG (если используется декоратор dag, то этот аргумент можно опустить).

Также у оператора есть и другие принимаемые аргументы, например, тело запроса, заголовки и даже есть возможность передать callable объект для обработки ответа в аргументе response_check. Но для нашего пайплайна это не нужно.

