# Описание проекта
В этом репозитории немного изменю проект который делал [тут](https://github.com/erohin94/Data-Engineer/tree/main/Airflow/airflow_project_ny_taxi) поэтому установку всех зависимостей, 
настройку подключений и тд беру из первого проекта. 

Проект представляет собой систему на базе Apache Airflow, которая выполняет несколько задач обработки данных, включая загрузку файлов с веб-ресурса, их сжатие в формат GZIP и загрузку в хранилище S3, а затем преобразование этих данных в формат Parquet.

Проект будет модифицирован следующим образом:

*1.Загрузка файла с веб-ресурса и сжатие в GZIP:*

Используется оператор ```PythonOperator``` для загрузки файла с веб-сайта. Загруженный файл будет сохраняться в сжатом формате GZIP. Для реализации сжатия файла используется библиотека gzip.

*2.Функция загрузки:*

Так как ```PythonOperator``` принимает объект типа callable, который будет выполнен, функция для загрузки файла будет вынесена в отдельный модуль ```functions.py```, что улучшит структуру проекта и обеспечит повторное использование кода. В этой функции будет происходить скачивание файла с веб-ресурса, его сжатие в GZIP и загрузка в S3. Функция будет возвращать путь до S3-объекта, который будет использован в следующем операторе для дальнейшей обработки.

*3.Трансформация в формат Parquet:*

После того как файл будет загружен в S3, следующий оператор PythonOperator скачает сжатый GZIP файл с S3. Этот файл будет преобразован в формат Parquet с помощью библиотеки pandas и pyarrow.
Результат (Parquet файл) будет сохранен обратно в S3.

**Структура проекта**

Проект будет состоять из нескольких файлов:

*DAG файл (например, airflow_dag.py):*

Здесь будет прописана основная логика для Airflow: задачи, зависимости между ними, а также настройка DAG.

*Модуль функций (functions.py):*

В нем будет находиться основная логика для загрузки файла, сжатия в GZIP и загрузки на S3, а также преобразования файла в формат Parquet.

# Про формат Parquet

Хранение данных в открытом текстовом формате, таком как CSV, может быть неэффективным по нескольким причинам.

*1. Высокие затраты на хранение:*

Открытые текстовые файлы, такие как CSV, занимают много места на диске, особенно если данные включают большое количество строк и столбцов.
В хранилищах данных, таких как Amazon S3, стоимость хранения зависит от объема данных. Чем больше размер файла, тем дороже его хранение.

*2. Неэффективность при чтении и передаче:*

В открытых текстовых файлах данные представлены построчно, что означает, что чтобы извлечь данные из одной колонки, вам нужно прочитать весь файл целиком.
Это также увеличивает время передачи данных по сети. Например, если вы хотите получить информацию из одного столбца, вам все равно нужно будет загружать весь файл, даже если вам не нужны другие столбцы.
Чтобы решить эти проблемы, используются эффективные форматы для хранения и обработки данных, такие как Apache Parquet.

**Что такое Parquet?**

Apache Parquet — это бинарный формат для хранения данных, который использует колоночное хранение. Это означает, что данные в файле Parquet хранятся по колонкам, а не по строкам, как в обычных CSV или текстовых файлах.

*Почему Parquet эффективен?*

Компрессия данных:

Parquet использует сжатие данных. Формат поддерживает несколько алгоритмов сжатия, таких как gzip, snappy, lzo и другие, что позволяет значительно уменьшить объем файлов по сравнению с текстовыми файлами.
Благодаря колоночному хранению данные могут быть сжаты более эффективно, особенно если колонки содержат повторяющиеся или однотипные значения.

Эффективное чтение данных:

При колоночном хранении данных, при запросах к файлам, часто требуется прочитать только некоторые колонки, а не весь файл. Это ускоряет время работы с данными.
Например, если ваш запрос требует только информации из нескольких столбцов, то в случае с Parquet будет достаточно прочитать только те колонки, которые нужны, а не загружать весь файл целиком.
Это значительно уменьшает время выполнения запросов, особенно когда работа идет с большими объемами данных.

*Как устроен формат Parquet?*

Можно представить Parquet как нечто похожее на файловую систему с несколькими уровнями:

Данные хранятся по колонкам. Каждая колонка записана в отдельном блоке данных, и для каждой колонки хранится метаинформация о типах данных и расположении значений.

В Parquet также есть метаданные, которые содержат информацию о структуре данных, типах столбцов и их расположении. Эти метаданные помогают быстрее обрабатывать данные, так как можно сразу понять, какие колонки в файле содержат нужную информацию.

Файлы Parquet не хранят все данные подряд (как это делает обычный текстовый файл), а используют более эффективные способы хранения и индексирования, что снижает затраты на поиск нужных данных.

Если не вдаваться в детали реализации Parquet, а попробовать объяснить представление данных внутри максимально просто, то Parquet выглядит как небольшая файловая система, где значения каждой колонки лежат в отдельных файлах, а также присутствует дополнительный файл с метаданными, где хранится информация о типах колонок и их расположении. То есть чтобы получить значения заданных колонок нужно прочитать только файлы, содержащие данные этих колонок (а не всё целиком).

**Преимущества использования Parquet**

*Меньше пространства для хранения:*

Паркетные файлы занимают гораздо меньше места на диске, чем текстовые файлы, за счет сжатия данных и эффективного представления информации.

*Быстрее выполняются аналитические запросы:*

Поскольку вы можете читать только нужные колонки, работа с большими объемами данных становится значительно быстрее.
Например, при использовании Parquet с инструментами аналитики, такими как Apache Spark, AWS Athena или Presto, запросы могут выполняться быстрее, чем при работе с CSV или текстовыми файлами.

*Совместимость с инструментами Big Data:*

Parquet является стандартом для хранения данных в экосистемах Big Data, таких как Apache Hive, Apache Spark, Apache Drill, AWS Glue и других.
Он идеально подходит для аналитических и машинных приложений, где необходимо работать с огромными объемами данных.

**Пример использования:**

Предположим, у вас есть большой CSV-файл с данными о такси в Нью-Йорке. Если этот файл будет сохранен в формате CSV, то его будет тяжело обрабатывать, особенно если вам нужно выполнить запросы, которые используют только одну или несколько колонок, например, только дату или количество поездок.

Если же этот файл преобразовать в Parquet, запросы, например, для поиска всех поездок по определенной дате, будут выполняться гораздо быстрее. Почему? Потому что вам не нужно читать все данные, вы будете читать только данные для нужной колонки (например, для даты), что существенно экономит время и ресурсы.

**Заключение**

Apache Parquet — это мощный и эффективный формат для хранения и обработки данных, который позволяет сжать данные, ускорить выполнение аналитических запросов и снизить затраты на хранение. Он идеально подходит для работы с большими объемами табличных данных и активно используется в аналитических платформах и Big Data экосистемах.


