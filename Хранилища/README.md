## Оглавление

- [Как хранят и обрабатывают большие данные?](#как-хранят-и-обрабатывают-большие-данные)
- [Что такое ETL/ELT процессы?](#что-такое-etlelt-процессы)
- [batch и streaming](#batch-и-streaming)
- [Виды баз данных](#виды-баз-данных)
- [Что такое OLAP/OLTP?](#что-такое-olapoltp)
- [Что такое таблица и из каких элементов она состоит?](#что-такое-таблица-и-из-каких-элементов-она-состоит)
- [ACID](#ACID)
- [Ключи](#ключи)
- [Виды связей таблиц](#виды-связей-таблиц)
- [Нормализация](#нормализация)

## Как хранят и обрабатывают большие данные?

**Big data**

Большие данные нельзя хранить в традиционных хранилищах и нельзя обрабатывать стандартными инструментами.

Потому что хранение будет неоптимизированным, а взятие информации будет занимать очень много времени, что для бизнеса недопустимо. 

Именно поэтому начали создавать специальный софт для хранения, обработки, загрузки и выгрузки больших данных.

---------------------------------------

**1. Распределенные файловые системы:**

**Hadoop Distributed File System (HDFS):**

Это распределенная файловая система, которая является основой платформы Hadoop. HDFS позволяет хранить большие объемы данных, распределяя их по кластерам серверов. Данные разбиваются на блоки, которые хранятся на разных узлах кластера, обеспечивая отказоустойчивость и высокую доступность.

**Amazon S3 (Simple Storage Service):**

Облачное хранилище, используется для хранения больших данных. S3 предлагает высокую масштабируемость, надежность и доступность, что делает его популярным выбором для хранения неструктурированных и полуструктурированных данных.

**Google Cloud Storage, Microsoft Azure Blob Storage:**

Подобные сервисы облачного хранения от Google и Microsoft предлагают аналогичные возможности для хранения больших объемов данных в облаке с высокой доступностью и безопасностью.

---------------------------------------

**2. NoSQL базы данных:**

**Apache Cassandra:**

Распределенная NoSQL база данных, которая позволяет хранить и управлять большими объемами данных на нескольких серверах. Cassandra хорошо подходит для работы с большими данными благодаря своей масштабируемости и высокой доступности.

**MongoDB:**

Документно-ориентированная база данных, которая позволяет хранить данные в формате BSON (расширение JSON). MongoDB хорошо справляется с большими объемами данных и обеспечивает гибкость в работе с полуструктурированными и неструктурированными данными.

**HBase:**

NoSQL база данных, построенная поверх HDFS, которая обеспечивает быструю запись и чтение данных, что делает её подходящей для работы с большими данными в реальном времени.

---------------------------------------

**3. И в отдельную группу вынесены еще 2 способа хранения данных, поскольку они не раскатываются образом по типу скачал-установил. Их дополнительно нужно настраивать, при чем не быстро.**

**Data Lake:**

Data Lake — это централизованное хранилище, в котором можно хранить структурированные, полуструктурированные и неструктурированные данные в их исходном виде. 

Data Lake поддерживает хранение данных любого типа и позволяет организациям собирать и обрабатывать данные из множества источников.

**Хранилища данных (Data Warehouses):**

Эти хранилища данных позволяют эффективно хранить и обрабатывать структурированные данные, а также выполнять сложные аналитические запросы над большими объемами данных. 

**Обработка больших данных**

**1. Распределенные вычисления:**

**Apache Hadoop:**

Платформа для распределенной обработки больших данных, которая включает в себя HDFS и MapReduce. MapReduce позволяет разбивать задачи на небольшие подзадачи, которые выполняются параллельно на разных узлах кластера, а затем собираются в единый результат. 

**Apache Spark:**

Платформа для распределенных вычислений, которая обеспечивает более быструю обработку данных по сравнению с Hadoop благодаря использованию in-memory вычислений (и не только). Spark поддерживает разнообразные задачи: от обработки потоков данных (Streaming) до машинного обучения (MLlib).

---------------------------------------

**2. Потоковая обработка данных:**

**Apache Kafka:**

Платформа для обработки потоков данных в реальном времени, которая позволяет обрабатывать большие объемы данных, поступающих с высокой скоростью. Kafka используется для сбора, хранения и обработки потоков данных, таких как события IoT или логи сервера. Но на самом деле обработку именно самих данных кафка не делает. Она их перенаправляет.

**Apache Flink, Apache Storm:**

Инструменты для потоковой обработки данных, которые обеспечивают низкую задержку и возможность обработки данных в реальном времени. Flink и Storm используются для задач, требующих мгновенного отклика на поступающие данные. Они являются аналогами Kafka.

---------------------------------------

**3. Инструменты для анализа данных:**

**Hive:**

Инструмент, работающий поверх Hadoop, который позволяет выполнять SQL-подобные запросы к данным, хранящимся в HDFS. Hive упрощает анализ больших данных, предоставляя интерфейс, знакомый пользователям SQL.

**Presto:**

Движок для выполнения SQL-запросов по распределённым данным, который обеспечивает высокую производительность и возможность работы с данными, хранящимися в разных системах (HDFS, S3, реляционные базы данных).

---------------------------------------

**4. Облачные платформы для обработки больших данных:**

**Google Cloud BigQuery:**

Инструмент для обработки больших данных, который позволяет выполнять SQL-запросы по большим наборам данных, хранящимся в облаке. BigQuery обеспечивает высокую производительность и масштабируемость, позволяя обрабатывать петабайты данных за считанные секунды.

**Amazon EMR (Elastic MapReduce):**

Облачная платформа от Amazon для обработки больших данных, основанная на Apache Hadoop и Apache Spark. EMR позволяет быстро разворачивать кластеры и обрабатывать большие объемы данных в облаке.

**Microsoft Azure Synapse Analytics:**

## Что такое ETL/ELT процессы?

**ETL**

**Extract (Извлечение):**

Данные извлекаются из различных источников данных, таких как базы данных, файловые системы, приложения и другие системы.
Источниками могут быть структурированные и полуструктурированные данные, такие как реляционные базы данных, CSV-файлы, XML, JSON и другие.

**Transform (Преобразование):**

Извлеченные данные преобразуются в соответствии с требованиями целевой системы. Это очистка данных, фильтрация, агрегация, сортировка, объединение и другие операции.
На этом этапе данные могут быть преобразованы в более подходящий формат для анализа и хранения, например, нормализованы, денормализованы, агрегированы и т.д.

**Load (Загрузка):**

Преобразованные данные загружаются в целевую систему, такую как хранилище данных. После этого данные готовы для использования аналитическими инструментами и бизнес-приложениями.

**Преимущества ETL:**

Преобразование данных до загрузки позволяет гарантировать, что в хранилище данных попадают только качественные, очищенные и готовые к анализу данные.

Поскольку данные уже преобразованы до загрузки, они могут быть быстрее обработаны аналитическими запросами.

**Недостатки ETL:**

Преобразование данных до их загрузки может потребовать значительных вычислительных ресурсов на промежуточных серверах.

Весь процесс ETL может занимать значительное время, особенно если объемы данных велики.

----------------------------------------

**ELT**

ELT — это более современный подход, который стал популярным с развитием мощных систем обработки и хранения данных. Он также состоит из трёх этапов, но порядок операций отличается:

**Extract (Извлечение):**

Как и в ETL, данные извлекаются из различных источников данных.

**Load (Загрузка):**

Извлеченные данные загружаются в целевую систему, такой как озеро данных или хранилище данных, без предварительного преобразования. Это может быть особенно полезно для хранения больших объемов неструктурированных или полуструктурированных данных.

**Transform (Преобразование):**

Преобразование данных происходит уже после их загрузки в целевую систему. Эта операция выполняется непосредственно на мощностях хранилища данных или озера данных, используя доступные вычислительные ресурсы.

Данные могут быть трансформированы по мере необходимости, например, при выполнении аналитических запросов или в процессе подготовки отчетов.

**Преимущества ELT:**

Подход ELT особенно эффективен для работы с большими объемами данных, так как вычислительные мощности целевых систем могут быть масштабированы в зависимости от потребностей.

Данные загружаются в "сыром" виде, что позволяет выполнять разные преобразования в зависимости от задачи. Это упрощает добавление новых типов данных и их анализ без необходимости изменения всего процесса загрузки.

Поскольку данные загружаются без предварительного преобразования, процесс загрузки может быть быстрее, что особенно важно для больших данных.

**Недостатки ELT:**

Преобразование данных требует значительных вычислительных ресурсов от хранилища данных, что может увеличивать стоимость и нагрузку на систему.

Поскольку данные загружаются без предварительного преобразования, существует риск того, что в хранилище данных попадут некачественные или ошибочные данные, что может усложнить последующую работу с ними.

## batch и streaming. 

ETL/ELT это просто приоритетность выполнения операций в инструментах, то batch и streaming диктует, какие инструменты нужны в этой цепочке.

**Batch обработка данных**

Это метод обработки данных, при котором данные собираются, группируются в пакеты или блоки, а затем обрабатываются в рамках единого процесса. Пакетная обработка выполняется периодически, по расписанию или по мере накопления данных. Этот метод подходит для ситуаций, когда данные не требуют мгновенной обработки и могут быть обработаны с задержкой.

**Примеры пакетной обработки:**

Ежедневные отчеты: Обработка данных о продажах за день, которая запускается в конце дня.

Периодическая загрузка данных: Загрузка и обновление данных в хранилище данных раз в сутки или раз в неделю.

Анализ логов: Обработка и анализ логов серверов или приложений, которые собираются за определенный период.

**Характеристики пакетной обработки:**

Задержка: Данные обрабатываются с определенной задержкой, поскольку обработка начинается только после накопления определенного объема данных.

Высокая производительность: Пакетная обработка позволяет оптимизировать ресурсы и производительность, поскольку данные обрабатываются большими объемами, а не в реальном времени.

Масштабируемость: Подходит для обработки больших объемов данных, так как операции могут быть параллелизированы и распределены по кластерам серверов.

Примеры технологий: Apache Hadoop, Apache Spark (в режиме batch), ETL-процессы.

--------------------------------------------

**Streaming обработка данных**

Это метод обработки данных, при котором данные обрабатываются непрерывно по мере их поступления. В отличие от пакетной обработки, потоковая обработка позволяет обрабатывать данные практически в реальном времени, что особенно важно в случаях, когда нужна быстрая реакция на поступающие данные.

**Примеры потоковой обработки:**

Мониторинг финансовых транзакций: Обнаружение мошеннических операций в реальном времени.

**Характеристики потоковой обработки:**

Минимальная задержка: Данные обрабатываются с минимальной задержкой после их поступления в систему, что позволяет получать результаты практически в реальном времени.

Непрерывность: Обработка данных происходит непрерывно, без необходимости накопления данных в пакеты.

Масштабируемость и адаптивность: Потоковая обработка должна быть способна справляться с переменным объемом данных, поддерживая масштабируемость и адаптивность системы.

Примеры технологий: Apache Kafka, Apache Flink, Apache Storm, Apache Spark Streaming, Google Cloud Dataflow.

##  Виды баз данных

Внутриуровнево базы данных делятся на виды:

**Реляционные базы данных** организуют данные в виде таблиц, где каждая таблица состоит из строк (записей) и столбцов (полей). Таблицы могут быть связаны между собой через ключи.

**Нереляционные NoSQL(NotOnlySQL) базы данных** предназначены для работы с данными, которые не вписываются в традиционные реляционные модели. 
Они могут хранить данные в виде документов (обычно JSON), ключ-значение пар, графов или столбцов. NoSQL базы данных часто используются для работы с большими объемами данных и в высоконагруженных системах.

**Колоночные базы данных** — это базы данных, в которых данные организуются и хранятся по столбцам, а не по строкам, как в традиционных реляционных базах данных. 
Этот подход обеспечивает высокую эффективность при выполнении аналитических запросов, особенно на больших объемах данных. Пример *ClickHouse*

![image](https://github.com/user-attachments/assets/e2c2c067-cf90-478f-a1fd-e5ab55352a1f)

**Так же есть другие виды баз данных:** Иерархические, Документоориентированные, Графовые, Объектно-ориентированные

## Что такое OLAP/OLTP?

Верхнеуровнево базы данных делятся на OLTP и OLAP.

**OLTP (Online Transaction Processing)** — это тип системы обработки данных, предназначенный для управления транзакциями в режиме реального времени. Эти системы обрабатывают большое количество коротких онлайн-транзакций (например, покупка товаров, внесение платежей, обновление записей и т.д.). OLTP-системы оптимизированы для быстрого выполнения большого количества запросов, таких как вставка, обновление и удаление данных.

**Транзакции** в контексте баз данных — это набор операций, выполняемых как единое целое. 

Транзакция гарантирует, что все операции в её составе будут выполнены полностью и корректно или не будут выполнены вовсе. 

Транзакции обеспечивают целостность данных и помогают избежать ситуаций, когда данные остаются в непоследовательном состоянии из-за сбоев или ошибок. 

**Основные характеристики OLTP:**

OLTP-системы поддерживают большое количество одновременных пользователей и операций.

Данные часто структурированы и нормализованы для уменьшения избыточности и обеспечения целостности данных.

OLTP-системы обеспечивают ACID-свойства (атомарность, согласованность, изолированность, долговечность), что важно для обеспечения точности и надежности транзакций.
Банковские системы, системы управления заказами, системы бронирования билетов, системы управления складом.

Пример: Интернет-магазин, который обрабатывает заказы клиентов в реальном времени, добавляет записи о заказах, обновляет информацию о запасах и обрабатывает платежи.

------------------------------------

**OLAP (Online Analytical Processing)** — это тип системы обработки данных, предназначенный для анализа больших объемов данных. OLAP-системы используются для поддержки сложных аналитических запросов, таких как анализ трендов, многомерный анализ данных, генерация отчетов и бизнес-аналитика. В отличие от OLTP, OLAP-системы оптимизированы для чтения данных и выполнения сложных запросов.

**Основные характеристики OLAP:**

OLAP-системы позволяют пользователям анализировать данные по различным измерениям (например, по времени, географии, продукту и т.д.).

Данные часто хранятся в денормализованном виде для ускорения выполнения сложных запросов.

OLAP-системы предназначены для выполнения сложных аналитических операций.

Пример: Руководитель компании использует OLAP-систему для анализа продаж по регионам за последние пять лет, чтобы выявить тенденции и принять стратегические решения.

Подводя итоги, можно сказать, что OLTP подходит для постоянной вставки данных, но на выборку будет работать долго. OLAP, наоборот, вставляет данные медленно, а считывает их моментально.

## Что такое таблица и из каких элементов она состоит?

Основная единица данных в базах данных - это таблица. И все запросы пишутся чаще всего к таблицам, а не к базам данных :)

Таблица в контексте баз данных — это основная структура, используемая для хранения данных. 

Таблица организована в виде строк и столбцов, где каждая строка представляет собой запись **(или экземпляр данных)**, а каждый столбец представляет собой поле **(или атрибут)** с определенным типом данных.

**Атрибуты - название столбца.**

**Строки (кортежи): Строки представляют собой записи в таблице, каждая из которых содержит значения для всех столбцов. В каждой строке хранится информация о конкретном объекте или событии.**

## ACID

**Транзакции** в контексте баз данных — это набор операций, выполняемых как единое целое. 

Транзакция гарантирует, что все операции в её составе будут выполнены полностью и корректно или не будут выполнены вовсе. 

Транзакции обеспечивают целостность данных и помогают избежать ситуаций, когда данные остаются в непоследовательном состоянии из-за сбоев или ошибок.

![image](https://github.com/user-attachments/assets/14ad37ec-b8b2-49c6-96d5-5cc02bb1fb06)

---------------------------------------------------------

Представим себе ситуацию, когда переводите деньги с одного банковского счета на другой. Эта операция может быть представлена как транзакция, состоящая из следующих шагов:

- Уменьшение суммы на одном счете.

 - Увеличение суммы на другом счете.

Если один из этих шагов не удастся (например, из-за технической ошибки), вся транзакция должна быть отменена, чтобы не произошло некорректное изменение баланса на счетах. 

---------------------------------------------------------

**Что делать, если база данных не поддерживает транзакции?**

Если у БД нет транзакций, то мы ее и выбирать не будем, но это не так. Транзакции не являются чем-то важным и супер необходимым.

Если транзакции "из коробки" не доступны, можно либо:

- отслеживать состояние базы данных вручную

- использовать сторонние инструменты для управления транзакциями (например Kafka)

---------------------------------------------------------

**Какие БД поддерживают транзакции?**

MySQL: Поддержка транзакций доступна в таблицах, использующих механизм хранения InnoDB.

PostgreSQL: Полная поддержка транзакций, включая сложные транзакции и вложенные транзакции.

MongoDB: Поддержка транзакций на уровне нескольких документов начиная с версии 4.0. 

И многие другие.

Транзакция - это вставка данных в таблицу, которая всегда должна закончиться либо успехом, либо неудачей, но без изменения данных в БД.

---------------------------------------------------------

**Какие еще свойства должна соблюдать транзакция?**

**ACID** — это акроним, описывающий четыре ключевых свойства транзакций в системах управления базами данных (СУБД). 

Эти свойства обеспечивают надёжность транзакций и целостность данных в базе данных, даже в случае сбоев, ошибок или других непредвиденных ситуаций. 

Вот они слева направо - Атомарность (Atomicity), Согласованность (Consistency), Изолированность (Isolation), Долговечность (Durability).

**1. Атомарность (Atomicity)**

Атомарность гарантирует, что все операции в транзакции будут выполнены полностью или не будут выполнены вовсе. 

Если какая-либо часть транзакции не удается, все изменения отменяются, и база данных возвращается в исходное состояние.

Отправляете деньги с одного банковского счёта на другой. Транзакция включает два шага:

Списать деньги с одного счёта.

Зачислить деньги на другой счёт.

Если первый шаг выполнен успешно, но второй шаг не удался из-за сбоя, деньги не должны исчезнуть. 

Атомарность гарантирует, что либо оба шага будут выполнены (деньги будут переведены), либо ни один из них (деньги останутся на исходном счёте).

Зачем это нужно? Атомарность защищает от частично выполненных транзакций, которые могут привести к некорректному состоянию данных.

**2. Согласованность (Consistency)**

Согласованность гарантирует, что после завершения транзакции база данных остаётся в согласованном состоянии. Это значит, что все правила, ограничения и целостность данных будут соблюдены.

Допустим, у есть правило в базе данных, что сумма всех денег на счетах не может изменяться. Если вы переводите деньги с одного счёта на другой, общая сумма должна остаться неизменной.

Согласованность гарантирует, что транзакция не нарушит это правило: сумма на одном счёте уменьшится, а на другом увеличится ровно на ту же сумму.

Зачем это нужно? Согласованность обеспечивает, что транзакции не приведут к нарушению целостности и правил базы данных, что важно для корректного функционирования системы.

**3. Изолированность (Isolation)**

Изолированность гарантирует, что параллельно выполняемые транзакции не будут влиять друг на друга. Каждая транзакция выполняется так, как будто она единственная в системе.

Два человека одновременно пытаются купить последний билет на концерт. Без изолированности могло бы произойти, что оба видят доступный билет, оба пытаются его купить, и система продаст один билет дважды.

Изолированность гарантирует, что одна из транзакций будет завершена первой (человек успешно купит билет), а вторая транзакция либо увидит, что билетов больше нет, либо будет обработана позже.

Зачем это нужно? Изолированность предотвращает конфликты и ошибки, которые могут возникнуть при параллельной работе с одними и теми же данными.

**4. Долговечность (Durability)**

Долговечность гарантирует, что после успешного завершения транзакции её результаты сохранятся в базе данных и не будут потеряны, даже в случае сбоя системы.

После успешной оплаты (транзакция завершена) ваша покупка зафиксирована, и подтверждение получено.

То есть, после завершения оплаты, система зафиксирует вашу покупку, и вы получите подтверждение об успешной транзакции.

Зачем это нужно? Долговечность обеспечивает сохранение результатов транзакции, что особенно важно для критически важных операций, таких как банковские переводы или покупки.

**Пример**

Так ли важны эти свойства для транзакций? Неужели никаким из них нельзя пренебречь? Ответ будет очень простой - представьте себя покупателем. И тут куча ситуаций, которые могут возникнуть, а самое главное, что у каждого из этих случаев будут последствия.

Вы видите, что ноутбук есть в наличии, и решаете его купить. Однако, поскольку не используется транзакция, другая покупка может одновременно снизить запасы на складе до нуля.

Деньги с вашего счёта успешно списываются, но между этим и обновлением запасов на складе происходит сбой.

Произошел сбой на сервере, и информация о том, что ноутбук был продан, не была обновлена на складе. В результате запас ноутбуков на складе остаётся прежним, хотя фактически ноутбук уже не доступен.

Последствия -  

Вы потеряли деньги, но заказ не был оформлен. В системе нет следов вашей покупки, и вы не получите ноутбук, хотя заплатили за него.

На сайте может отображаться, что ноутбук всё ещё доступен для покупки, хотя его уже нет на складе. Это может привести к множеству разочарованных клиентов, которые будут пытаться купить недоступный товар.

Без следов транзакции в системе может быть сложно доказать, что вы совершили покупку, и вернуть свои деньги.

## Ключи

**Ключи в реляционных базах данных** — это особые столбцы или комбинации столбцов в таблицах, которые служат для идентификации записей и определения связей между таблицами. 

Ключи играют важную роль в обеспечении целостности данных и организации отношений между таблицами в базе данных.

**2 ключевых момента:** 

- Ключи нужны для идентификации каждой строки в таблице. То есть, чтобы у каждой строки было свое уникальное значение. Например, у человека это серия и номер паспорта, у машин VIN номер.

- Ключи нужны для образования отношений между таблицами.

-------------------------------------------------------------

**Первичный ключ (Primary Key).**

Таблица 1: Employees (Сотрудники)

![image](https://github.com/user-attachments/assets/0b8d920a-8c4d-439c-9575-ccc7d58f7c37)

Таблица 2: Departments (Отделы)

![image](https://github.com/user-attachments/assets/8dbc4f63-3f03-4895-8b01-658c0b64600a)

*Таблица: Employees*

Описание: В таблице Employees столбец EmpID является первичным ключом, потому что он уникально идентифицирует каждого сотрудника. Значения в этом столбце уникальны и не могут быть пустыми.

Пример: EmpID = 1 относится к сотруднику по имени Alice.

*Таблица: Departments*

Описание: В таблице Departments столбец DeptID является первичным ключом, потому что он уникально идентифицирует каждый отдел.

Пример: DeptID = 2 относится к отделу Engineering.

**Внешний ключ (Foreign Key).**

Таблица: Employees

Описание: В таблице Employees столбец DeptID является внешним ключом, который ссылается на столбец DeptID в таблице Departments. Это означает, что каждый сотрудник связан с отделом через этот внешний ключ.

Пример: У сотрудника с EmpID = 1 (Alice) DeptID = 1, что в свою очередь является ссылкой на отдел HR в таблице Departments.

**Составной ключ (Composite Key)**

Ситуация: Допустим, у нас есть таблица EmployeeProjects (которая описывает проекты сотрудников), которая отслеживает, какие сотрудники работают над какими проектами. Для уникальной идентификации записи необходимо использовать комбинацию двух столбцов: EmpID и ProjectID.

*Таблица: EmployeeProjects*

![image](https://github.com/user-attachments/assets/b3546b7d-b438-4289-9391-60a3049ca9b9)

В этой таблице комбинация EmpID и ProjectID служит составным первичным ключом, что значит, что для каждого проекта сотрудник может быть указан только один раз. Уникально идентифицирует сочетание сотрудника и проекта, на котором он работает. Ни один отдельный столбец (EmpID или ProjectID) не может быть уникальным, но их комбинация является уникальной.

![image](https://github.com/user-attachments/assets/a4f93468-94f1-4220-87f3-da03c4a573e9)

## Виды связей таблиц

**Связь "Один к одному" (One-to-One)** - Это когда одна запись в таблице A соответствует одной записи в таблице B. (Пример пользователи и профили).

**Связь "Один ко многим" (One-to-Many)** - Это когда одна запись в таблице A может соответствовать нескольким записям в таблице B.

Рассмотрим две таблицы: Клиенты и Заказы. В этой связи один клиент может сделать несколько заказов, но каждый заказ принадлежит только одному клиенту.

**Связь "Многие ко многим" (Many-to-Many)** - Это когда одна запись в таблице A может соответствовать многим записям в таблице B, и наоборот.

Рассмотрим три таблицы: Студенты, Курсы и промежуточную таблицу Регистрации. В этой связи один студент может записаться на несколько курсов, и один курс может включать нескольких студентов.

## Нормализация

**Нормализация** — это процесс организации данных в базе данных таким образом, чтобы минимизировать избыточность (дублирование) и избежать аномалий при вставке, обновлении или удалении данных. Идея заключается в том, чтобы разложить данные на логически связанные таблицы, каждая из которых отвечает за хранение определенного набора информации.

**Зачем нужна нормализация?**

Когда одна и та же информация хранится в нескольких местах, это может привести к ошибкам и неконсистентности данных. Нормализация помогает хранить каждую единицу информации только один раз.

Если данные хранятся правильно и связаны логически, то уменьшается вероятность ошибок при их обновлении.

Правильно организованная база данных легче модифицируется и обновляется без риска нарушить работу всей системы.

**А что такое нормальные формы?** 

**Нормальные формы** — это шаги нормализации, которые следуют друг за другом. Каждая следующая нормальная форма строится на основе предыдущей и требует выполнения определенных правил.
 ----
 
**Первая нормальная форма (1NF)**

Суть:

Данные в таблице должны быть атомарными, то есть каждая ячейка таблицы должна содержать только одно значение.
В таблице не должно быть повторяющихся строк.
Возьмем таблицу Заказы.

![image](https://github.com/user-attachments/assets/91e4967d-db02-486f-ba7c-0e5b0ac90e03)

Здесь, в колонке Products содержится несколько значений в одной ячейке, что нарушает 1NF. 

![image](https://github.com/user-attachments/assets/172e1997-e37f-4f6b-a102-c0f615544276)

Теперь каждая ячейка содержит только одно значение.

----

**Вторая нормальная форма (2NF)**

Суть:

Таблица должна соответствовать 1NF.
Все неключевые атрибуты должны зависеть от всего первичного ключа, а не от его части (если ключ составной).
Представим таблицу Продажи:

![image](https://github.com/user-attachments/assets/d3db6fad-1a99-4fb3-acd4-bc1e9ee39cdb)

Здесь CustomerName зависит только от OrderID, а не от составного ключа (OrderID, ProductID). Это нарушение 2NF.

Приведем к 2NF, разделим таблицу на две: 

Таблица Заказы

![image](https://github.com/user-attachments/assets/9d11eae6-8632-4472-821e-7ec972f7426b)

Таблица Продажи:

![image](https://github.com/user-attachments/assets/a5e9f494-0d28-459e-a9eb-08083a186d64)

Теперь CustomerName зависит только от OrderID, а продукты связаны с заказами через другую таблицу. 

**Третья нормальная форма (3NF)**

Суть:

Таблица должна соответствовать 2NF.
Не должно быть транзитивных зависимостей, то есть неключевые атрибуты не должны зависеть друг от друга.
Таблица Сотрудники:

![image](https://github.com/user-attachments/assets/a260fd1b-359a-4f71-9a09-d8803bd0f1d1)

Здесь DepartmentName зависит от DepartmentID, а DepartmentID — от EmployeeID. Это транзитивная зависимость.

Приведение к 3NF:

Разделяем таблицу на две:

Таблица Сотрудники:

![image](https://github.com/user-attachments/assets/980ccadb-eb4d-4222-855d-adda574753d6)

Таблица Отделы: 

![image](https://github.com/user-attachments/assets/12aad3a4-95d3-4680-ac64-91881dc85883)

Теперь каждая таблица содержит данные, зависящие только от своего первичного ключа. 

Почему не всегда используют нормализацию?

Иногда, для улучшения производительности, базы данных сознательно не нормализуют полностью. Это называется денормализацией. Она позволяет ускорить выполнение запросов, уменьшая количество необходимых соединений (join) таблиц. Однако, это может привести к дублированию данных и сложности в поддержании их целостности.
