## Оглавление

- [Как хранят и обрабатывают большие данные?](#как-хранят-и-обрабатывают-большие-данные)
- [Что такое ETL/ELT процессы?](#что-такое-etlelt-процессы)
- [batch и streaming](#batch-и-streaming)
- [Виды баз данных](#виды-баз-данных)
- [Что такое OLAP/OLTP?](#что-такое-olapoltp)
- [Что такое таблица и из каких элементов она состоит?](#что-такое-таблица-и-из-каких-элементов-она-состоит)
- [ACID](#ACID)
- [Ключи](#ключи)
- [Виды связей таблиц](#виды-связей-таблиц)
- [Нормализация](#нормализация)
- [DWH Data Warehouse](#DWH-Data-Warehouse)
- [Модели данных](#Модели-данных)
- [Озеро данных](#Озеро-данных)
- [Болото данных](#Болото-данных)
- [Batch-обработки и Lambda-архитектуры](#Batch-обработки-и-Lambda-архитектуры)
- [Подходы к проектированию DWH](#Подходы-к-проектированию-DWH)
- [SCD](#SCD)
  
## Как хранят и обрабатывают большие данные?

**Big data**

Большие данные нельзя хранить в традиционных хранилищах и нельзя обрабатывать стандартными инструментами.

Потому что хранение будет неоптимизированным, а взятие информации будет занимать очень много времени, что для бизнеса недопустимо. 

Именно поэтому начали создавать специальный софт для хранения, обработки, загрузки и выгрузки больших данных.

---------------------------------------

**1. Распределенные файловые системы:**

**Hadoop Distributed File System (HDFS):**

Это распределенная файловая система, которая является основой платформы Hadoop. HDFS позволяет хранить большие объемы данных, распределяя их по кластерам серверов. Данные разбиваются на блоки, которые хранятся на разных узлах кластера, обеспечивая отказоустойчивость и высокую доступность.

**Amazon S3 (Simple Storage Service):**

Облачное хранилище, используется для хранения больших данных. S3 предлагает высокую масштабируемость, надежность и доступность, что делает его популярным выбором для хранения неструктурированных и полуструктурированных данных.

**Google Cloud Storage, Microsoft Azure Blob Storage:**

Подобные сервисы облачного хранения от Google и Microsoft предлагают аналогичные возможности для хранения больших объемов данных в облаке с высокой доступностью и безопасностью.

---------------------------------------

**2. NoSQL базы данных:**

**Apache Cassandra:**

Распределенная NoSQL база данных, которая позволяет хранить и управлять большими объемами данных на нескольких серверах. Cassandra хорошо подходит для работы с большими данными благодаря своей масштабируемости и высокой доступности.

**MongoDB:**

Документно-ориентированная база данных, которая позволяет хранить данные в формате BSON (расширение JSON). MongoDB хорошо справляется с большими объемами данных и обеспечивает гибкость в работе с полуструктурированными и неструктурированными данными.

**HBase:**

NoSQL база данных, построенная поверх HDFS, которая обеспечивает быструю запись и чтение данных, что делает её подходящей для работы с большими данными в реальном времени.

---------------------------------------

**3. И в отдельную группу вынесены еще 2 способа хранения данных, поскольку они не раскатываются образом по типу скачал-установил. Их дополнительно нужно настраивать, при чем не быстро.**

**Data Lake:**

Data Lake — это централизованное хранилище, в котором можно хранить структурированные, полуструктурированные и неструктурированные данные в их исходном виде. 

Data Lake поддерживает хранение данных любого типа и позволяет организациям собирать и обрабатывать данные из множества источников.

**Хранилища данных (Data Warehouses):**

Эти хранилища данных позволяют эффективно хранить и обрабатывать структурированные данные, а также выполнять сложные аналитические запросы над большими объемами данных. 

**Обработка больших данных**

**1. Распределенные вычисления:**

**Apache Hadoop:**

Платформа для распределенной обработки больших данных, которая включает в себя HDFS и MapReduce. MapReduce позволяет разбивать задачи на небольшие подзадачи, которые выполняются параллельно на разных узлах кластера, а затем собираются в единый результат. 

**Apache Spark:**

Платформа для распределенных вычислений, которая обеспечивает более быструю обработку данных по сравнению с Hadoop благодаря использованию in-memory вычислений (и не только). Spark поддерживает разнообразные задачи: от обработки потоков данных (Streaming) до машинного обучения (MLlib).

---------------------------------------

**2. Потоковая обработка данных:**

**Apache Kafka:**

Платформа для обработки потоков данных в реальном времени, которая позволяет обрабатывать большие объемы данных, поступающих с высокой скоростью. Kafka используется для сбора, хранения и обработки потоков данных, таких как события IoT или логи сервера. Но на самом деле обработку именно самих данных кафка не делает. Она их перенаправляет.

**Apache Flink, Apache Storm:**

Инструменты для потоковой обработки данных, которые обеспечивают низкую задержку и возможность обработки данных в реальном времени. Flink и Storm используются для задач, требующих мгновенного отклика на поступающие данные. Они являются аналогами Kafka.

---------------------------------------

**3. Инструменты для анализа данных:**

**Hive:**

Инструмент, работающий поверх Hadoop, который позволяет выполнять SQL-подобные запросы к данным, хранящимся в HDFS. Hive упрощает анализ больших данных, предоставляя интерфейс, знакомый пользователям SQL.

**Presto:**

Движок для выполнения SQL-запросов по распределённым данным, который обеспечивает высокую производительность и возможность работы с данными, хранящимися в разных системах (HDFS, S3, реляционные базы данных).

---------------------------------------

**4. Облачные платформы для обработки больших данных:**

**Google Cloud BigQuery:**

Инструмент для обработки больших данных, который позволяет выполнять SQL-запросы по большим наборам данных, хранящимся в облаке. BigQuery обеспечивает высокую производительность и масштабируемость, позволяя обрабатывать петабайты данных за считанные секунды.

**Amazon EMR (Elastic MapReduce):**

Облачная платформа от Amazon для обработки больших данных, основанная на Apache Hadoop и Apache Spark. EMR позволяет быстро разворачивать кластеры и обрабатывать большие объемы данных в облаке.

**Microsoft Azure Synapse Analytics:**

## Что такое ETL/ELT процессы?

**ETL**

**Extract (Извлечение):**

Данные извлекаются из различных источников данных, таких как базы данных, файловые системы, приложения и другие системы.
Источниками могут быть структурированные и полуструктурированные данные, такие как реляционные базы данных, CSV-файлы, XML, JSON и другие.

**Transform (Преобразование):**

Извлеченные данные преобразуются в соответствии с требованиями целевой системы. Это очистка данных, фильтрация, агрегация, сортировка, объединение и другие операции.
На этом этапе данные могут быть преобразованы в более подходящий формат для анализа и хранения, например, нормализованы, денормализованы, агрегированы и т.д.

**Load (Загрузка):**

Преобразованные данные загружаются в целевую систему, такую как хранилище данных. После этого данные готовы для использования аналитическими инструментами и бизнес-приложениями.

**Преимущества ETL:**

Преобразование данных до загрузки позволяет гарантировать, что в хранилище данных попадают только качественные, очищенные и готовые к анализу данные.

Поскольку данные уже преобразованы до загрузки, они могут быть быстрее обработаны аналитическими запросами.

**Недостатки ETL:**

Преобразование данных до их загрузки может потребовать значительных вычислительных ресурсов на промежуточных серверах.

Весь процесс ETL может занимать значительное время, особенно если объемы данных велики.

----------------------------------------

**ELT**

ELT — это более современный подход, который стал популярным с развитием мощных систем обработки и хранения данных. Он также состоит из трёх этапов, но порядок операций отличается:

**Extract (Извлечение):**

Как и в ETL, данные извлекаются из различных источников данных.

**Load (Загрузка):**

Извлеченные данные загружаются в целевую систему, такой как озеро данных или хранилище данных, без предварительного преобразования. Это может быть особенно полезно для хранения больших объемов неструктурированных или полуструктурированных данных.

**Transform (Преобразование):**

Преобразование данных происходит уже после их загрузки в целевую систему. Эта операция выполняется непосредственно на мощностях хранилища данных или озера данных, используя доступные вычислительные ресурсы.

Данные могут быть трансформированы по мере необходимости, например, при выполнении аналитических запросов или в процессе подготовки отчетов.

**Преимущества ELT:**

Подход ELT особенно эффективен для работы с большими объемами данных, так как вычислительные мощности целевых систем могут быть масштабированы в зависимости от потребностей.

Данные загружаются в "сыром" виде, что позволяет выполнять разные преобразования в зависимости от задачи. Это упрощает добавление новых типов данных и их анализ без необходимости изменения всего процесса загрузки.

Поскольку данные загружаются без предварительного преобразования, процесс загрузки может быть быстрее, что особенно важно для больших данных.

**Недостатки ELT:**

Преобразование данных требует значительных вычислительных ресурсов от хранилища данных, что может увеличивать стоимость и нагрузку на систему.

Поскольку данные загружаются без предварительного преобразования, существует риск того, что в хранилище данных попадут некачественные или ошибочные данные, что может усложнить последующую работу с ними.

## batch и streaming. 

ETL/ELT это просто приоритетность выполнения операций в инструментах, то batch и streaming диктует, какие инструменты нужны в этой цепочке.

**Batch обработка данных**

Это метод обработки данных, при котором данные собираются, группируются в пакеты или блоки, а затем обрабатываются в рамках единого процесса. Пакетная обработка выполняется периодически, по расписанию или по мере накопления данных. Этот метод подходит для ситуаций, когда данные не требуют мгновенной обработки и могут быть обработаны с задержкой.

**Примеры пакетной обработки:**

Ежедневные отчеты: Обработка данных о продажах за день, которая запускается в конце дня.

Периодическая загрузка данных: Загрузка и обновление данных в хранилище данных раз в сутки или раз в неделю.

Анализ логов: Обработка и анализ логов серверов или приложений, которые собираются за определенный период.

**Характеристики пакетной обработки:**

Задержка: Данные обрабатываются с определенной задержкой, поскольку обработка начинается только после накопления определенного объема данных.

Высокая производительность: Пакетная обработка позволяет оптимизировать ресурсы и производительность, поскольку данные обрабатываются большими объемами, а не в реальном времени.

Масштабируемость: Подходит для обработки больших объемов данных, так как операции могут быть параллелизированы и распределены по кластерам серверов.

Примеры технологий: Apache Hadoop, Apache Spark (в режиме batch), ETL-процессы.

--------------------------------------------

**Streaming обработка данных**

Это метод обработки данных, при котором данные обрабатываются непрерывно по мере их поступления. В отличие от пакетной обработки, потоковая обработка позволяет обрабатывать данные практически в реальном времени, что особенно важно в случаях, когда нужна быстрая реакция на поступающие данные.

**Примеры потоковой обработки:**

Мониторинг финансовых транзакций: Обнаружение мошеннических операций в реальном времени.

**Характеристики потоковой обработки:**

Минимальная задержка: Данные обрабатываются с минимальной задержкой после их поступления в систему, что позволяет получать результаты практически в реальном времени.

Непрерывность: Обработка данных происходит непрерывно, без необходимости накопления данных в пакеты.

Масштабируемость и адаптивность: Потоковая обработка должна быть способна справляться с переменным объемом данных, поддерживая масштабируемость и адаптивность системы.

Примеры технологий: Apache Kafka, Apache Flink, Apache Storm, Apache Spark Streaming, Google Cloud Dataflow.

##  Виды баз данных

Внутриуровнево базы данных делятся на виды:

**Реляционные базы данных** организуют данные в виде таблиц, где каждая таблица состоит из строк (записей) и столбцов (полей). Таблицы могут быть связаны между собой через ключи.

**Нереляционные NoSQL(NotOnlySQL) базы данных** предназначены для работы с данными, которые не вписываются в традиционные реляционные модели. 
Они могут хранить данные в виде документов (обычно JSON), ключ-значение пар, графов или столбцов. NoSQL базы данных часто используются для работы с большими объемами данных и в высоконагруженных системах.

**Колоночные базы данных** — это базы данных, в которых данные организуются и хранятся по столбцам, а не по строкам, как в традиционных реляционных базах данных. 
Этот подход обеспечивает высокую эффективность при выполнении аналитических запросов, особенно на больших объемах данных. Пример *ClickHouse*

![image](https://github.com/user-attachments/assets/e2c2c067-cf90-478f-a1fd-e5ab55352a1f)

**Так же есть другие виды баз данных:** Иерархические, Документоориентированные, Графовые, Объектно-ориентированные

## Что такое OLAP/OLTP?

Верхнеуровнево базы данных делятся на OLTP и OLAP.

**OLTP (Online Transaction Processing)** — это тип системы обработки данных, предназначенный для управления транзакциями в режиме реального времени. Эти системы обрабатывают большое количество коротких онлайн-транзакций (например, покупка товаров, внесение платежей, обновление записей и т.д.). OLTP-системы оптимизированы для быстрого выполнения большого количества запросов, таких как вставка, обновление и удаление данных.

**Транзакции** в контексте баз данных — это набор операций, выполняемых как единое целое. 

Транзакция гарантирует, что все операции в её составе будут выполнены полностью и корректно или не будут выполнены вовсе. 

Транзакции обеспечивают целостность данных и помогают избежать ситуаций, когда данные остаются в непоследовательном состоянии из-за сбоев или ошибок. 

**Основные характеристики OLTP:**

OLTP-системы поддерживают большое количество одновременных пользователей и операций.

Данные часто структурированы и нормализованы для уменьшения избыточности и обеспечения целостности данных.

OLTP-системы обеспечивают ACID-свойства (атомарность, согласованность, изолированность, долговечность), что важно для обеспечения точности и надежности транзакций.
Банковские системы, системы управления заказами, системы бронирования билетов, системы управления складом.

Пример: Интернет-магазин, который обрабатывает заказы клиентов в реальном времени, добавляет записи о заказах, обновляет информацию о запасах и обрабатывает платежи.

------------------------------------

**OLAP (Online Analytical Processing)** — это тип системы обработки данных, предназначенный для анализа больших объемов данных. OLAP-системы используются для поддержки сложных аналитических запросов, таких как анализ трендов, многомерный анализ данных, генерация отчетов и бизнес-аналитика. В отличие от OLTP, OLAP-системы оптимизированы для чтения данных и выполнения сложных запросов.

**Основные характеристики OLAP:**

OLAP-системы позволяют пользователям анализировать данные по различным измерениям (например, по времени, географии, продукту и т.д.).

Данные часто хранятся в денормализованном виде для ускорения выполнения сложных запросов.

OLAP-системы предназначены для выполнения сложных аналитических операций.

Пример: Руководитель компании использует OLAP-систему для анализа продаж по регионам за последние пять лет, чтобы выявить тенденции и принять стратегические решения.

Подводя итоги, можно сказать, что OLTP подходит для постоянной вставки данных, но на выборку будет работать долго. OLAP, наоборот, вставляет данные медленно, а считывает их моментально.

## Что такое таблица и из каких элементов она состоит?

Основная единица данных в базах данных - это таблица. И все запросы пишутся чаще всего к таблицам, а не к базам данных :)

Таблица в контексте баз данных — это основная структура, используемая для хранения данных. 

Таблица организована в виде строк и столбцов, где каждая строка представляет собой запись **(или экземпляр данных)**, а каждый столбец представляет собой поле **(или атрибут)** с определенным типом данных.

**Атрибуты - название столбца.**

**Строки (кортежи): Строки представляют собой записи в таблице, каждая из которых содержит значения для всех столбцов. В каждой строке хранится информация о конкретном объекте или событии.**

## ACID

**Транзакции** в контексте баз данных — это набор операций, выполняемых как единое целое. 

Транзакция гарантирует, что все операции в её составе будут выполнены полностью и корректно или не будут выполнены вовсе. 

Транзакции обеспечивают целостность данных и помогают избежать ситуаций, когда данные остаются в непоследовательном состоянии из-за сбоев или ошибок.

![image](https://github.com/user-attachments/assets/14ad37ec-b8b2-49c6-96d5-5cc02bb1fb06)

---------------------------------------------------------

Представим себе ситуацию, когда переводите деньги с одного банковского счета на другой. Эта операция может быть представлена как транзакция, состоящая из следующих шагов:

- Уменьшение суммы на одном счете.

 - Увеличение суммы на другом счете.

Если один из этих шагов не удастся (например, из-за технической ошибки), вся транзакция должна быть отменена, чтобы не произошло некорректное изменение баланса на счетах. 

---------------------------------------------------------

**Что делать, если база данных не поддерживает транзакции?**

Если у БД нет транзакций, то мы ее и выбирать не будем, но это не так. Транзакции не являются чем-то важным и супер необходимым.

Если транзакции "из коробки" не доступны, можно либо:

- отслеживать состояние базы данных вручную

- использовать сторонние инструменты для управления транзакциями (например Kafka)

---------------------------------------------------------

**Какие БД поддерживают транзакции?**

MySQL: Поддержка транзакций доступна в таблицах, использующих механизм хранения InnoDB.

PostgreSQL: Полная поддержка транзакций, включая сложные транзакции и вложенные транзакции.

MongoDB: Поддержка транзакций на уровне нескольких документов начиная с версии 4.0. 

И многие другие.

Транзакция - это вставка данных в таблицу, которая всегда должна закончиться либо успехом, либо неудачей, но без изменения данных в БД.

---------------------------------------------------------

**Какие еще свойства должна соблюдать транзакция?**

**ACID** — это акроним, описывающий четыре ключевых свойства транзакций в системах управления базами данных (СУБД). 

Эти свойства обеспечивают надёжность транзакций и целостность данных в базе данных, даже в случае сбоев, ошибок или других непредвиденных ситуаций. 

Вот они слева направо - Атомарность (Atomicity), Согласованность (Consistency), Изолированность (Isolation), Долговечность (Durability).

**1. Атомарность (Atomicity)**

Атомарность гарантирует, что все операции в транзакции будут выполнены полностью или не будут выполнены вовсе. 

Если какая-либо часть транзакции не удается, все изменения отменяются, и база данных возвращается в исходное состояние.

Отправляете деньги с одного банковского счёта на другой. Транзакция включает два шага:

Списать деньги с одного счёта.

Зачислить деньги на другой счёт.

Если первый шаг выполнен успешно, но второй шаг не удался из-за сбоя, деньги не должны исчезнуть. 

Атомарность гарантирует, что либо оба шага будут выполнены (деньги будут переведены), либо ни один из них (деньги останутся на исходном счёте).

Зачем это нужно? Атомарность защищает от частично выполненных транзакций, которые могут привести к некорректному состоянию данных.

**2. Согласованность (Consistency)**

Согласованность гарантирует, что после завершения транзакции база данных остаётся в согласованном состоянии. Это значит, что все правила, ограничения и целостность данных будут соблюдены.

Допустим, у есть правило в базе данных, что сумма всех денег на счетах не может изменяться. Если вы переводите деньги с одного счёта на другой, общая сумма должна остаться неизменной.

Согласованность гарантирует, что транзакция не нарушит это правило: сумма на одном счёте уменьшится, а на другом увеличится ровно на ту же сумму.

Зачем это нужно? Согласованность обеспечивает, что транзакции не приведут к нарушению целостности и правил базы данных, что важно для корректного функционирования системы.

**3. Изолированность (Isolation)**

Изолированность гарантирует, что параллельно выполняемые транзакции не будут влиять друг на друга. Каждая транзакция выполняется так, как будто она единственная в системе.

Два человека одновременно пытаются купить последний билет на концерт. Без изолированности могло бы произойти, что оба видят доступный билет, оба пытаются его купить, и система продаст один билет дважды.

Изолированность гарантирует, что одна из транзакций будет завершена первой (человек успешно купит билет), а вторая транзакция либо увидит, что билетов больше нет, либо будет обработана позже.

Зачем это нужно? Изолированность предотвращает конфликты и ошибки, которые могут возникнуть при параллельной работе с одними и теми же данными.

**4. Долговечность (Durability)**

Долговечность гарантирует, что после успешного завершения транзакции её результаты сохранятся в базе данных и не будут потеряны, даже в случае сбоя системы.

После успешной оплаты (транзакция завершена) ваша покупка зафиксирована, и подтверждение получено.

То есть, после завершения оплаты, система зафиксирует вашу покупку, и вы получите подтверждение об успешной транзакции.

Зачем это нужно? Долговечность обеспечивает сохранение результатов транзакции, что особенно важно для критически важных операций, таких как банковские переводы или покупки.

**Пример**

Так ли важны эти свойства для транзакций? Неужели никаким из них нельзя пренебречь? Ответ будет очень простой - представьте себя покупателем. И тут куча ситуаций, которые могут возникнуть, а самое главное, что у каждого из этих случаев будут последствия.

Вы видите, что ноутбук есть в наличии, и решаете его купить. Однако, поскольку не используется транзакция, другая покупка может одновременно снизить запасы на складе до нуля.

Деньги с вашего счёта успешно списываются, но между этим и обновлением запасов на складе происходит сбой.

Произошел сбой на сервере, и информация о том, что ноутбук был продан, не была обновлена на складе. В результате запас ноутбуков на складе остаётся прежним, хотя фактически ноутбук уже не доступен.

Последствия -  

Вы потеряли деньги, но заказ не был оформлен. В системе нет следов вашей покупки, и вы не получите ноутбук, хотя заплатили за него.

На сайте может отображаться, что ноутбук всё ещё доступен для покупки, хотя его уже нет на складе. Это может привести к множеству разочарованных клиентов, которые будут пытаться купить недоступный товар.

Без следов транзакции в системе может быть сложно доказать, что вы совершили покупку, и вернуть свои деньги.

## Ключи

**Ключи в реляционных базах данных** — это особые столбцы или комбинации столбцов в таблицах, которые служат для идентификации записей и определения связей между таблицами. 

Ключи играют важную роль в обеспечении целостности данных и организации отношений между таблицами в базе данных.

**2 ключевых момента:** 

- Ключи нужны для идентификации каждой строки в таблице. То есть, чтобы у каждой строки было свое уникальное значение. Например, у человека это серия и номер паспорта, у машин VIN номер.

- Ключи нужны для образования отношений между таблицами.

-------------------------------------------------------------

**Первичный ключ (Primary Key).**

Таблица 1: Employees (Сотрудники)

![image](https://github.com/user-attachments/assets/0b8d920a-8c4d-439c-9575-ccc7d58f7c37)

Таблица 2: Departments (Отделы)

![image](https://github.com/user-attachments/assets/8dbc4f63-3f03-4895-8b01-658c0b64600a)

*Таблица: Employees*

Описание: В таблице Employees столбец EmpID является первичным ключом, потому что он уникально идентифицирует каждого сотрудника. Значения в этом столбце уникальны и не могут быть пустыми.

Пример: EmpID = 1 относится к сотруднику по имени Alice.

*Таблица: Departments*

Описание: В таблице Departments столбец DeptID является первичным ключом, потому что он уникально идентифицирует каждый отдел.

Пример: DeptID = 2 относится к отделу Engineering.

**Внешний ключ (Foreign Key).**

Таблица: Employees

Описание: В таблице Employees столбец DeptID является внешним ключом, который ссылается на столбец DeptID в таблице Departments. Это означает, что каждый сотрудник связан с отделом через этот внешний ключ.

Пример: У сотрудника с EmpID = 1 (Alice) DeptID = 1, что в свою очередь является ссылкой на отдел HR в таблице Departments.

**Составной ключ (Composite Key)**

Ситуация: Допустим, у нас есть таблица EmployeeProjects (которая описывает проекты сотрудников), которая отслеживает, какие сотрудники работают над какими проектами. Для уникальной идентификации записи необходимо использовать комбинацию двух столбцов: EmpID и ProjectID.

*Таблица: EmployeeProjects*

![image](https://github.com/user-attachments/assets/b3546b7d-b438-4289-9391-60a3049ca9b9)

В этой таблице комбинация EmpID и ProjectID служит составным первичным ключом, что значит, что для каждого проекта сотрудник может быть указан только один раз. Уникально идентифицирует сочетание сотрудника и проекта, на котором он работает. Ни один отдельный столбец (EmpID или ProjectID) не может быть уникальным, но их комбинация является уникальной.

![image](https://github.com/user-attachments/assets/a4f93468-94f1-4220-87f3-da03c4a573e9)

## Виды связей таблиц

**Связь "Один к одному" (One-to-One)** - Это когда одна запись в таблице A соответствует одной записи в таблице B. (Пример пользователи и профили).

**Связь "Один ко многим" (One-to-Many)** - Это когда одна запись в таблице A может соответствовать нескольким записям в таблице B.

Рассмотрим две таблицы: Клиенты и Заказы. В этой связи один клиент может сделать несколько заказов, но каждый заказ принадлежит только одному клиенту.

**Связь "Многие ко многим" (Many-to-Many)** - Это когда одна запись в таблице A может соответствовать многим записям в таблице B, и наоборот.

Рассмотрим три таблицы: Студенты, Курсы и промежуточную таблицу Регистрации. В этой связи один студент может записаться на несколько курсов, и один курс может включать нескольких студентов.

## Нормализация

**Нормализация** — это процесс организации данных в базе данных таким образом, чтобы минимизировать избыточность (дублирование) и избежать аномалий при вставке, обновлении или удалении данных. Идея заключается в том, чтобы разложить данные на логически связанные таблицы, каждая из которых отвечает за хранение определенного набора информации.

**Зачем нужна нормализация?**

Когда одна и та же информация хранится в нескольких местах, это может привести к ошибкам и неконсистентности данных. Нормализация помогает хранить каждую единицу информации только один раз.

Если данные хранятся правильно и связаны логически, то уменьшается вероятность ошибок при их обновлении.

Правильно организованная база данных легче модифицируется и обновляется без риска нарушить работу всей системы.

**А что такое нормальные формы?** 

**Нормальные формы**-это шаги нормализации, которые следуют друг за другом. Каждая следующая нормальная форма строится на основе предыдущей и требует выполнения определенных правил.
 
 ----------------------------------------
 
**Первая нормальная форма (1NF)**

Суть:

Данные в таблице должны быть атомарными, то есть каждая ячейка таблицы должна содержать только одно значение.
В таблице не должно быть повторяющихся строк.
Возьмем таблицу Заказы.

![image](https://github.com/user-attachments/assets/91e4967d-db02-486f-ba7c-0e5b0ac90e03)

Здесь, в колонке Products содержится несколько значений в одной ячейке, что нарушает 1NF. 

![image](https://github.com/user-attachments/assets/172e1997-e37f-4f6b-a102-c0f615544276)

Теперь каждая ячейка содержит только одно значение.

 ----------------------------------------

**Вторая нормальная форма (2NF)**

Суть:

Таблица должна соответствовать 1NF.
Все неключевые атрибуты должны зависеть от всего первичного ключа, а не от его части (если ключ составной).
Представим таблицу Продажи:

![image](https://github.com/user-attachments/assets/d3db6fad-1a99-4fb3-acd4-bc1e9ee39cdb)

Здесь CustomerName зависит только от OrderID, а не от составного ключа (OrderID, ProductID). Это нарушение 2NF.

Приведем к 2NF, разделим таблицу на две: 

Таблица Заказы

![image](https://github.com/user-attachments/assets/9d11eae6-8632-4472-821e-7ec972f7426b)

Таблица Продажи:

![image](https://github.com/user-attachments/assets/a5e9f494-0d28-459e-a9eb-08083a186d64)

Теперь CustomerName зависит только от OrderID, а продукты связаны с заказами через другую таблицу. 

 ----------------------------------------

**Третья нормальная форма (3NF)**

Суть:

Таблица должна соответствовать 2NF.
Не должно быть транзитивных зависимостей, то есть неключевые атрибуты не должны зависеть друг от друга.
Таблица Сотрудники:

![image](https://github.com/user-attachments/assets/a260fd1b-359a-4f71-9a09-d8803bd0f1d1)

Здесь DepartmentName зависит от DepartmentID, а DepartmentID — от EmployeeID. Это транзитивная зависимость.

Приведение к 3NF:

Разделяем таблицу на две:

Таблица Сотрудники:

![image](https://github.com/user-attachments/assets/980ccadb-eb4d-4222-855d-adda574753d6)

Таблица Отделы: 

![image](https://github.com/user-attachments/assets/12aad3a4-95d3-4680-ac64-91881dc85883)

Теперь каждая таблица содержит данные, зависящие только от своего первичного ключа. 

Почему не всегда используют нормализацию?

Иногда, для улучшения производительности, базы данных сознательно не нормализуют полностью. Это называется денормализацией. Она позволяет ускорить выполнение запросов, уменьшая количество необходимых соединений (join) таблиц. Однако, это может привести к дублированию данных и сложности в поддержании их целостности.

## DWH (Data Warehouse)

**DWH (Data Warehouse)** - это централизованное хранилище данных, предназначенное для хранения и анализа больших объемов данных из различных источников. 

**DWH (Data Warehouse)** обычно включают в себя следующие основные компоненты:

1. ETL (Extract, Transform, Load): ETL является ключевым компонентом DWH и отвечает за извлечение данных из различных источников, их трансформацию и загрузку в хранилище данных. ETL процесс обеспечивает очистку, преобразование и интеграцию данных, чтобы они соответствовали требованиям DWH.

2. Хранилище данных: Хранилище данных представляет собой централизованное место для хранения и управления данными в DWH. Обычно используется реляционная база данных, специально оптимизированная для аналитических операций и запросов.

3. Моделирование данных: Моделирование данных включает определение структуры данных в DWH, таких как схемы звезды или снежинки. Моделирование данных облегчает аналитический процесс и обеспечивает эффективность выполнения запросов.

4. Аналитический инструментарий: Аналитический инструментарий включает набор инструментов и технологий для анализа данных в DWH. Это может включать OLAP (Online Analytical Processing) инструменты, инструменты визуализации данных, инструменты машинного обучения и другие.

5. Метаданные: Метаданные представляют собой описание и характеристики данных в DWH. Они содержат информацию о структуре данных, источниках, связях между таблицами и другие метаданные, необходимые для понимания и использования данных в DWH.

6. Инструменты администрирования: Инструменты администрирования обеспечивают управление и мониторинг DWH, включая задачи управления пользователями, безопасностью, мониторинг производительности, резервное копирование и восстановление данных и другие административные функции.

![photo_2025-07-30_15-59-17](https://github.com/user-attachments/assets/6fd79836-ebc3-43eb-bfce-15c63c1efb85)

<img width="1498" height="712" alt="image" src="https://github.com/user-attachments/assets/6737d1b6-2e57-47ac-a974-dd7f4d0eee8f" />

**Архитектура DWH**

<img width="1380" height="476" alt="image" src="https://github.com/user-attachments/assets/188cbbba-5d04-45f3-9ce6-5b2c6ea7ec41" />

**Первичный слой данных**

Операционный слой первичных данных (Primary Data Layer, raw или staging) – это уровень, на котором выполняется загрузка информации из систем-источников в исходном качестве с сохранением полной истории изменений. На этом слое происходит абстрагирование следующих слоев хранилища от физического устройства источников данных, способов их сбора и методов выделения изменений.

**Центральный слой данных**

Ядро хранилища (Core Data Layer) – центральный слой, в котором происходит консолидация данных из разных источников, приводя их к единым структурам и ключам. Здесь осуществляется основная работа с качеством данных и трансформациями, чтобы абстрагировать потребителей от особенностей логического устройства источников данных и необходимости их взаимного сопоставления.

**Слой витрин данных**

Слой аналитических витрин (Data Mart Layer) – уровень, где данные преобразуются в структуры, удобные для анализа и использования в BI-дэшбордах или других системах-потребителях. Витрина данных (Data Mart) представляет собой срез хранилища данных в виде массива тематической, узконаправленной информации, ориентированной, например, на пользователей одной рабочей группы или департамента.

## Модели данных

**«Звезда»** - имеет централизованное хранилище данных, которое хранится в таблице фактов. Схема разбивает таблицу фактов на ряд денормализованных таблиц измерений. Таблица фактов FACT содержит данные, которые будут использоваться для составления отчетов, а таблица измерений DIM (от англ. Dimension - измерение) описывает хранимые данные.

<img width="450" height="409" alt="image" src="https://github.com/user-attachments/assets/97306fed-6955-4596-9da8-de4b4bdcc4cb" />

В нашем примере центральная таблица (фактическая таблица) как раз и содержит в себе значения фактов (поставка заказа), а 4 таблицы вокруг (измерительные таблицы) - содержат в себе измерения (dimension): поставщика(supplier), сотрудника(employee), времени (time) и товара (product).  Обратите внимание, что таблица фактов содержит в основном только идентификаторы - ссылки на измерительные таблицы.

**«Снежинка»** - отличается тем, что использует нормализованные данные. Нормализация означает эффективную организацию данных так, чтобы все зависимости данных были определены, и каждая таблица содержала минимум избыточности. Таким образом, отдельные таблицы измерений разветвляются на отдельные дополнительные таблицы измерений.

<img width="450" height="442" alt="image" src="https://github.com/user-attachments/assets/225a3dfa-8d07-4e9e-834d-c1c8eb3e1a1f" />

Схема «снежинки» использует меньше дискового пространства и лучше сохраняет целостность данных. Основным недостатком является сложность запросов, необходимых для доступа к данным — каждый запрос должен пройти несколько соединений таблиц, чтобы получить соответствующие данные.Ну просто представьте сколько JOINов надо выполнить для получения "человекочитаемой" строки.

В этом примере фактическая таблица - это продажи `(Sales)`. Все остальные таблицы - измерительные.

Таблица `Sales` соединяет все измерительные таблицы через внешние ключи `(FK)`:

```
id_product с Product.
id_client с Client.
id_shop с Shop.
id_date с Time.
```

Таблицы измерений нормализованы:

```
Product соединена с Product Type и Brand.
Client соединена с Client Group.
Shop соединена с City, а City с Region.
Brand соединена с Supplier.
Time соединена с Month, а Month с Quarter.
```

**Data Vault** - это методология проектирования хранилищ данных, которая обеспечивает гибкость, масштабируемость и отслеживаемость изменений в данных.

Data Vault состоит из трех основных элементов: хабов `(hubs)`, связей `(links)` и спутников `(satellites)`.

<img width="843" height="306" alt="image" src="https://github.com/user-attachments/assets/f39e900e-9e18-4b0c-b9b2-531b61930c25" />

- `Хабы (Hubs)`: Хабы представляют собой уникальные списки ключей для каждой сущности данных. Они являются центральными элементами модели и служат для централизованного хранения ключевых атрибутов сущности.

- `Связи (Links)`: Связи представляют собой ассоциативные таблицы, которые устанавливают связь между хабами. Они позволяют моделировать связи между различными сущностями данных.

- `Спутники (Satellites)`: Спутники содержат атрибуты, которые описывают контекст или детали данных из хабов или связей. Они могут содержать дополнительные атрибуты сущности, исторические данные или метаданные.

---------------------------------
*Пример*

Рассмотрим пример интернет-магазина, который хочет создать хранилище данных для анализа поведения клиентов. В названиях таблиц для наглядности укажем их принадлежность.

Еще раз уточним насчет содержания: хабы содержат ключевые бизнес-сущности, связи объединяют их, а сателлиты добавляют описательные и исторические данные. Этот подход обеспечивает гибкость, масштабируемость и возможность отслеживания изменений во времени.

Посмотрим на схему получившегося Data Vault:

<img width="868" height="596" alt="image" src="https://github.com/user-attachments/assets/156a196b-690b-495e-8eca-daedc6a6a1c3" />

*Хабы:*

Хабы содержат бизнес-ключи и являются основными таблицами для идентификации сущностей. Например, order_hub идентифицирует заказы, product_hub идентифицирует продукты, а customer_hub идентифицирует клиентов.

*Линки:*

Линки соединяют хабы между собой и представляют отношения между ними. Например, order_product_link соединяет заказы и продукты, а customer_order_link соединяет клиентов и заказы.

*Сателлиты:*

Сателлиты содержат описательные атрибуты и дополнительную информацию, связанную с хабами и линками. Например, order_satellite содержит данные о датах и статусах заказов, product_satellite содержит информацию о продуктах, а customer_satellite содержит информацию о клиентах.
 

Подведем небольшие итоги:  В очередной раз нельзя однозначно ответить на вопрос какая модель лучше. Выбор зависит от конкретных требований и контекста (опять всплывает основная сложность в работе DE - выбор оптимального инструмента):

-Если нужна простота и высокая производительность для отчетов и анализа, выбирайте модель "Звезда".

-Если требуется баланс между нормализацией и производительностью, рассмотрите модель "Снежинка".

-Если важна гибкость, масштабируемость и управление историчностью данных, Data Vault будет лучшим выбором.

## Озеро данных

Озеро данных (Data Lake) – это хранилище большого объема неструктурированных данных, собранных или генерированных одной компанией. В таком подходе в озеро данных поступают все данные, которые собирает компания, без предварительной очистки и подготовки.

Примеры данных:

-Видеозаписи с беспилотников и камер наружного наблюдения.

-Транспортная телеметрия.

-Фотографии.

-Логи пользовательского поведения.

-Метрики сайтов.

-Показатели нагрузки информационных систем и пр.

Эти данные пока непригодны для типового использования в ежедневной аналитике в рамках BI-систем, но могут быть использованы для быстрой отработки новых бизнес-гипотез с помощью ML-алгоритмов.

<img width="1684" height="1190" alt="image" src="https://github.com/user-attachments/assets/00e437ff-9298-469a-b553-c85d6e74ab98" />


**Основные особенности использования подхода:**

-Хранятся все данные, включая «бесполезные», которые могут пригодиться в будущем или не понадобиться никогда.

-Структурированные, полуструктурированные и неструктурированные разнородные данные различных форматов: от мультимедийных файлов до текстовых и бинарных из разных источников.

-Высокая гибкость, позволяющая добавлять новые типы и структуры данных в процессе эксплуатации.

-Из-за отсутствия четкой структуры необходима дополнительная обработка данных для их практического использования.

-Озеро данных дешевле DWH с точки зрения проектирования.

**Преимущества озера данных:**

-Масштабируемость: распределенная файловая система позволяет подключать новые машины или узлы без изменения структуры хранилища.

-Экономичность: Data Lake можно построить на базе свободного ПО Apache Hadoop, без дорогих лицензий и серверов.

-Универсальность: большие объемы разнородных данных могут использоваться для различных исследовательских задач (например, прогнозирование спроса или выявление пользовательских предпочтений).

-Быстрота запуска: накопленные объемы Data Lake позволяют быстро проверять новые модели, не тратя время на сбор информации из различных источников.

<img width="1388" height="674" alt="image" src="https://github.com/user-attachments/assets/c297fd47-7841-40de-bcff-6751ae7ac637" />

## Болото данных

**Болото данных** — это термин, используемый для описания озера данных, которое вышло из-под контроля. В болотах данных хранится множество данных, которые становятся неуправляемыми, неорганизованными и трудно доступными. Это может привести к низкому качеству данных и невозможности их эффективного использования.

## Batch-обработки и Lambda-архитектуры

На всех графиках мы используем стрелочки для описания перехода данных. Каждая отдельная стрелочка - это ETL процесс. 

**ETL** - это процесс преобразования данных, который состоит из:

- **Извлечение данных (Extraction - E)** - из одного или нескольких источников и подготовка их к преобразованию (загрузка в промежуточную область, проверка данных на соответствие спецификациям и возможность последующей загрузки в ХД);

- **Трансформация данных (Transform - T)** - преобразование форматов и кодировки, агрегация и очистка;

- **Загрузка данных (Load - L)** - запись преобразованных данных, включая информацию о структуре их представления (метаданные), в необходимую систему хранения (КХД) или витрину данных.

Существует также ELT подход. ETL и ELT — два разных способа загрузки данных в хранилище.

**ETL (Extract, Transform, Load)**

ETL сначала извлекают данные из пула источников данных. Данные хранятся во временной промежуточной базе данных. Затем выполняются операции преобразования, чтобы структурировать и преобразовать данные в подходящую форму для целевой системы хранилища данных. После этого структурированные данные загружаются в хранилище и готовы к анализу.

**ELT (Extract, Load, Transform)**

В случае ELT данные сразу же загружаются после извлечения из исходных пулов данных. Промежуточная база данных отсутствует, что означает, что данные немедленно загружаются в единый централизованный репозиторий. Данные преобразуются в системе хранилища данных для использования с инструментами бизнес-аналитики и аналитики.

-----------------------------------------------

# **Пакетная обработка**

ETL процесс может быть разным в зависимости от типа данных, которые в него передаются. В пакетной обработке данных существует разбиение данных по каким-либо диапазонам, обычно по временным диапазонам. В таком подходе данные обычно доставляются с задержкой. Для пакетной обработки характерны простая (относительная) разработка и тестирование, а также высокая эффективность для OLAP-систем и высокая пиковая нагрузка на железо.

**Пакетная обработка** - классический подход в построении DWH. Обычно хранилище данных строится за t−1, то есть данные в хранилище актуальны за вчерашний день. Пакетная обработка может быть и за последний час, и за предыдущие полчаса; это зависит от частоты жизни хранилища данных.

<img width="1274" height="227" alt="image" src="https://github.com/user-attachments/assets/dffa4770-ce2c-4922-984d-3f5e9a568011" />

# **Потоковая обработка**

**Потоковая обработка** - сервис обрабатывает и загружает весь поток информации, результат получается в режиме реального времени. Потоковая обработка сложнее в разработке и тестировании (относительно) чем пакетная. Для нее характерны низкая эффективность для OLAP-систем и равномерная нагрузка на железо.

<img width="1203" height="236" alt="image" src="https://github.com/user-attachments/assets/a4fc97da-b07c-444a-99c7-ef8164b88e7c" />

# **Lambda-архитектура**

Если мы объединим потоковую и пакетную обработку, то получим Lambda-архитектуру. У Lambda-архитектуры очень простой подход: мы делим общий поток данных на два потока. Первый поток — это пакетная обработка (Batch layer), а второй поток — это потоковая обработка (Real-time layer).

В **Batch layer** представлены Primary data layer и Core layer из классического DWH. Затем данные из Batch layer попадают в Serving layer, где находится витрина данных. В Real-time layer появляются представления Real-time View, которые попадают в Serving layer и к которым могут обращаться аналитики.

У Lambda-архитектуры есть свой минус: нам необходимо дублировать логику в оба потока обработки данных. Если нам не нужна пакетная обработка, мы можем убрать её из архитектуры.

<img width="1300" height="590" alt="image" src="https://github.com/user-attachments/assets/c7b9b499-f691-4db6-a181-5509fcbddd25" />

# **Kappa-архитектура**

Kappa-архитектура — это архитектура только потоковой обработки данных. При этом есть возможность сохранять данные из Serving layer в долговременное хранилище.

<img width="1295" height="500" alt="image" src="https://github.com/user-attachments/assets/f65a2bfe-008d-4708-83ab-db940f316921" />

## Подходы к проектированию DWH

В зависимости от наличия центрального слоя существует два основополагающих подхода:

**DWH** – это корпоративное централизованное хранилище данных

# **DWH по Инмону**

-Проектирование ХД модели “сверху вниз”.

-Тщательный анализ бизнеса в целом.

-Выявление бизнес-областей.

-Определение ключевых бизнес-сущностей.

-Определение их характеристик (атрибутов) и связей между ними.

В результате анализа появляется понимание, какие сущности участвуют в бизнес-процессах и как они взаимодействуют друг с другом.

**Пример DWH по Инмону**

<img width="1388" height="772" alt="image" src="https://github.com/user-attachments/assets/6c151adc-437f-461c-82f2-10d0a1024325" />

**Преимущества:**

-«Единая версия правды».

-Отсутствие противоречивости в данных.

-Детальный слой содержит проекцию бизнес-процессов.

-Легкость поддержки при увеличении количества источников.

**Недостатки:**

-Сложность в проектировании, требуется высококлассная команда.

-Долгая реализация на первоначальном этапе анализа бизнеса.

# **DWH по Кимбаллу**

DWH по Кимбаллу – это копия транзакционных данных, специально структурированных для запроса и анализа в виде витрин данных. Хранилище по Кимбаллу можно назвать коллекцией витрин данных (отчетов).

-Проектирование снизу вверх.

-Анализ потребностей – определение необходимых отчетов.

-Анализ источников – идентификация доступных данных.

-Проектирование витрины под конкретного потребителя.

-Преобразование первичных данных из источников в витрины.

**Пример DWH по Кимбаллу**

<img width="1382" height="772" alt="image" src="https://github.com/user-attachments/assets/91b35343-d8b9-46c0-b618-b7f48f7594c4" />

**Преимущества:**

-Быстрый эффект.

-Достаточно поэтапного анализа бизнес-областей.

-Не требуется высококвалифицированных специалистов (на старте).

**Недостатки:**

-Высокая стоимость поддержки новых источников.

-Отсутствие стандартизации показателей (в каждой витрине может быть свой алгоритм).

# SCD

**SCD [Slowly Changing Dimensions]**

## **SCD 0**

**SCD 0** — заключается в том, что данные после первого попадания в таблицу далее никогда не изменяются. Этот метод практически никем не используется, т.к. он не поддерживает версионности. Он нужен лишь как нулевая точка отсчета для методологии SCD. По сути, вообще не SCD. Таблица, которая хранит пол родственников Дональда Дака - женский, мужской, не определено. Она также не требует ведения истории.

## **SCD 1**

**SCD 1** — это обычная перезапись старых данных новыми. В чистом виде этот метод тоже не содержит версионности и используется лишь там, где история фактически не нужна.

Пример: паспортные данные изменились и были перезаписаны

## **SCD 2**

**SCD 2** - есть два столбца. Первый столбец с датой, когда запись начала действовать. Вторая дата ставится 9999-01-01. Значит, что строка имеет актуальные данные. При обновлении данных, 9999-01-01 меняется на текущую дату и строка становится уже исторической. При этом новые данные появляются на следующей строке. Смотри пример

Пример:

| ID | Name           | Number | Team   | Date_start  | Date_end    |
|----|----------------|--------|--------|-------------|-------------|
| 1  | Marc Marquez   | 93     | Honda  | 2013-11-08  | 9999-01-01  |
| 2  | Valentino Rossi | 46    | Yamaha | 2010-11-07  | 9999-01-01  |
| 3  | Dani Pedrosa   | 26     | Honda  | 2014-11-08  | 2018-01-11  |
| 4  | Jorge Lorenzo  | 99     | Ducati | 2017-01-01  | 2019-01-01  |
| 5  | Jorge Lorenzo  | 99     | Honda  | 2019-01-02  | 9999-01-01  |

## **SCD 3**

**SCD 3** — В самой записи содержатся дополнительные поля для предыдущих значений атрибута. При получении новых данных, старые данные перезаписываются текущими значениями.

Пример:

| ID | Name           | Num | Previous_team | Current_team | Date_start  |
|----|----------------|-----|---------------|--------------|-------------|
| 1  | Marc Marquez   | 93  | NULL          | Honda        | 2013-11-08  |
| 2  | Valentino Rossi| 46  | NULL          | Yamaha       | 2010-11-07  |
| 3  | Dani Pedrosa   | 26  | NULL          | Honda        | 2014-11-08  |
| 4  | Jorge Lorenzo  | 99  | Ducati        | Honda        | 2019-01-02  |

## **SCD 4**

История изменений содержится в отдельной таблице: основная таблица всегда перезаписывается текущими данными с перенесением старых данных в другую таблицу. Обычно этот тип используют для аудита изменений или создания архивных таблиц.

Дальше есть еще 5 и 6 версии, но они являются уже просто комбинациями из выше перечисленных. Шарить за них не нужно. Да и о них мало кто знает вообще.
