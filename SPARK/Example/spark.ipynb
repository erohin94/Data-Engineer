{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02dbd35c-c05b-4e74-9cb5-a3fc2a0e5351",
   "metadata": {},
   "source": [
    "### Start SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fcf1a3f-7641-4f37-9935-eb1cd80bf098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/06 10:25:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Активные Spark сессии: http://4dab6efce3c1:4040\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"erohin_pyspark_local\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Активные Spark сессии:\", spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f81f1bb-00ec-4840-8998-0d2056e81e26",
   "metadata": {},
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2030acf5-9a3b-445b-87d3-9727ad4e3dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'work/data/customs_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e72cd71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c0|\n",
      "+--------------------+\n",
      "|month;country;cod...|\n",
      "|01/2016;IT;620469...|\n",
      "|01/2016;CN;900190...|\n",
      "|01/2016;BY;841430...|\n",
      "|01/2016;US;901850...|\n",
      "|01/2016;EE;902110...|\n",
      "|01/2016;FR;381600...|\n",
      "|01/2016;MX;852351...|\n",
      "|01/2016;JP;620452...|\n",
      "|01/2016;KR;611020...|\n",
      "|01/2016;KG;852713...|\n",
      "|01/2016;ZA;842123...|\n",
      "|01/2016;CN;851810...|\n",
      "|01/2016;TR;841790...|\n",
      "|01/2016;IT;390610...|\n",
      "|01/2016;CZ;870840...|\n",
      "|01/2016;ES;640419...|\n",
      "|01/2016;IT;940490...|\n",
      "|01/2016;UA;820780...|\n",
      "|01/2016;CN;330410...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(PATH).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23a89abd-da06-40ed-a1b3-706fc97aaf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+------+-----+--------+------+--------+-------------+-----------+--------------------+\n",
      "|  month|country|      code| value|netto|quantity|region|district|direction_eng|measure_eng|           load_date|\n",
      "+-------+-------+----------+------+-----+--------+------+--------+-------------+-----------+--------------------+\n",
      "|01/2016|     IT|6204695000|   131|    1|       7| 46000|      01|           IM|        ShT|2024-07-01T00:00:...|\n",
      "|01/2016|     CN|9001900009|112750|   18|       0| 46000|      01|           IM|          1|2024-01-01T00:00:...|\n",
      "|01/2016|     BY|8414302004|   392|   57|       8| 50000|      06|           IM|        ShT|2024-06-01T00:00:...|\n",
      "|01/2016|     US|9018509000| 54349|  179|       0| 40000|      02|           IM|          1|2024-04-01T00:00:...|\n",
      "|01/2016|     EE|9021101000| 17304|  372|       0| 46000|      01|           IM|          1|2024-02-01T00:00:...|\n",
      "+-------+-------+----------+------+-----+--------+------+--------+-------------+-----------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(PATH, sep=';', header=True).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83700e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(PATH, sep=';', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01692be0-e017-4c0f-8f51-32a0c39d7439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+------+------+--------+------+--------+-------------+-----------+-----------------------------+\n",
      "|month  |country|code      |value |netto |quantity|region|district|direction_eng|measure_eng|load_date                    |\n",
      "+-------+-------+----------+------+------+--------+------+--------+-------------+-----------+-----------------------------+\n",
      "|01/2016|IT     |6204695000|131   |1     |7       |46000 |01      |IM           |ShT        |2024-07-01T00:00:00.000+03:00|\n",
      "|01/2016|CN     |9001900009|112750|18    |0       |46000 |01      |IM           |1          |2024-01-01T00:00:00.000+03:00|\n",
      "|01/2016|BY     |8414302004|392   |57    |8       |50000 |06      |IM           |ShT        |2024-06-01T00:00:00.000+03:00|\n",
      "|01/2016|US     |9018509000|54349 |179   |0       |40000 |02      |IM           |1          |2024-04-01T00:00:00.000+03:00|\n",
      "|01/2016|EE     |9021101000|17304 |372   |0       |46000 |01      |IM           |1          |2024-02-01T00:00:00.000+03:00|\n",
      "|01/2016|FR     |3816000000|323488|253600|0       |40000 |02      |IM           |1          |2024-02-01T00:00:00.000+03:00|\n",
      "|01/2016|MX     |8523519300|1611  |0     |4       |40000 |02      |IM           |ShT        |2024-04-01T00:00:00.000+03:00|\n",
      "|01/2016|JP     |6204520000|29    |1     |2       |46000 |01      |IM           |ShT        |2024-02-01T00:00:00.000+03:00|\n",
      "|01/2016|KR     |6110209100|815   |2     |5       |46000 |01      |IM           |ShT        |2024-01-01T00:00:00.000+03:00|\n",
      "|01/2016|KG     |8527139900|11868 |2127  |2630    |46000 |01      |IM           |ShT        |2024-06-01T00:00:00.000+03:00|\n",
      "|01/2016|ZA     |8421230000|12686 |1785  |3451    |45000 |01      |IM           |ShT        |2024-03-01T00:00:00.000+03:00|\n",
      "|01/2016|CN     |8518109500|12    |0     |10      |65000 |05      |IM           |ShT        |2024-03-01T00:00:00.000+03:00|\n",
      "|01/2016|TR     |8417900000|206453|17297 |1       |92000 |04      |IM           |ShT        |2024-06-01T00:00:00.000+03:00|\n",
      "|01/2016|IT     |3906100000|4492  |1075  |0       |45000 |01      |IM           |1          |2024-02-01T00:00:00.000+03:00|\n",
      "|01/2016|CZ     |8708409909|41    |2     |0       |46000 |01      |IM           |1          |2024-01-01T00:00:00.000+03:00|\n",
      "|01/2016|ES     |6404191000|11822 |346   |760     |45000 |01      |IM           |PAR        |2024-01-01T00:00:00.000+03:00|\n",
      "|01/2016|IT     |9404909000|6801  |485   |0       |46000 |01      |IM           |1          |2024-03-01T00:00:00.000+03:00|\n",
      "|01/2016|UA     |8207801900|35793 |1020  |0       |14000 |01      |IM           |1          |2024-02-01T00:00:00.000+03:00|\n",
      "|01/2016|CN     |3304100000|59678 |10829 |0       |46000 |01      |IM           |1          |2024-06-01T00:00:00.000+03:00|\n",
      "|01/2016|SI     |6104440000|1470  |13    |15      |45000 |01      |IM           |ShT        |2024-05-01T00:00:00.000+03:00|\n",
      "+-------+-------+----------+------+------+--------+------+--------+-------------+-----------+-----------------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False) # Выведет колонки с длинными значениями полностью, выше колонка load_date обрезана"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71cf4a66-846c-43b4-a6e6-d8227adc4b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------\n",
      " month         | 01/2016                       \n",
      " country       | IT                            \n",
      " code          | 6204695000                    \n",
      " value         | 131                           \n",
      " netto         | 1                             \n",
      " quantity      | 7                             \n",
      " region        | 46000                         \n",
      " district      | 01                            \n",
      " direction_eng | IM                            \n",
      " measure_eng   | ShT                           \n",
      " load_date     | 2024-07-01T00:00:00.000+03:00 \n",
      "-RECORD 1--------------------------------------\n",
      " month         | 01/2016                       \n",
      " country       | CN                            \n",
      " code          | 9001900009                    \n",
      " value         | 112750                        \n",
      " netto         | 18                            \n",
      " quantity      | 0                             \n",
      " region        | 46000                         \n",
      " district      | 01                            \n",
      " direction_eng | IM                            \n",
      " measure_eng   | 1                             \n",
      " load_date     | 2024-01-01T00:00:00.000+03:00 \n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "df.show(2, False, True) #(количество строк, не обрезать длинные строки, выводить каждую строку вертикально)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1caf03e-fc30-4bc3-ac05-19fbc6336538",
   "metadata": {},
   "source": [
    "### PrintSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21a90f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- code: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- netto: string (nullable = true)\n",
      " |-- quantity: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- direction_eng: string (nullable = true)\n",
      " |-- measure_eng: string (nullable = true)\n",
      " |-- load_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccf633ef-415f-417d-b3b8-fb56b5109029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['month',\n",
       " 'country',\n",
       " 'code',\n",
       " 'value',\n",
       " 'netto',\n",
       " 'quantity',\n",
       " 'region',\n",
       " 'district',\n",
       " 'direction',\n",
       " 'measure',\n",
       " 'load_date']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# withColumnRenamed(\"direction_eng\", \"direction\") возьми колонку и поменяй название\n",
    "result = (\n",
    "    df\n",
    "    .withColumnRenamed(\"direction_eng\", \"direction\")\n",
    "    .withColumnRenamed(\"measure_eng\", \"measure\")\n",
    ")\n",
    "\n",
    "result.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b58a01b-29c9-4a1e-b87a-9d9184d26671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- code: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- netto: string (nullable = true)\n",
      " |-- quantity: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- measure: string (nullable = true)\n",
      " |-- load_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8464d803",
   "metadata": {},
   "source": [
    "### Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c67966ef-90cd-42d4-a260-236b60220312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==============================================>          (13 + 3) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|country|\n",
      "+-------+\n",
      "|LT     |\n",
      "|MM     |\n",
      "|DZ     |\n",
      "|CI     |\n",
      "|TC     |\n",
      "|FI     |\n",
      "|SC     |\n",
      "|AZ     |\n",
      "|UA     |\n",
      "|RO     |\n",
      "+-------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result \\\n",
    "      .select('country')\\\n",
    "      .distinct()\\\n",
    "      .show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799d080a-b5e8-45cb-aec0-8e03eb5e5bb7",
   "metadata": {},
   "source": [
    "### GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d94e8c7-41df-4a64-a0db-e302111f05d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=============================================>          (13 + 3) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|country|total_rows|\n",
      "+-------+----------+\n",
      "|     BY|   3509568|\n",
      "|     KZ|   2519896|\n",
      "|     CN|   2454792|\n",
      "|     DE|   1542311|\n",
      "|     UA|   1158498|\n",
      "|     IT|   1102837|\n",
      "|     US|    835936|\n",
      "|     PL|    666690|\n",
      "|     FR|    593040|\n",
      "|     JP|    571756|\n",
      "|     TR|    463432|\n",
      "|     KR|    446907|\n",
      "|     GB|    443091|\n",
      "|     AM|    438705|\n",
      "|     CZ|    407360|\n",
      "|     KG|    403565|\n",
      "|     ES|    401644|\n",
      "|     IN|    374151|\n",
      "|     NL|    365193|\n",
      "|     UZ|    329707|\n",
      "+-------+----------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "(\n",
    "    result\n",
    "    .groupBy('country')\n",
    "    .agg(F.count('*').alias('total_rows'))\n",
    "    .orderBy(F.col('total_rows').desc())\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f564628-8bf3-45cb-95f2-6f80ed4cec6a",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "391feb76-afba-485b-8824-a512114c2c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Два варианта написании фильтрации\n",
    "df_de = (\n",
    "    result\n",
    "    .where(F.col('country') == 'DE')\n",
    "    .where(F.col('value').isNotNull())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9a02e78-4c08-4d94-8990-448306b2a83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+-----+-----+--------+------+--------+---------+-------+--------------------+\n",
      "|  month|country|      code|value|netto|quantity|region|district|direction|measure|           load_date|\n",
      "+-------+-------+----------+-----+-----+--------+------+--------+---------+-------+--------------------+\n",
      "|01/2016|     DE|4016995709| 5901|  172|       0| 46000|      01|       IM|      1|2024-01-01T00:00:...|\n",
      "|01/2016|     DE|8708809109| 1213|   94|       0| 45000|      01|       IM|      1|2024-01-01T00:00:...|\n",
      "+-------+-------+----------+-----+-----+--------+------+--------+---------+-------+--------------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "df_de.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cd2a20f-8af3-439a-a487-6c7d3fbcc64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_de2 = (\n",
    "    result\n",
    "    .where(''' country == \"DE\" ''')\n",
    "    .where(''' value IS NOT NULL ''')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b357783-7572-4dfe-a613-6bb3e3a7a0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+-----+-----+--------+------+--------+---------+-------+--------------------+\n",
      "|  month|country|      code|value|netto|quantity|region|district|direction|measure|           load_date|\n",
      "+-------+-------+----------+-----+-----+--------+------+--------+---------+-------+--------------------+\n",
      "|01/2016|     DE|4016995709| 5901|  172|       0| 46000|      01|       IM|      1|2024-01-01T00:00:...|\n",
      "|01/2016|     DE|8708809109| 1213|   94|       0| 45000|      01|       IM|      1|2024-01-01T00:00:...|\n",
      "+-------+-------+----------+-----+-----+--------+------+--------+---------+-------+--------------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "df_de2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c97ce1a7-8ec3-43df-a84a-8f34b3ef8428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:=============================================>          (13 + 3) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Проверка что результат фильтрации при первом и втором варианте один и тот же\n",
    "print(df_de.count() == df_de2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da244f3-9084-4c9c-9857-e87e381d87e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "230b564c-86f8-4633-a5ff-5cf8da7b6eea",
   "metadata": {},
   "source": [
    "### Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7058846-f7b2-4a9e-9388-3a5a9ed26bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+-----+-----+--------+------+--------+---------+-------+-----------------------------+\n",
      "|month  |country|code      |value|netto|quantity|region|district|direction|measure|load_date                    |\n",
      "+-------+-------+----------+-----+-----+--------+------+--------+---------+-------+-----------------------------+\n",
      "|01/2016|DE     |4016995709|5901 |172  |0       |46000 |01      |IM       |1      |2024-01-01T00:00:00.000+03:00|\n",
      "|01/2016|DE     |8708809109|1213 |94   |0       |45000 |01      |IM       |1      |2024-01-01T00:00:00.000+03:00|\n",
      "+-------+-------+----------+-----+-----+--------+------+--------+---------+-------+-----------------------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "df_de.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "945d5e8c-c146-4221-9a29-9279d6911de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['month',\n",
       " 'country',\n",
       " 'code',\n",
       " 'value',\n",
       " 'netto',\n",
       " 'quantity',\n",
       " 'region',\n",
       " 'district',\n",
       " 'direction',\n",
       " 'measure',\n",
       " 'load_date']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_de.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d62869a2-9781-428d-9292-6775b5bd5b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+-----+-----+--------+------+--------+---------+-------+----------+\n",
      "|month  |country|code      |value|netto|quantity|region|district|direction|measure|load_date |\n",
      "+-------+-------+----------+-----+-----+--------+------+--------+---------+-------+----------+\n",
      "|01/2016|DE     |4016995709|5901 |172  |0       |46000 |01      |IM       |1      |2024-01-01|\n",
      "|01/2016|DE     |8708809109|1213 |94   |0       |45000 |01      |IM       |1      |2024-01-01|\n",
      "+-------+-------+----------+-----+-----+--------+------+--------+---------+-------+----------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "# В селекте можно CASTовать, приводить к определенным типам\n",
    "final = (\n",
    "    df_de\n",
    "    .select(\n",
    "        'month',\n",
    "        'country',\n",
    "        'code',\n",
    "        'value',\n",
    "        'netto',\n",
    "        'quantity',\n",
    "        'region',\n",
    "        'district',\n",
    "        'direction',\n",
    "        'measure',\n",
    "        F.col('load_date').cast('date'),\n",
    "    )\n",
    ")\n",
    "\n",
    "final.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd591868-1cd7-411e-b7f9-ce879bd15eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Сохранение неконтроллируемое по кол-ву файлов\n",
    "(\n",
    "    final\n",
    "    .write\n",
    "    .format('csv')\n",
    "    .options(header='True', sep=';')\n",
    "    .csv('data/final_no_control')\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd5c28bf-6fa5-4fac-bc4a-74e2999a25b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во партиций 16\n"
     ]
    }
   ],
   "source": [
    "# Узнать количество разделов\n",
    "partition_num = final.rdd.getNumPartitions()\n",
    "print(f'Кол-во партиций {partition_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bee1442e-127c-4fb1-9a58-fbb154b3d1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Сохранение контроллируемое по кол-ву файлов - ОДИН ФАЙЛ\n",
    "(\n",
    "    final\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .format('csv')\n",
    "    .options(header='True', sep=';')\n",
    "    .csv('data/final_one_file')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ceece9ad-3e23-48fd-8323-bfc62ce91e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во партиций 1\n"
     ]
    }
   ],
   "source": [
    "partition_num = final.coalesce(1).rdd.getNumPartitions()\n",
    "print(f'Кол-во партиций {partition_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f3bd27b4-8bfa-4215-8e3c-508bab2af1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Сохранения с партицированием\n",
    "(\n",
    "    final\n",
    "    .write\n",
    "    .partitionBy('load_date')\n",
    "    .format('csv')\n",
    "    .options(header='True', sep=';')\n",
    "    .csv('data/final_partitioned')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c0d6a11-ddf4-4e41-a480-e4767a2369b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:=============================================>          (13 + 3) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| load_date|\n",
      "+----------+\n",
      "|2024-08-01|\n",
      "|2024-09-01|\n",
      "|2024-10-01|\n",
      "|2024-04-01|\n",
      "|2024-05-01|\n",
      "|2024-07-01|\n",
      "|2024-02-01|\n",
      "|2024-03-01|\n",
      "|2024-01-01|\n",
      "|2024-06-01|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final.select('load_date').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "537a7810-3c7d-4afe-b7be-cd7d62739dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:=============================================>          (13 + 3) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load_date distinct: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print_df = final.select('load_date').distinct()\n",
    "print(f'Load_date distinct: {print_df.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8eff1058-85c4-4f3a-ad43-b3e69f41abab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Сохранения с партицированием и repartition внутри самой партиции\n",
    "(\n",
    "    final\n",
    "    .repartition(1, 'load_date')\n",
    "    .write\n",
    "    .partitionBy('load_date')\n",
    "    .format('csv')\n",
    "    .options(header='True', sep=';')\n",
    "    .csv('data/final_partitioned_repart')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af3ddd4b-cd95-4e86-a1c7-cb6cec13cd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:=============================================>          (13 + 3) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во партиций 1\n"
     ]
    }
   ],
   "source": [
    "partition_num = final.repartition(1, 'load_date').rdd.getNumPartitions()\n",
    "print(f'Кол-во партиций {partition_num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f71e8-ab17-4104-85d1-fc55f3f9a43f",
   "metadata": {},
   "source": [
    "### Read Transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e766b95-40b7-46b6-93f1-091a45974d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350998"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader_no_control = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv('data/final_no_control/', header=True, sep=';')\n",
    "    .where(''' load_date = \"2024-01-01\" ''')\n",
    ")\n",
    "\n",
    "reader_no_control.count() # number of files read: 16 | size of files read: 88.4 MiB | 4 s (132 ms, 289 ms, 362 ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2eea3b2-71d8-4152-b6f8-f172574b3f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350998"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader_final_one_file = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv('data/final_one_file/', header=True, sep=';')\n",
    "    .where(''' load_date = \"2024-01-01\" ''')\n",
    ")\n",
    "\n",
    "reader_final_one_file.count() # number of files read: 1 | size of files read: 88.4 MiB | 4.2 s (202 ms, 361 ms, 380 ms )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "438e0e1e-b09d-4242-b7b5-a9e63009dbed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350998"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader_partitioned = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv('data/final_partitioned', header=True, sep=';')\n",
    "    .where(''' load_date = \"2024-01-01\" ''')\n",
    ")\n",
    "\n",
    "reader_partitioned.count() # number of files read: 16 | size of files read: 16.4 MiB | 1.5 s (65 ms, 175 ms, 200 ms )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70e3d82e-b20d-4e3d-8ad9-e752662da308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350998"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader_partitioned_repart = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv('data/final_partitioned_repart', header=True, sep=';')\n",
    "    .where(''' load_date = \"2024-01-01\" ''')\n",
    ")\n",
    "\n",
    "reader_partitioned_repart.count() # number of files read: 1 | size of files read: 16.4 MiB | 199 ms (8 ms, 38 ms, 77 ms )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022f5468",
   "metadata": {},
   "source": [
    "### JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e4f7e85-2256-49f0-98b1-c88e0cb4da79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|region_id|       name|\n",
      "+---------+-----------+\n",
      "|    14000|   Северный|\n",
      "|    11000|      Южный|\n",
      "|    10000|  Восточный|\n",
      "|    26000|   Западный|\n",
      "|    56000|Центральный|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (14000, \"Северный\"),\n",
    "    (11000, \"Южный\"),\n",
    "    (10000, \"Восточный\"),\n",
    "    (26000, \"Западный\"),\n",
    "    (56000, \"Центральный\"),\n",
    "]\n",
    "\n",
    "region_df = spark.createDataFrame(data, schema='region_id long, name string')\n",
    "\n",
    "region_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7d9203a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+------+-----+--------+------+--------+-------------+-----------+--------------------+\n",
      "|  month|country|      code| value|netto|quantity|region|district|direction_eng|measure_eng|           load_date|\n",
      "+-------+-------+----------+------+-----+--------+------+--------+-------------+-----------+--------------------+\n",
      "|01/2016|     IT|6204695000|   131|    1|       7| 46000|      01|           IM|        ShT|2024-07-01T00:00:...|\n",
      "|01/2016|     CN|9001900009|112750|   18|       0| 46000|      01|           IM|          1|2024-01-01T00:00:...|\n",
      "+-------+-------+----------+------+-----+--------+------+--------+-------------+-----------+--------------------+\n",
      "only showing top 2 rows\n"
     ]
    }
   ],
   "source": [
    "customs_data = (\n",
    "    spark\n",
    "    .read\n",
    "    .csv(PATH, header=True, sep=';')\n",
    ")\n",
    "\n",
    "customs_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5939bc3f-1bb0-4240-afff-da26e9818381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отключим автоматический Broadcast JOIN\n",
    "import time\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42d681ee-cd9b-4caa-8dc3-6b1853f03a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 52:==================================>                       (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for join operation: 20.02 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Замерим выполнение запроса без broadcast join\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "customs_data.join(region_df, customs_data.region==region_df.region_id, \"left\").count()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Elapsed time for join operation: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b661180-bcac-439b-a2bd-df3b1f8573c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:=============================================>          (13 + 3) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for broadcast join operation: 12.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Замерим выполнение запроса c broadcast join\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "customs_data.join(F.broadcast(region_df), customs_data.region == region_df.region_id, \"left\").count()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Elapsed time for broadcast join operation: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c9bc17",
   "metadata": {},
   "source": [
    "### Cache | Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c6344db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[month: string, country: string, code: string, value: string, netto: string, quantity: string, region: string, district: string, direction_eng: string, measure_eng: string, load_date: string]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customs_data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec566c31-fc90-4d28-9c52-d72a208bf4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 11:42:07 WARN CacheManager: Asked to cache already cached data.\n",
      "25/12/06 11:42:25 WARN MemoryStore: Not enough space to cache rdd_228_10 in memory! (computed 34.3 MiB so far)\n",
      "25/12/06 11:42:25 WARN MemoryStore: Not enough space to cache rdd_228_6 in memory! (computed 35.2 MiB so far)\n",
      "25/12/06 11:42:25 WARN MemoryStore: Not enough space to cache rdd_228_9 in memory! (computed 34.2 MiB so far)\n",
      "25/12/06 11:42:25 WARN BlockManager: Persisting block rdd_228_9 to disk instead.\n",
      "25/12/06 11:42:25 WARN BlockManager: Persisting block rdd_228_10 to disk instead.\n",
      "25/12/06 11:42:25 WARN BlockManager: Persisting block rdd_228_6 to disk instead.\n",
      "25/12/06 11:42:25 WARN MemoryStore: Not enough space to cache rdd_228_5 in memory! (computed 33.6 MiB so far)\n",
      "25/12/06 11:42:25 WARN BlockManager: Persisting block rdd_228_5 to disk instead.\n",
      "25/12/06 11:42:32 WARN MemoryStore: Not enough space to cache rdd_228_3 in memory! (computed 45.3 MiB so far)\n",
      "25/12/06 11:42:32 WARN BlockManager: Persisting block rdd_228_3 to disk instead.\n",
      "25/12/06 11:42:32 WARN MemoryStore: Not enough space to cache rdd_228_1 in memory! (computed 44.6 MiB so far)\n",
      "25/12/06 11:42:32 WARN BlockManager: Persisting block rdd_228_1 to disk instead.\n",
      "25/12/06 11:42:35 WARN MemoryStore: Not enough space to cache rdd_228_10 in memory! (computed 19.8 MiB so far)\n",
      "25/12/06 11:42:35 WARN MemoryStore: Not enough space to cache rdd_228_5 in memory! (computed 5.2 MiB so far)\n",
      "25/12/06 11:42:37 WARN MemoryStore: Not enough space to cache rdd_228_3 in memory! (computed 9.9 MiB so far)\n",
      "25/12/06 11:42:37 WARN MemoryStore: Not enough space to cache rdd_228_1 in memory! (computed 10.5 MiB so far)\n",
      "25/12/06 11:42:37 WARN MemoryStore: Not enough space to cache rdd_228_15 in memory! (computed 5.2 MiB so far)\n",
      "25/12/06 11:42:37 WARN BlockManager: Persisting block rdd_228_15 to disk instead.\n",
      "25/12/06 11:42:38 WARN MemoryStore: Not enough space to cache rdd_228_14 in memory! (computed 5.2 MiB so far)\n",
      "25/12/06 11:42:38 WARN BlockManager: Persisting block rdd_228_14 to disk instead.\n",
      "25/12/06 11:42:39 WARN MemoryStore: Not enough space to cache rdd_228_13 in memory! (computed 9.5 MiB so far)\n",
      "25/12/06 11:42:39 WARN BlockManager: Persisting block rdd_228_13 to disk instead.\n",
      "25/12/06 11:42:39 WARN MemoryStore: Not enough space to cache rdd_228_12 in memory! (computed 9.9 MiB so far)\n",
      "25/12/06 11:42:39 WARN BlockManager: Persisting block rdd_228_12 to disk instead.\n",
      "25/12/06 11:42:43 WARN MemoryStore: Not enough space to cache rdd_228_15 in memory! (computed 19.9 MiB so far)\n",
      "25/12/06 11:42:48 WARN MemoryStore: Not enough space to cache rdd_228_12 in memory! (computed 5.0 MiB so far)\n",
      "25/12/06 11:42:48 WARN MemoryStore: Not enough space to cache rdd_228_13 in memory! (computed 4.8 MiB so far)\n",
      "25/12/06 11:42:48 WARN MemoryStore: Not enough space to cache rdd_228_14 in memory! (computed 20.2 MiB so far)\n",
      "25/12/06 11:42:48 WARN MemoryStore: Not enough space to cache rdd_228_10 in memory! (computed 5.0 MiB so far)\n",
      "25/12/06 11:42:48 WARN MemoryStore: Not enough space to cache rdd_228_5 in memory! (computed 5.2 MiB so far)\n",
      "25/12/06 11:42:48 WARN MemoryStore: Not enough space to cache rdd_228_1 in memory! (computed 10.5 MiB so far)\n",
      "25/12/06 11:42:48 WARN MemoryStore: Not enough space to cache rdd_228_3 in memory! (computed 5.1 MiB so far)\n",
      "25/12/06 11:42:49 WARN MemoryStore: Not enough space to cache rdd_228_13 in memory! (computed 9.5 MiB so far)\n",
      "25/12/06 11:42:49 WARN MemoryStore: Not enough space to cache rdd_228_12 in memory! (computed 9.9 MiB so far)\n",
      "25/12/06 11:42:49 WARN MemoryStore: Not enough space to cache rdd_228_15 in memory! (computed 5.2 MiB so far)\n",
      "25/12/06 11:42:49 WARN MemoryStore: Not enough space to cache rdd_228_14 in memory! (computed 20.2 MiB so far)\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26392290"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customs_data.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2be3547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[month: string, country: string, code: string, value: string, netto: string, quantity: string, region: string, district: string, direction_eng: string, measure_eng: string, load_date: string]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customs_data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eedfda2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26392290"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "customs_data.persist(StorageLevel.DISK_ONLY).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9566a5",
   "metadata": {},
   "source": [
    "### Repartition & Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "39c4a394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|number|\n",
      "+---+------+\n",
      "|  1|   one|\n",
      "|  2|   two|\n",
      "|  3| three|\n",
      "|  4|  four|\n",
      "|  5|  five|\n",
      "|  6|   six|\n",
      "|  7| seven|\n",
      "|  8| eight|\n",
      "|  9|  nine|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1,'one'),\n",
    "    (2,'two'),\n",
    "    (3,'three'),\n",
    "    (4,'four'),\n",
    "    (5,'five'),\n",
    "    (6,'six'),\n",
    "    (7, 'seven'),\n",
    "    (8, 'eight'),\n",
    "    (9, 'nine'),\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, ['id', 'number'])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9195556d-4c80-4b70-8407-7abde1732c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Когда только создали датафрейм у нас 12 разделов\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "97fb32db-5f55-4bce-b01d-07b6a9991d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [Row(id=1, number='one')],\n",
       " [Row(id=2, number='two')],\n",
       " [Row(id=3, number='three')],\n",
       " [],\n",
       " [Row(id=4, number='four')],\n",
       " [Row(id=5, number='five')],\n",
       " [Row(id=6, number='six')],\n",
       " [],\n",
       " [Row(id=7, number='seven')],\n",
       " [Row(id=8, number='eight')],\n",
       " [Row(id=9, number='nine')]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Покажи эти разделы, в каждом разделе список с элементами, но так же есть и пустые\n",
    "df.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "44e33bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Row(id=2, number='two'), Row(id=4, number='four')],\n",
       " [],\n",
       " [Row(id=1, number='one'), Row(id=8, number='eight')],\n",
       " [],\n",
       " [Row(id=6, number='six')],\n",
       " [],\n",
       " [Row(id=3, number='three'),\n",
       "  Row(id=7, number='seven'),\n",
       "  Row(id=9, number='nine')],\n",
       " [Row(id=5, number='five')]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Намеренно перемешаем и поделим на 8 разделов\n",
    "mix = df.repartition(8)\n",
    "mix.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "077c2346-2a84-4a5a-80f9-01f0a3b5bb02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Row(id=2, number='two'), Row(id=4, number='four'), Row(id=5, number='five')],\n",
       " [Row(id=1, number='one'), Row(id=8, number='eight')],\n",
       " [Row(id=6, number='six')],\n",
       " [Row(id=3, number='three'),\n",
       "  Row(id=7, number='seven'),\n",
       "  Row(id=9, number='nine')]]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Намеренно перемешаем и поделим на 4 раздела, видим пустых списков нет\n",
    "mix_2 = df.repartition(4)\n",
    "mix_2.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "978ee041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Row(id=3, number='three'),\n",
       "  Row(id=5, number='five'),\n",
       "  Row(id=7, number='seven'),\n",
       "  Row(id=8, number='eight'),\n",
       "  Row(id=9, number='nine')],\n",
       " [Row(id=1, number='one')],\n",
       " [Row(id=2, number='two'), Row(id=4, number='four'), Row(id=6, number='six')]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix.repartition(3).rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1eb8bba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Row(id=2, number='two'), Row(id=4, number='four')],\n",
       " [Row(id=1, number='one'),\n",
       "  Row(id=8, number='eight'),\n",
       "  Row(id=3, number='three'),\n",
       "  Row(id=7, number='seven'),\n",
       "  Row(id=9, number='nine')],\n",
       " [Row(id=6, number='six'), Row(id=5, number='five')]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix.coalesce(3).rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255abe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Еще вариант как можно вывести датафрейм, можем сделать из него пандас датафрейм\n",
    "# С большими данными не работает, делает соединение со всех экзекуторов в один, перекидывает все на драйвер и может вызвать OUT OF MEMORY\n",
    "mix.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e010c0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/06 12:46:27 ERROR Utils: uncaught error in thread Spark Context Cleaner, stopping SparkContext\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/12/06 12:46:27 ERROR Utils: uncaught error in thread spark-listener-group-appStatus, stopping SparkContext\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.status.LiveEntityHelpers$.createMetrics(LiveEntity.scala:789)\n",
      "\tat org.apache.spark.status.LiveEntityHelpers$.makeNegative(LiveEntity.scala:895)\n",
      "\tat org.apache.spark.status.LiveTask.doUpdate(LiveEntity.scala:207)\n",
      "\tat org.apache.spark.status.LiveEntity.write(LiveEntity.scala:50)\n",
      "\tat org.apache.spark.status.AppStatusListener.update(AppStatusListener.scala:1233)\n",
      "\tat org.apache.spark.status.AppStatusListener.maybeUpdate(AppStatusListener.scala:1239)\n",
      "\tat org.apache.spark.status.AppStatusListener.$anonfun$onExecutorMetricsUpdate$6(AppStatusListener.scala:976)\n",
      "\tat org.apache.spark.status.AppStatusListener.$anonfun$onExecutorMetricsUpdate$6$adapted(AppStatusListener.scala:976)\n",
      "\tat org.apache.spark.status.AppStatusListener$$Lambda/0x00007e3b8cb8ae80.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n",
      "\tat org.apache.spark.status.AppStatusListener.flush(AppStatusListener.scala:1034)\n",
      "\tat org.apache.spark.status.AppStatusListener.onExecutorMetricsUpdate(AppStatusListener.scala:976)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:59)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:38)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:38)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:118)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:102)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:106)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:106)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$Lambda/0x00007e3b8c565a80.apply$mcJ$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:97)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2$$Lambda/0x00007e3b8c5657a0.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1279)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:97)\n",
      "25/12/06 12:46:27 ERROR Utils: throw uncaught fatal error in thread Spark Context Cleaner\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"Spark Context Cleaner\" java.lang.OutOfMemoryError: Java heap space\n",
      "25/12/06 12:46:27 ERROR Executor: Exception in task 6.0 in stage 112.0 (TID 680)\n",
      "scala.MatchError: java.lang.OutOfMemoryError: Java heap space (of class java.lang.OutOfMemoryError)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/12/06 12:46:27 ERROR Executor: Exception in task 9.0 in stage 112.0 (TID 683)\n",
      "scala.MatchError: java.lang.OutOfMemoryError: Java heap space: failed reallocation of scalar replaced objects (of class java.lang.OutOfMemoryError)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/12/06 12:46:27 ERROR Executor: Exception in task 3.0 in stage 112.0 (TID 677)\n",
      "scala.MatchError: java.lang.OutOfMemoryError: Java heap space (of class java.lang.OutOfMemoryError)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.FileDataSourceV2$.attachFilePath(FileDataSourceV2.scala:127)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:142)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/12/06 12:46:27 ERROR Executor: Exception in task 7.0 in stage 112.0 (TID 681)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/12/06 12:46:27 ERROR Utils: throw uncaught fatal error in thread spark-listener-group-appStatus\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.status.LiveEntityHelpers$.createMetrics(LiveEntity.scala:789)\n",
      "\tat org.apache.spark.status.LiveEntityHelpers$.makeNegative(LiveEntity.scala:895)\n",
      "\tat org.apache.spark.status.LiveTask.doUpdate(LiveEntity.scala:207)\n",
      "\tat org.apache.spark.status.LiveEntity.write(LiveEntity.scala:50)\n",
      "\tat org.apache.spark.status.AppStatusListener.update(AppStatusListener.scala:1233)\n",
      "\tat org.apache.spark.status.AppStatusListener.maybeUpdate(AppStatusListener.scala:1239)\n",
      "\tat org.apache.spark.status.AppStatusListener.$anonfun$onExecutorMetricsUpdate$6(AppStatusListener.scala:976)\n",
      "\tat org.apache.spark.status.AppStatusListener.$anonfun$onExecutorMetricsUpdate$6$adapted(AppStatusListener.scala:976)\n",
      "\tat org.apache.spark.status.AppStatusListener$$Lambda/0x00007e3b8cb8ae80.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n",
      "\tat org.apache.spark.status.AppStatusListener.flush(AppStatusListener.scala:1034)\n",
      "\tat org.apache.spark.status.AppStatusListener.onExecutorMetricsUpdate(AppStatusListener.scala:976)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:59)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:38)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:38)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:118)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:102)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:106)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:106)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$Lambda/0x00007e3b8c565a80.apply$mcJ$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:97)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2$$Lambda/0x00007e3b8c5657a0.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1279)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:97)\n",
      "Exception in thread \"spark-listener-group-appStatus\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.status.LiveEntityHelpers$.createMetrics(LiveEntity.scala:789)\n",
      "\tat org.apache.spark.status.LiveEntityHelpers$.makeNegative(LiveEntity.scala:895)\n",
      "\tat org.apache.spark.status.LiveTask.doUpdate(LiveEntity.scala:207)\n",
      "\tat org.apache.spark.status.LiveEntity.write(LiveEntity.scala:50)\n",
      "\tat org.apache.spark.status.AppStatusListener.update(AppStatusListener.scala:1233)\n",
      "\tat org.apache.spark.status.AppStatusListener.maybeUpdate(AppStatusListener.scala:1239)\n",
      "\tat org.apache.spark.status.AppStatusListener.$anonfun$onExecutorMetricsUpdate$6(AppStatusListener.scala:976)\n",
      "\tat org.apache.spark.status.AppStatusListener.$anonfun$onExecutorMetricsUpdate$6$adapted(AppStatusListener.scala:976)\n",
      "\tat org.apache.spark.status.AppStatusListener$$Lambda/0x00007e3b8cb8ae80.apply(Unknown Source)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n",
      "\tat org.apache.spark.status.AppStatusListener.flush(AppStatusListener.scala:1034)\n",
      "\tat org.apache.spark.status.AppStatusListener.onExecutorMetricsUpdate(AppStatusListener.scala:976)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:59)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:38)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:38)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:118)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:102)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:106)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:106)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$Lambda/0x00007e3b8c565a80.apply$mcJ$sp(Unknown Source)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:97)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2$$Lambda/0x00007e3b8c5657a0.apply$mcV$sp(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1279)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:97)\n",
      "25/12/06 12:46:27 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#1610,Executor task launch worker for task 7.0 in stage 112.0 (TID 681),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "25/12/06 12:46:27 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda/0x00007e3b8d0145c0@6e73592 rejected from java.util.concurrent.ThreadPoolExecutor@36ca1f6d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 674]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2081)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:841)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1376)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:140)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:813)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:786)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:73)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:116)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/12/06 12:46:27 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@45823b7a rejected from java.util.concurrent.ThreadPoolExecutor@5399cf2b[Shutting down, pool size = 9, active threads = 9, queued tasks = 0, completed tasks = 677]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2081)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:841)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1376)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:383)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:97)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:95)\n",
      "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:95)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:116)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/12/06 12:46:27 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda/0x00007e3b8d0145c0@5aa15534 rejected from java.util.concurrent.ThreadPoolExecutor@36ca1f6d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 674]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2081)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:841)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1376)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:140)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:813)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:786)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:73)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:116)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/12/06 12:46:27 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@6c26da5e rejected from java.util.concurrent.ThreadPoolExecutor@5399cf2b[Shutting down, pool size = 9, active threads = 9, queued tasks = 0, completed tasks = 677]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2081)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:841)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1376)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:383)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:97)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:95)\n",
      "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:95)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:116)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/12/06 12:46:27 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda/0x00007e3b8d0145c0@74bf82c0 rejected from java.util.concurrent.ThreadPoolExecutor@36ca1f6d[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 674]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2081)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:841)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1376)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:140)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:813)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:786)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:73)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:116)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/12/06 12:46:27 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@6d78162a rejected from java.util.concurrent.ThreadPoolExecutor@5399cf2b[Shutting down, pool size = 9, active threads = 9, queued tasks = 0, completed tasks = 677]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2081)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:841)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1376)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:383)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:97)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:95)\n",
      "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:95)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:116)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/12/06 12:46:27 ERROR Inbox: Ignoring error\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.executor.Executor$TaskRunner@386bb55e rejected from java.util.concurrent.ThreadPoolExecutor@5399cf2b[Shutting down, pool size = 9, active threads = 9, queued tasks = 0, completed tasks = 677]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2081)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:841)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1376)\n",
      "\tat org.apache.spark.executor.Executor.launchTask(Executor.scala:383)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1(LocalSchedulerBackend.scala:97)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.$anonfun$reviveOffers$1$adapted(LocalSchedulerBackend.scala:95)\n",
      "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint.reviveOffers(LocalSchedulerBackend.scala:95)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:116)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o431.collectToPython.\n: org.apache.spark.SparkException: Job 87 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1301)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1299)\n\tat scala.collection.mutable.HashSet$Node.foreach(HashSet.scala:450)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:376)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1299)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3234)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:85)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:3120)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1300)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:3120)\n\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2346)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1300)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2346)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2297)\n\tat org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:2284)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:462)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$collectToPython$1(Dataset.scala:2057)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n\tat org.apache.spark.sql.classic.Dataset.collectToPython(Dataset.scala:2054)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# OUT OF MEMORY - Делаю падение по памяти\u001b[39;00m\n\u001b[1;32m      3\u001b[0m d \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(PATH, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/classic/dataframe.py:443\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Row]:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m--> 443\u001b[0m         sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o431.collectToPython.\n: org.apache.spark.SparkException: Job 87 cancelled because SparkContext was shut down\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1301)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1299)\n\tat scala.collection.mutable.HashSet$Node.foreach(HashSet.scala:450)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:376)\n\tat org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1299)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3234)\n\tat org.apache.spark.util.EventLoop.stop(EventLoop.scala:85)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:3120)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1300)\n\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:3120)\n\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2346)\n\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1300)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2346)\n\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2297)\n\tat org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:2284)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:462)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$collectToPython$1(Dataset.scala:2057)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\n\tat org.apache.spark.sql.classic.Dataset.collectToPython(Dataset.scala:2054)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 38976)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 535, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 566, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/accumulators.py\", line 299, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/accumulators.py\", line 275, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/serializers.py\", line 597, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# OUT OF MEMORY - Делаю падение по памяти\n",
    "\n",
    "d = spark.read.csv(PATH, header=True, sep='\\t')\n",
    "d.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb1f01e-5698-432a-b725-78ef3c063e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db51066-2a7a-4c2e-92b8-ba908bb1cae5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
