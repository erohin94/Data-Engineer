## Описание

Пример тестовых данных [customs_data.csv](https://github.com/erohin94/Data-Engineer/blob/main/SPARK/Example/customs_data.csv)

Полный набор можно скачать по [ссылке](https://huggingface.co/datasets/halltape/customs_data/resolve/main/customs_data.csv?download=true)

Настройка Spark, можно посмотреть [здесь](https://github.com/erohin94/Spark-Data-mart/blob/main/SETUP_V2.md) и [здесь](https://github.com/erohin94/Data-Engineer/tree/main/SPARK/%D0%A3%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BA%D0%B0)

## Apache Spark

Spark - фреймворк для работы с большими данными. Если данные не вмещаются на один компьютер. То можем посчитать их на кластере - на нескольких серверах, распределенно и паралельно. 

Spark работает в оперативке. 

Spark разбивает данные на кусочки (партиции) и распределяет по серверам. И каждый сервер считает свой кусок данных.

Пример:

Есть например файл `1GB` (csv, text, parquet или любой другой). Загружаем файл в компьютер, на котором есть pandas. Делаем трансформации (join, фильтрацию и тд) и сохраняем результат. Получаем агрегационный файл размером  `50МБ`. Один компьютер справился.

<img width="600" height="288" alt="image" src="https://github.com/user-attachments/assets/64bc8d00-bbae-4633-a60c-8fdfc25d9b73" />

Но может быть такое, что файл весит `100GB` и у нас нет столько ресурсов (оперативки, жесткого диска, памяти и тд). Поэтому pandas не сможет посчитать данный файл, так как нет мощностей и всего один компьютер. Тоесть не можем распаралелить и отдать часть расчетов другому компьютеру.

<img width="596" height="264" alt="image" src="https://github.com/user-attachments/assets/551f8ef5-6a95-4401-9160-290f6a632950" />

Поэтому есть Spark. Например есть файл размером `100GB` который spark условно делит на куски по `1GB` (базово делит на 128 МБ, но поделили по 1GB чтобы было удобнее считать). Каждый кусочек называется партиция, эта партиция летит на свой сервер (машину). И каждый сервер начинает обрабатывать этот маленький кусок (`1GB`). На серваке может быть несколько ядер, слотов оперативной памяти, жестких дисков. Все это обрабатывается (джоин, фильтр и тд). 

Если происходит джоин, то эти партици бегают из одного сервера, в другой. Где то собираются, где то фильтруются, убираются и тд (такая операция называется shuffle).

Далее когда spark говорим сохранить, то он сохраняет все эти файлы например в формате `txt` или может сохранить в S3, Hadoop, Greenplum. Тоесть отдали серверам посчитать, они посчитали и сохраняем на диск.

<img width="652" height="352" alt="image" src="https://github.com/user-attachments/assets/29165ff1-f227-4c36-a52a-52ed5eab5f61" />

Есть сервера (шкаф с полками (компьютерами)). У одного сервера(компьютера) есть процесор у которого есть ядра, так же у сервера есть оперативка, жесткий диск. Когда сервера обрабатывают данные, на каждом сервере запускается конкретный Java процесс (executor - исполнитель). Каждый Java процесс берет определенное количество ядер и оперативки. То есть использует оперативку сервера и ядра процессора именно этого сервера, где он запускается. 

Например на одном сервере запускаем два executor, и каждый executor использует для работы 2 ядра процессора и 6GB оперативки. Файл который мы нарезали и разбили на куски - это RDD, куски данных внутри спарк которые он понимает. Когда происходит джоин, мы должы соединить, отсортировать, т.е перекинуть RDD с одного executor на другой - это называют shuffle. Чем больше shuffle, тем хуже. Это дорогостоящая операция. 

Так же есть еще один исполнитель который называется Driver. Это тоже Java процесс который потребляет память. Он не хранит в себе RDD, он просто управляет executor. Говорит кому что считать и что делать.

<img width="681" height="363" alt="image" src="https://github.com/user-attachments/assets/c47e27f2-c0f5-4877-ac88-cfbe75542e72" />

Внутри это происходит след образом. В Hadooop есть ресурс менеджер Yarn. Тоесть запускаем Spark, запускается Driver, который обращается к Hadoop Yarn. И говорит ему, мне нужно 3 executor и дай им по одному ядру и 1GB оперативки. Yarn это делает и создает 3 executor и выделяет им необходимые ресурсы. Можно задать ресурсы статически, то есть сказал 3 ядра и 3 гига так и будет. Но можно и динамически, то есть например если ты не пользуешься спарк, то он убирает количество экзекуторов. А когда пользуюсь увеличивать.

<img width="562" height="674" alt="image" src="https://github.com/user-attachments/assets/d7721c84-e653-4af9-b7bb-1218623e710b" />

## Практика

Когда запустили спарк, в UI можно посмотреть executor. Сейчас всего один executor - driver.

<img width="1927" height="556" alt="image" src="https://github.com/user-attachments/assets/d978357a-a314-4696-a431-02b402a1bc5b" />

### Команда Select

Когда запускаю ячейку, то вижу следующее `[Stage 13:=============================================>          (13 + 3) / 16]`

<img width="822" height="414" alt="image" src="https://github.com/user-attachments/assets/1f8f337d-cd7f-4edc-ab53-d2849cac4d78" />

У меня датасет весит `2 ГБ`. `2048 / 128 = 16`

Тоесть получаем 16 разделов, партиций (тот момент когда дробим датафрейм на файлики партиции). Только у нас, он раздробил на 16 и считает на одном компьютере, не распределенно и не паралельно. Ядра считают последовательно друг за другом. 

В кластере эти 16 кусков считались бы паралельно и распределенно на разных серверах.

### Работа с модулем functions

Это основной набор трансформаций, аналог SQL-функций: агрегации, строки, даты, JSON, массивы, окна и т.д. 

`import pyspark.sql.functions as F`

В спарке надо импортировать функции так же как и библиотеки в python.

<img width="641" height="634" alt="image" src="https://github.com/user-attachments/assets/a5e37b95-19b4-4958-bfd1-8529c3056d0d" />

Каждый раз когда выполняем ячейку, спарк заново пересчитывает датафрейм. Чтобы каждый раз заново не пересчитывать, датафрейм можно закешировать, он один раз прочитает и сохранит в оперативной памяти. И будет его держать, пока ему не скажем освободить от него память. В результате расчеты будут быстрее. Если в оперативку не лезет, то будет скидывать на диск, в результате возникает много нюансов.

### Spark UI

Если зайти на вкладку `SQL / DataFrame` и кликнуть на `Description` в таблице.

<img width="1924" height="449" alt="image" src="https://github.com/user-attachments/assets/b2453e79-7b34-449d-bfa5-9e355150cf82" />

То увидим план выполнения запроса, тоесть визуально как спарк считает.

<img width="464" height="726" alt="image" src="https://github.com/user-attachments/assets/4837a551-b9df-44a5-aada-098c7147caea" />

### Сохранение на диск

После того как сделали преобразования, результат надо сохранить.

**Как сохраняет Spark?**

1) **Сохранение неконтроллируемое по кол-ву файлов.**

<img width="699" height="153" alt="image" src="https://github.com/user-attachments/assets/304779cb-5549-4b18-95c0-2c996923f137" />

```
(
    final
    .write
    .format('csv')
    .options(header='True', sep=';')
    .csv('data/final_no_control')
)
```   
`write` - запиши. `format` - в CSV. `options(Название колонок в первой строке, разделитель)`. `csv('data/final_no_control')` - путь куда сохранять.

Запускаем и видим как появилась папка `data/final_no_control` с файлами.

<img width="562" height="490" alt="image" src="https://github.com/user-attachments/assets/d5fd84db-4005-4495-8f71-de8f1288e7e9" />

Т.е запускаем, он заново(потому что не кешировали) все фильтрует, преобразовывает и тд. И только потом сохраняет.

В папке видим 16 CSV файлов. Это те самые кусочки, которые делили в начале. 

`.rdd.getNumPartitions()` - позволяет узнать количество разделов.

Когда делаем: `df.write.csv("path")` Spark сохраняет каждую партицию в отдельный файл. Если у датафрейм был распределен по 16 executor, т.е 16 партиций — будет:

```
part-00000.csv
part-00001.csv
...
part-00015.csv
```
Сколько партиций у датафрейма — столько файлов при сохранении.

Это логика работы Spark: он пишет результат так же, как он его распределил внутри себя.

2) **Сохранение контроллируемое по кол-ву файлов - ОДИН ФАЙЛ**

```
(
    final
    .coalesce(1)
    .write
    .format('csv')
    .options(header='True', sep=';')
    .csv('data/final_one_file')
)
```

Запускаем и видим как появилась папка `data/final_one_file` с одним файлом.

<img width="512" height="115" alt="image" src="https://github.com/user-attachments/assets/ca3b6064-7443-452f-9f83-a1c9814b7a63" />

Если нужно НЕ 16 файлов, а меньше — используем инструменты управления партициями: `coalesce(n)`.

Объединяет существующие партиции без shuffle. `df.coalesce(1).write.csv("path")` Получится один файл. Если написать `coalesce(4)`, Spark просто соберёт данные в 4 части, но размер файлов может быть неравномерный — Spark не перераспределяет данные. Плюсы: быстрый, без shuffle. Минусы: ненадёжно распределяет файлы по размеру.

Т.е перед тем как писать `write` делаем `coalesce(1)` и говорим, возьми все разделы, все рдд, все партиции со всех серверов, машин и соедини все в одно.

<img width="415" height="68" alt="image" src="https://github.com/user-attachments/assets/e0d6de6d-b2a3-49b8-b4be-5ee09813e79d" />

Так же есть еще `repartition(n)`. Полное перераспределение данных с shuffle.

`df.repartition(4).write.csv("path")` - Получится 4 файла, примерно одинакового размера. Плюсы: ровное распределение. Минусы: shuffle — тяжёлая операция

3) **Сохранения с партицированием.**

```
(
    final
    .write
    .partitionBy('load_date')
    .format('csv')
    .options(header='True', sep=';')
    .csv('data/final_partitioned')
)
```

Партиционирование при сохранении: `partitionBy`. Это уже про структуру папок в HDFS, а не про внутренние партиции Spark.

Например, у нас в таблице есть отчетные периоды и мы хотим сохранить данные по дате отчетного периода: `df.write.partitionBy("load_date").csv("path")`

Получится:

```
path/load_date=2025-01-01/
path/load_date=2025-01-02/
...
```

В каждой папке будет столько файлов, сколько партиций было у датафрейма. То есть если было 16 партиций → в каждой папке с датой будет 16 файлов.

Зачем так делают? Когда потом читаем данные: `spark.read.csv("path").where("load_date = '2025-01-01'")`. Spark смотрит только в папку с нужной датой, а не во весь датасет. Это ускоряет чтение и уменьшает нагрузку.

Проблема: слишком много файлов. Если у нас 10 дат × 16 csv файлов в каждой → уже 160 csv файлов. NameNode в HDFS не любит много маленьких файлов — тратит ресурсы на хранение метаданных.

Например у нас есть 10 дат.

<img width="789" height="509" alt="image" src="https://github.com/user-attachments/assets/cf8065ff-9352-4371-bb68-62196419c3b8" />

После запуска ячейки видим появилась папка `data/final_partitioned/` в которой много папок, это как раз те самые уникальные даты.

<img width="1072" height="335" alt="image" src="https://github.com/user-attachments/assets/a57c8dde-7236-4b7e-8e81-0abf2f7d999c" />

И внутри кажой папки с датой по 16 CSV файлов, для определенной даты

<img width="501" height="458" alt="image" src="https://github.com/user-attachments/assets/c6b02466-be86-4e4d-85c5-0e9895d1a018" />

Если бы закешировали на уровне трансформации, то спарк стартовал бы с измененного состояния датафрейма, а не так как сейчас с самого начала.


Так как NameNode в HDFS не любит много маленьких файлов — тратит ресурсы на хранение метаданных. То можно использовать следующий подход.

4) **Сохранение с партицированием и repartition внутри: partitionBy + repartition**

Чтобы в каждой папке была ровно одна партиция, делают так:

```
(
    final
    .repartition(1, 'load_date')
    .write
    .partitionBy('load_date')
    .format('csv')
    .options(header='True', sep=';')
    .csv('data/final_partitioned_repart')
)
```

Получим:

<img width="509" height="341" alt="image" src="https://github.com/user-attachments/assets/4cadb542-cbeb-423e-b699-006ff74ae359" />

И в каждой папке будет по одному файлу

<img width="506" height="90" alt="image" src="https://github.com/user-attachments/assets/a0eeda8d-74df-4258-8352-5367a1657d7d" />

Это разгружает NameNode в HDFS и она не будет перегружаться от метаданных по каждому блоку, так же это ускоряет последующее чтение, снижает количество мелких файлов.

**Про кеширование**

Если делаем: `df.cache()`. Например сделали все необходимые преобразования исходного датафрейма (группировку, фильтрацию, джоин и тд). Сохраняем этот датафрейм в переменную df и делаем кеширование. Spark сохранит преобразованный датафрейм в памяти и при последующих сохранениях не будет заново пересчитывать весь pipeline — фильтры, join’ы, groupBy.

Если не закешировать — Spark каждый раз берет исходный датафрейм и пересчитает всю цепочку трансформаций заново.

В зависимости от того как сохранили данные будет зависеть скорость чтения в дальнейшем и сам размер файла

## Чтение данных

1) **Читаю файл который сохранился безконтрольно**

Хочу прочитать файл с фильтрацией где столбец `load_date = "2024-01-01"`

```
reader_no_control = (
    spark
    .read
    .csv('data/final_no_control/', header=True, sep=';')
    .where(''' load_date = "2024-01-01" ''')
)

reader_no_control.count() # number of files read: 16 | size of files read: 88.4 MiB | 4 s (132 ms, 289 ms, 362 ms)
```

Говорим посчитай количество строк `reader_no_control.count()`. Он не сможет посчитать количество строк не прочтя датафрейм. Получим 350 998 строк. Далее переходим в Spark UI -> SQL/DataFrame. И открываем последний запрос.

<img width="1892" height="423" alt="image" src="https://github.com/user-attachments/assets/0174a967-2a90-41d3-8024-9ce4c7492dc8" />

Видим что прочитано 16 файлов, размер 88.4 MiB, 350 998 строк. `WholeStageCodegen (1) duration: total (min, med, max ) 4.0 s (132 ms, 289 ms, 362 ms )` - как долго это происходило (мин, средне, макс).

<img width="468" height="838" alt="image" src="https://github.com/user-attachments/assets/707cd0ac-9d53-4109-ab42-cbac84079571" />

<img width="366" height="866" alt="image" src="https://github.com/user-attachments/assets/62b764ff-7096-4fc0-8376-101a111ef1b5" />

2) **Читаю файл который сохранился один**

```
reader_final_one_file = (
    spark
    .read
    .csv('data/final_one_file/', header=True, sep=';')
    .where(''' load_date = "2024-01-01" ''')
)

reader_final_one_file.count() # number of files read: 1 | size of files read: 88.4 MiB | 4.2 s (202 ms, 361 ms, 380 ms )
```

Видим что прочитан действительно один файл. Так же видно что увеличилось время. Это так же долго как и предыдущий вариант, так как считываем весь набор данных для того чтобы получить `load_date = "2024-01-01"`.

<img width="452" height="228" alt="image" src="https://github.com/user-attachments/assets/6975941a-a9dc-4fa6-a18a-21ba22f74418" />

3) **Читаю файл который сохранился партицированно**

Это файл в котором есть разбивка по пакам с датами, скорее всего спарк должен идти в определенную папку и считывать только те данные которые в этой папке. Тоесть прочитай `read` датафрейм где `load_date = "2024-01-01"`. Тоесть должен зайти в папку `data/final_partitioned/load_date=2024-01-01/` и прочитать из нее все 16 файлов. И эти файлы должны весить намного меньше, так как всего 16 файлов из 88.4 мегабайт.

```
reader_partitioned = (
    spark
    .read
    .csv('data/final_partitioned', header=True, sep=';')
    .where(''' load_date = "2024-01-01" ''')
)

reader_partitioned.count() # number of files read: 16 | size of files read: 16.4 MiB | 1.5 s (65 ms, 175 ms, 200 ms )
```

Видим что прочитано действительно 16 файлов и размер уже 16.4 MiB и это заняло 1.5 s

<img width="462" height="270" alt="image" src="https://github.com/user-attachments/assets/f3dbc5ac-85ef-49c4-95cd-9f078aace0ce" />

4) **Читаю файл который сохранился партицированно и мы его еще склеили в один файл**

```
reader_partitioned_repart = (
    spark
    .read
    .csv('data/final_partitioned_repart', header=True, sep=';')
    .where(''' load_date = "2024-01-01" ''')
)

reader_partitioned_repart.count() # number of files read: 1 | size of files read: 16.4 MiB | 199 ms (8 ms, 38 ms, 77 ms )
```

<img width="468" height="274" alt="image" src="https://github.com/user-attachments/assets/ac628b13-d27d-4b2a-992f-c6f6024694ea" />

На таких данных малого размера для этих 4 подходов, разница не особо видна, но на больших данных это все вылезит в минуты, часы. Поэтому важно понимать как сохраняем. Например если читаем таблицу за 10 лет, мы не должны ее читать за 10 лет, мы должны ее читать по партициям.

### JOIN в Spark

В целом сдесь все так же как и в pandas, но есть некие моменты. Так как мы джоиним когда разбили свой датафрейм, он перекидывает все эти рдд между экзекуторами, смешивает, происходит шафл и тд. Это очень дорогостоящая операция. Важно когда джоиним большую таблицу и маленькую, например 2ГБ и 1МБ.

Создадим для примера датафрейм 

```
data = [
    (14000, "Северный"),
    (11000, "Южный"),
    (10000, "Восточный"),
    (26000, "Западный"),
    (56000, "Центральный"),
]

region_df = spark.createDataFrame(data, schema='region_id long, name string')

region_df.show()
```

И так же прочитаем customs_data.csv - 2GB 

```
customs_data = (
    spark
    .read
    .csv(PATH, header=True, sep=';')
)

customs_data.show(2)
```

И нам надо сделать джоин двух таблиц. Как это происходит, представим что мы взяли 2ГБ, разбили это все на 16 или больше партиций и все это лежит на экзекуторах. И тут приходит маленькая таблица и она тоже будет как то разбиваться, отправляться по частям на каждый экзекутор и тд. Чтобы этого не делать, проще можно поступить следующим образом, взять эту маленькую табличку и скопировать ее на каждый экзекутор, чтобы каждый экзекутор внутри у себя сджоинил кусок данных который у него есть от большого датафрейма с этой маленькой таблицей. Каждый экзекутор сджоинит у себя локально и потом вернет уже результат который нужен будет, то есть готовый. Такой джоин, когда мы копируем, называется **Broadcast Join**. По дефолту он включен в спарк, до 10МБ спарк включает Broadcast Join. Но так же мы можем его выключить.

Использование **Broadcast Join** намного ускоряет процес, нежели если бы мы разбили эту маленькую таблицу на разделы и раскинули бы ее на экзекуторы. Потом это все перемешивалось бы, искало друг другу пару по ключам и это было бы не рационально.     

Делаем замеры без Broadcast Join, т.е выключили его, джоиним как есть и с Broadcast Join, тоесть добавляем `F.broadcast(region_df)` - какую таблицу мы будем отправлять в копию броадкастом. Получим разницу в два раза. На больших данных это будет еще более заметно, например когда есть датафрейм на 500 ГБ и нам надо его сджоинить с маленьким словарем или еще чем то.

<img width="1265" height="576" alt="image" src="https://github.com/user-attachments/assets/f477619b-3a83-4a07-a5e1-fed7259e0592" />










