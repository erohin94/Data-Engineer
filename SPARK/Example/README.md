## Описание

Пример тестовых данных [customs_data.csv](https://github.com/erohin94/Data-Engineer/blob/main/SPARK/Example/customs_data.csv)

Полный набор можно скачать по [ссылке](https://huggingface.co/datasets/halltape/customs_data/resolve/main/customs_data.csv?download=true)

Настройка Spark, можно посмотреть [здесь](https://github.com/erohin94/Spark-Data-mart/blob/main/SETUP_V2.md) и [здесь](https://github.com/erohin94/Data-Engineer/tree/main/SPARK/%D0%A3%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BA%D0%B0)

## Apache Spark

Spark - фреймворк для работы с большими данными. Если данные не вмещаются на один компьютер. То можем посчитать их на кластере - на нескольких серверах, распределенно и паралельно. 

Spark работает в оперативке. 

Spark разбивает данные на кусочки (партиции) и распределяет по серверам. И каждый сервер считает свой кусок данных.

Пример:

Есть например файл `1GB` (csv, text, parquet или любой другой). Загружаем файл в компьютер, на котором есть pandas. Делаем трансформации (join, фильтрацию и тд) и сохраняем результат. Получаем агрегационный файл размером  `50МБ`. Один компьютер справился.

<img width="600" height="288" alt="image" src="https://github.com/user-attachments/assets/64bc8d00-bbae-4633-a60c-8fdfc25d9b73" />

Но может быть такое, что файл весит `100GB` и у нас нет столько ресурсов (оперативки, жесткого диска, памяти и тд). Поэтому pandas не сможет посчитать данный файл, так как нет мощностей и всего один компьютер. Тоесть не можем распаралелить и отдать часть расчетов другому компьютеру.

<img width="596" height="264" alt="image" src="https://github.com/user-attachments/assets/551f8ef5-6a95-4401-9160-290f6a632950" />

Поэтому есть Spark. Например есть файл размером `100GB` который spark условно делит на куски по `1GB` (базово делит на 128 МБ, но поделили по 1GB чтобы было удобнее считать). Каждый кусочек называется партиция, эта партиция летит на свой сервер (машину). И каждый сервер начинает обрабатывать этот маленький кусок (`1GB`). На серваке может быть несколько ядер, слотов оперативной памяти, жестких дисков. Все это обрабатывается (джоин, фильтр и тд). 

Если происходит джоин, то эти партици бегают из одного сервера, в другой. Где то собираются, где то фильтруются, убираются и тд (такая операция называется shuffle).

Далее когда spark говорим сохранить, то он сохраняет все эти файлы например в формате `txt` или может сохранить в S3, Hadoop, Greenplum. Тоесть отдали серверам посчитать, они посчитали и сохраняем на диск.

<img width="652" height="352" alt="image" src="https://github.com/user-attachments/assets/29165ff1-f227-4c36-a52a-52ed5eab5f61" />

Есть сервера (шкаф с полками (компьютерами)). У одного сервера(компьютера) есть процесор у которого есть ядра, так же у сервера есть оперативка, жесткий диск. Когда сервера обрабатывают данные, на каждом сервере запускается конкретный Java процесс (executor - исполнитель). Каждый Java процесс берет определенное количество ядер и оперативки. То есть использует оперативку сервера и ядра процессора именно этого сервера, где он запускается. 

Например на одном сервере запускаем два executor, и каждый executor использует для работы 2 ядра процессора и 6GB оперативки. Файл который мы нарезали и разбили на куски - это RDD, куски данных внутри спарк которые он понимает. Когда происходит джоин, мы должы соединить, отсортировать, т.е перекинуть RDD с одного executor на другой - это называют shuffle. Чем больше shuffle, тем хуже. Это дорогостоящая операция. 

Так же есть еще один исполнитель который называется Driver. Это тоже Java процесс который потребляет память. Он не хранит в себе RDD, он просто управляет executor. Говорит кому что считать и что делать.

<img width="681" height="363" alt="image" src="https://github.com/user-attachments/assets/c47e27f2-c0f5-4877-ac88-cfbe75542e72" />

Внутри это происходит след образом. В Hadooop есть ресурс менеджер Yarn. Тоесть запускаем Spark, запускается Driver, который обращается к Hadoop Yarn. И говорит ему, мне нужно 3 executor и дай им по одному ядру и 1GB оперативки. Yarn это делает и создает 3 executor и выделяет им необходимые ресурсы. Можно задать ресурсы статически, то есть сказал 3 ядра и 3 гига так и будет. Но можно и динамически, то есть например если ты не пользуешься спарк, то он убирает количество экзекуторов. А когда пользуюсь увеличивать.

<img width="562" height="674" alt="image" src="https://github.com/user-attachments/assets/d7721c84-e653-4af9-b7bb-1218623e710b" />

## Практика

Когда запустили спарк, в UI можно посмотреть executor. Сейчас всего один executor - driver.

<img width="1927" height="556" alt="image" src="https://github.com/user-attachments/assets/d978357a-a314-4696-a431-02b402a1bc5b" />

### Команда Select

Когда запускаю ячейку, то вижу следующее `[Stage 13:=============================================>          (13 + 3) / 16]`

<img width="822" height="414" alt="image" src="https://github.com/user-attachments/assets/1f8f337d-cd7f-4edc-ab53-d2849cac4d78" />

У меня датасет весит `2 ГБ`. `2048 / 128 = 16`

Тоесть получаем 16 разделов, партиций (тот момент когда дробим датафрейм на файлики партиции). Только у нас, он раздробил на 16 и считает на одном компьютере, не распределенно и не паралельно. Ядра считают последовательно друг за другом. 

В кластере эти 16 кусков считались бы паралельно и распределенно на разных серверах.

### Работа с модулем functions

Это основной набор трансформаций, аналог SQL-функций: агрегации, строки, даты, JSON, массивы, окна и т.д. 

`import pyspark.sql.functions as F`

В спарке надо импортировать функции так же как и библиотеки в python.

<img width="641" height="634" alt="image" src="https://github.com/user-attachments/assets/a5e37b95-19b4-4958-bfd1-8529c3056d0d" />

Каждый раз когда выполняем ячейку, спарк заново пересчитывает датафрейм. Чтобы каждый раз заново не пересчитывать, датафрейм можно закешировать, он один раз прочитает и сохранит в оперативной памяти. И будет его держать, пока ему не скажем освободить от него память. В результате расчеты будут быстрее. Если в оперативку не лезет, то будет скидывать на диск, в результате возникает много нюансов.

### Spark UI

Если зайти на вкладку `SQL / DataFrame` и кликнуть на `Description` в таблице.

<img width="1924" height="449" alt="image" src="https://github.com/user-attachments/assets/b2453e79-7b34-449d-bfa5-9e355150cf82" />

То увидим план выполнения запроса, тоесть визуально как спарк считает.

<img width="464" height="726" alt="image" src="https://github.com/user-attachments/assets/4837a551-b9df-44a5-aada-098c7147caea" />

### Сохранение на диск

После того как сделали преобразования, результат надо сохранить.

**Как сохраняет Spark?**

1) **Сохранение неконтроллируемое по кол-ву файлов.**

<img width="699" height="153" alt="image" src="https://github.com/user-attachments/assets/304779cb-5549-4b18-95c0-2c996923f137" />

```
(
    final
    .write
    .format('csv')
    .options(header='True', sep=';')
    .csv('data/final_no_control')
)
```   
`write` - запиши. `format` - в CSV. `options(Название колонок в первой строке, разделитель)`. `csv('data/final_no_control')` - путь куда сохранять.

Запускаем и видим как появилась папка `data/final_no_control` с файлами.

<img width="562" height="490" alt="image" src="https://github.com/user-attachments/assets/d5fd84db-4005-4495-8f71-de8f1288e7e9" />

Т.е запускаем, он заново(потому что не кешировали) все фильтрует, преобразовывает и тд. И только потом сохраняет.

В папке видим 16 CSV файлов. Это те самые кусочки, которые делили в начале. 

`.rdd.getNumPartitions()` - позволяет узнать количество разделов.

Когда делаем: `df.write.csv("path")` Spark сохраняет каждую партицию в отдельный файл. Если у датафрейм был распределен по 16 executor, т.е 16 партиций — будет:

```
part-00000.csv
part-00001.csv
...
part-00015.csv
```
Сколько партиций у датафрейма — столько файлов при сохранении.

Это логика работы Spark: он пишет результат так же, как он его распределил внутри себя.

2) **Сохранение контроллируемое по кол-ву файлов - ОДИН ФАЙЛ**

```
(
    final
    .coalesce(1)
    .write
    .format('csv')
    .options(header='True', sep=';')
    .csv('data/final_one_file')
)
```

Запускаем и видим как появилась папка `data/final_one_file` с одним файлом.

<img width="512" height="115" alt="image" src="https://github.com/user-attachments/assets/ca3b6064-7443-452f-9f83-a1c9814b7a63" />

Если нужно НЕ 16 файлов, а меньше — используем инструменты управления партициями: `coalesce(n)`.

Объединяет существующие партиции без shuffle. `df.coalesce(1).write.csv("path")` Получится один файл. Если написать `coalesce(4)`, Spark просто соберёт данные в 4 части, но размер файлов может быть неравномерный — Spark не перераспределяет данные. Плюсы: быстрый, без shuffle. Минусы: ненадёжно распределяет файлы по размеру.

Т.е перед тем как писать `write` делаем `coalesce(1)` и говорим, возьми все разделы, все рдд, все партиции со всех серверов, машин и соедини все в одно.

<img width="415" height="68" alt="image" src="https://github.com/user-attachments/assets/e0d6de6d-b2a3-49b8-b4be-5ee09813e79d" />

Так же есть еще `repartition(n)`. Полное перераспределение данных с shuffle.

`df.repartition(4).write.csv("path")` - Получится 4 файла, примерно одинакового размера. Плюсы: ровное распределение. Минусы: shuffle — тяжёлая операция

3) **Сохранения с партицированием.**

```
(
    final
    .write
    .partitionBy('load_date')
    .format('csv')
    .options(header='True', sep=';')
    .csv('data/final_partitioned')
)
```

Партиционирование при сохранении: `partitionBy`. Это уже про структуру папок в HDFS, а не про внутренние партиции Spark.

Например, у нас в таблице есть отчетные периоды и мы хотим сохранить данные по дате отчетного периода: `df.write.partitionBy("load_date").csv("path")`

Получится:

```
path/load_date=2025-01-01/
path/load_date=2025-01-02/
...
```

В каждой папке будет столько файлов, сколько партиций было у датафрейма. То есть если было 16 партиций → в каждой папке с датой будет 16 файлов.

Зачем так делают? Когда потом читаем данные: `spark.read.csv("path").where("load_date = '2025-01-01'")`. Spark смотрит только в папку с нужной датой, а не во весь датасет. Это ускоряет чтение и уменьшает нагрузку.

Проблема: слишком много файлов. Если у нас 10 дат × 16 csv файлов в каждой → уже 160 csv файлов. NameNode в HDFS не любит много маленьких файлов — тратит ресурсы на хранение метаданных.

Например у нас есть 10 дат.

<img width="789" height="509" alt="image" src="https://github.com/user-attachments/assets/cf8065ff-9352-4371-bb68-62196419c3b8" />

После запуска ячейки видим появилась папка `data/final_partitioned/` в которой много папок, это как раз те самые уникальные даты.

<img width="1072" height="335" alt="image" src="https://github.com/user-attachments/assets/a57c8dde-7236-4b7e-8e81-0abf2f7d999c" />

И внутри кажой папки с датой по 16 CSV файлов, для определенной даты

<img width="501" height="458" alt="image" src="https://github.com/user-attachments/assets/c6b02466-be86-4e4d-85c5-0e9895d1a018" />
