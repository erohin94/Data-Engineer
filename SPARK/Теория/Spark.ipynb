{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["bAVUle09sF4A","vB2uj24vr8ag","CmxJr2h1Uy_x","cySrErzWJAW4"],"authorship_tag":"ABX9TyOGIdZdKtTCTKdOvCCoc8t4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **Установка**"],"metadata":{"id":"bAVUle09sF4A"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_gHulmlwrW2h","executionInfo":{"status":"ok","timestamp":1760120021988,"user_tz":-180,"elapsed":414,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"46a90e95-a50a-47a2-b1dd-a25c17a1c96b"},"outputs":[{"output_type":"stream","name":"stdout","text":["pip 24.1.2 from /usr/local/lib/python3.12/dist-packages/pip (python 3.12)\n"]}],"source":["pip --version"]},{"cell_type":"code","source":["!pip install pyspark py4j"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8tJuJ2yWrbep","executionInfo":{"status":"ok","timestamp":1760120028062,"user_tz":-180,"elapsed":4631,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"74ee559c-1b4c-4181-aadb-5b6740ce4d06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n","Requirement already satisfied: py4j in /usr/local/lib/python3.12/dist-packages (0.10.9.7)\n"]}]},{"cell_type":"markdown","source":["## **Проверка работы установленного pyspark.**"],"metadata":{"id":"vB2uj24vr8ag"}},{"cell_type":"code","source":["from pyspark import SparkContext, SparkConf\n","\n","# Настройка Spark\n","conf = SparkConf().setAppName(\"Simple RDD Example\").setMaster(\"local[*]\")\n","sc = SparkContext(conf=conf)\n","\n","# 1. Создание RDD из списка чисел\n","numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n","rdd = sc.parallelize(numbers)\n","\n","# 2. Трансформации: Фильтрация чётных чисел\n","even_numbers_rdd = rdd.filter(lambda x: x % 2 == 0)\n","\n","# 3. Действие: Подсчёт суммы чётных чисел\n","sum_even_numbers = even_numbers_rdd.sum()\n","\n","# Вывод результата\n","print(\"Чётные числа:\", even_numbers_rdd.collect())\n","print(\"Сумма чётных чисел:\", sum_even_numbers)\n","\n","a = 5 + 6\n","print(a)\n","\n","# Остановка SparkContext\n","sc.stop()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MAAwhQX6rne5","executionInfo":{"status":"ok","timestamp":1759487523500,"user_tz":-180,"elapsed":2684,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"c1db5e2f-929c-467c-a140-060c1166a399"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Чётные числа: [2, 4, 6, 8, 10]\n","Сумма чётных чисел: 30\n","11\n"]}]},{"cell_type":"markdown","source":["## **Avro vs ORC vs Parquet**"],"metadata":{"id":"CmxJr2h1Uy_x"}},{"cell_type":"code","source":["# Запись данных в Avro. В colab не работает\n","\n","from pyspark.sql import SparkSession\n","\n","# Создание SparkSession\n","spark = SparkSession.builder.appName(\"Write Avro Example\").getOrCreate()\n","\n","# Пример данных\n","data = [(\"Alice\", 25), (\"Bob\", 30), (\"Cathy\", 29)]\n","df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n","\n","# Запись данных в Avro\n","df.write.format(\"avro\").save(\"path/to/output/avro\")"],"metadata":{"id":"8kbQ0LaHYHKL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Чтение данных из Avro\n","df_avro = spark.read.format(\"avro\").load(\"path/to/output/avro\")\n","df_avro.show()"],"metadata":{"id":"HKSoUQktZjMj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Запись данных в Parquet\n","\n","from pyspark.sql import SparkSession\n","\n","# Создание SparkSession\n","spark = SparkSession.builder.appName(\"Write Parquet Example\").getOrCreate()\n","\n","# Пример данных\n","data = [(\"Alice\", 25), (\"Bob\", 30), (\"Cathy\", 29)]\n","df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n","\n","# Запись данных в Parquet\n","df.write.parquet(\"path/to/output/parquet\")"],"metadata":{"id":"Q54_CZD1XjkM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Чтение данных из Parquet\n","df_parquet = spark.read.parquet(\"path/to/output/parquet\")\n","df_parquet.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QfYUL-oBZm5V","executionInfo":{"status":"ok","timestamp":1760121232270,"user_tz":-180,"elapsed":4525,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"c3c7c9dd-4e98-4282-f6bf-61b8de74d50b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+---+\n","| Name|Age|\n","+-----+---+\n","|Alice| 25|\n","|  Bob| 30|\n","|Cathy| 29|\n","+-----+---+\n","\n"]}]},{"cell_type":"code","source":["#Запись данных в ORC\n","\n","from pyspark.sql import SparkSession\n","\n","# Создание SparkSession\n","spark = SparkSession.builder.appName(\"Write ORC Example\").getOrCreate()\n","\n","# Пример данных\n","data = [(\"Alice\", 25), (\"Bob\", 30), (\"Cathy\", 29)]\n","df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n","\n","# Запись данных в ORC\n","df.write.orc(\"path/to/output/orc\")"],"metadata":{"id":"PILG7laXZbd7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Чтение данных из ORC\n","df_orc = spark.read.orc(\"path/to/output/orc\")\n","df_orc.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"67vD9VWyZrgL","executionInfo":{"status":"ok","timestamp":1760121458645,"user_tz":-180,"elapsed":430,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"2354deee-aec7-43eb-aac2-62e4d7273fbf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+---+\n","| Name|Age|\n","+-----+---+\n","|Alice| 25|\n","|  Bob| 30|\n","|Cathy| 29|\n","+-----+---+\n","\n"]}]},{"cell_type":"code","source":["spark.stop()"],"metadata":{"id":"Y_EGbgF3gv9c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Чтение файлов и запись в rdd/dataframe**"],"metadata":{"id":"X3EbJ5tGgYdO"}},{"cell_type":"markdown","source":["Чтение данных из файлов в Apache Spark является одним из наиболее часто выполняемых действий, так как Spark предназначен для работы с большими объемами данных, хранящихся в различных форматах. В Spark вы можете читать данные из различных источников, таких как HDFS, S3, локальная файловая система и другие. Spark поддерживает множество форматов файлов, включая текстовые файлы, CSV, JSON, Parquet, Avro и другие."],"metadata":{"id":"1FJV3nbKlNWc"}},{"cell_type":"markdown","source":["**Чтение данных с использованием RDD**"],"metadata":{"id":"xK9aDG-hly-_"}},{"cell_type":"code","source":["# Создание текстового файла\n","text_data = \"\"\"Hello world\n","Hello Spark\n","PySpark is amazing\n","Spark is fast\n","Hello PySpark\n","Data processing with Spark is fun\n","\"\"\"\n","\n","file_path = \"/content/text_file.txt\"\n","\n","with open(file_path, \"w\", encoding=\"utf-8\") as f:\n","    f.write(text_data)\n","\n","print(f\"Текстовый файл успешно создан: {file_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hi4y_-yolaLk","executionInfo":{"status":"ok","timestamp":1760124361674,"user_tz":-180,"elapsed":58,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"1ee996f3-5f0f-4cf5-aa3f-36a3095646ac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Текстовый файл успешно создан: /content/text_file.txt\n"]}]},{"cell_type":"code","source":["# Чтение данных с использованием RDD\n","\n","from pyspark import SparkContext\n","\n","# Создание SparkContext\n","sc = SparkContext(\"local\", \"Read Text File Example\").getOrCreate()\n","\n","# Чтение текстового файла\n","rdd = sc.textFile(\"/content/text_file.txt\")\n","\n","# Пример обработки данных\n","words = rdd.flatMap(lambda line: line.split(\" \"))\n","word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n","\n","# Вывод результата\n","print(word_counts.collect())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LmVF_kVcgZQz","executionInfo":{"status":"ok","timestamp":1760124400610,"user_tz":-180,"elapsed":2168,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"c7b424a2-5a9e-43e6-cb8c-e2e0ecfafaa1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('Hello', 3), ('world', 1), ('Spark', 3), ('PySpark', 2), ('is', 3), ('amazing', 1), ('fast', 1), ('Data', 1), ('processing', 1), ('with', 1), ('fun', 1)]\n"]}]},{"cell_type":"code","source":["sc.stop()"],"metadata":{"id":"gi88Mc9FielN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Чтение данных с использованием DataFrame**"],"metadata":{"id":"-liyUKu_l2od"}},{"cell_type":"code","source":["import csv\n","\n","# Данные для CSV\n","data = [\n","    [\"Name\", \"Age\", \"City\"],  # заголовки\n","    [\"Alice\", 25, \"Amsterdam\"],\n","    [\"Bob\", 30, \"Rotterdam\"],\n","    [\"Charlie\", 22, \"Utrecht\"],\n","    [\"Diana\", 28, \"The Hague\"],\n","    [\"Eve\", 35, \"Eindhoven\"]\n","]\n","\n","# Путь к файлу\n","file_path = \"/content/text_file_1.csv\"\n","\n","# Создание CSV-файла\n","with open(file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n","    writer = csv.writer(file)\n","    writer.writerows(data)\n","\n","print(f\"CSV-файл успешно создан: {file_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BHoRPeonmR2n","executionInfo":{"status":"ok","timestamp":1760124591316,"user_tz":-180,"elapsed":51,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"acb3daa0-eec9-49e2-f9d5-6e9c9855814a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CSV-файл успешно создан: /content/text_file_1.csv\n"]}]},{"cell_type":"code","source":["#Чтение данных с использованием DataFrame\n","\n","from pyspark.sql import SparkSession\n","\n","# Создание SparkSession\n","spark = SparkSession.builder.appName(\"Read CSV Example\").getOrCreate()\n","\n","# Чтение CSV-файла\n","df = spark.read.csv(\"/content/text_file_1.csv\", header=True, inferSchema=True)\n","\n","# Печать схемы DataFrame\n","df.printSchema()\n","\n","# Показ первых 5 строк\n","df.show(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4TdSBEaCikKD","executionInfo":{"status":"ok","timestamp":1760124605942,"user_tz":-180,"elapsed":1116,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"687d706d-10fa-4ed3-ee2f-19d703177cac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- Name: string (nullable = true)\n"," |-- Age: integer (nullable = true)\n"," |-- City: string (nullable = true)\n","\n","+-------+---+---------+\n","|   Name|Age|     City|\n","+-------+---+---------+\n","|  Alice| 25|Amsterdam|\n","|    Bob| 30|Rotterdam|\n","|Charlie| 22|  Utrecht|\n","|  Diana| 28|The Hague|\n","|    Eve| 35|Eindhoven|\n","+-------+---+---------+\n","\n"]}]},{"cell_type":"code","source":["spark.stop()"],"metadata":{"id":"prsaVyxvlSPN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Пример кода с использованием различных параметров**"],"metadata":{"id":"59b6LASWmJaM"}},{"cell_type":"code","source":["import csv\n","\n","# Данные для CSV\n","data = [\n","    [\"Name\", \"Age\", \"City\"],  # заголовки\n","    [\"Alice\", 25, \"Amsterdam\"],\n","    [\"Bob\", 30, \"Rotterdam\"],\n","    [\"Charlie\", 22, \"Utrecht\"],\n","    [\"Diana\", 28, \"The Hague\"],\n","    [\"Eve\", 35, \"Eindhoven\"]\n","]\n","\n","# Создание CSV-файла с разделителем ';'\n","with open(\"/content/text_file.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n","    writer = csv.writer(file, delimiter=\";\")\n","    writer.writerows(data)\n","\n","print(\"CSV-файл успешно создан!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LlmSTHeBjiNm","executionInfo":{"status":"ok","timestamp":1760124633756,"user_tz":-180,"elapsed":13,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"5b343455-2841-4331-9646-913228540345"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CSV-файл успешно создан!\n"]}]},{"cell_type":"code","source":["#Пример кода с использованием различных параметров\n","\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n","\n","spark = SparkSession.builder.appName(\"Read all Example\").getOrCreate()\n","\n","# Создание схемы для CSV-файла\n","schema = StructType([\n","    StructField(\"Name\", StringType(), True),\n","    StructField(\"Age\", IntegerType(), True),\n","    StructField(\"City\", StringType(), True)\n","])\n","\n","# Чтение CSV-файла с явной схемой и указанием разделителя\n","df_custom = spark.read.csv(\"/content/text_file.csv\", header=True, schema=schema, sep=\";\")\n","\n","# Печать схемы DataFrame\n","df_custom.printSchema()\n","\n","# Показ первых 5 строк\n","df_custom.show(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l_na6s2UjGi8","executionInfo":{"status":"ok","timestamp":1760124640381,"user_tz":-180,"elapsed":470,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"b02c7a7b-a5b8-4490-bcf4-03392402aead"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- Name: string (nullable = true)\n"," |-- Age: integer (nullable = true)\n"," |-- City: string (nullable = true)\n","\n","+-------+---+---------+\n","|   Name|Age|     City|\n","+-------+---+---------+\n","|  Alice| 25|Amsterdam|\n","|    Bob| 30|Rotterdam|\n","|Charlie| 22|  Utrecht|\n","|  Diana| 28|The Hague|\n","|    Eve| 35|Eindhoven|\n","+-------+---+---------+\n","\n"]}]},{"cell_type":"code","source":["spark.stop()"],"metadata":{"id":"NUt1y53RmxF0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Чтение JSON-файла**"],"metadata":{"id":"3CwNYwrAmtp-"}},{"cell_type":"code","source":["import json\n","\n","# Данные для JSON-файла\n","data = [\n","    {\"name\": \"Alice\", \"age\": 25, \"city\": \"Amsterdam\"},\n","    {\"name\": \"Bob\", \"age\": 35, \"city\": \"Rotterdam\"},\n","    {\"name\": \"Charlie\", \"age\": 40, \"city\": \"Utrecht\"},\n","    {\"name\": \"Diana\", \"age\": 28, \"city\": \"The Hague\"},\n","    {\"name\": \"Eve\", \"age\": 50, \"city\": \"Eindhoven\"}\n","]\n","\n","# Создание JSON-файла\n","with open(\"/content/jsonfile.json\", \"w\", encoding=\"utf-8\") as f:\n","    for record in data:\n","        f.write(json.dumps(record) + \"\\n\")  # JSON Lines формат (одна запись на строку)\n","\n","print(\"JSON-файл успешно создан!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CzkPSzvikqGs","executionInfo":{"status":"ok","timestamp":1760124663310,"user_tz":-180,"elapsed":16,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"85817e6e-f03b-4174-ab66-1639a04dc480"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["JSON-файл успешно создан!\n"]}]},{"cell_type":"code","source":["#Чтение JSON-файла\n","\n","from pyspark.sql import SparkSession\n","\n","# Создание SparkSession\n","spark = SparkSession.builder.appName(\"Read JSON Example\").getOrCreate()\n","\n","# Чтение JSON-файла\n","df_json = spark.read.json(\"/content/jsonfile.json\")\n","\n","# Печать схемы DataFrame\n","df_json.printSchema()\n","\n","# Показ первых 5 строк\n","df_json.show(5)\n","\n","# Пример обработки данных\n","df_filtered = df_json.filter(df_json[\"age\"] > 30)\n","df_filtered.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aNtveReUkiuN","executionInfo":{"status":"ok","timestamp":1760124677417,"user_tz":-180,"elapsed":1036,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"c854b224-893c-406f-c3af-5f8bbd1c0e5f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- age: long (nullable = true)\n"," |-- city: string (nullable = true)\n"," |-- name: string (nullable = true)\n","\n","+---+---------+-------+\n","|age|     city|   name|\n","+---+---------+-------+\n","| 25|Amsterdam|  Alice|\n","| 35|Rotterdam|    Bob|\n","| 40|  Utrecht|Charlie|\n","| 28|The Hague|  Diana|\n","| 50|Eindhoven|    Eve|\n","+---+---------+-------+\n","\n","+---+---------+-------+\n","|age|     city|   name|\n","+---+---------+-------+\n","| 35|Rotterdam|    Bob|\n","| 40|  Utrecht|Charlie|\n","| 50|Eindhoven|    Eve|\n","+---+---------+-------+\n","\n"]}]},{"cell_type":"markdown","source":["Параметры конфигурации при чтении файлов\n","\n","**header:** Указывает, содержит ли первый ряд файла имена столбцов (только для CSV).\n","\n","**inferSchema:** Автоматически определяет типы данных (только для CSV и JSON).\n","\n","**schema:** Явное указание схемы данных.\n","\n","**delimiter:** Указывает разделитель для значений (например, для CSV)."],"metadata":{"id":"Q_ygIDXem6Uk"}},{"cell_type":"markdown","source":["## **Параметр для автоматического определения типов данных при чтении CSV-файла в DataFrame**"],"metadata":{"id":"cySrErzWJAW4"}},{"cell_type":"markdown","source":["`inferSchema=True` — это параметр, который говорит Spark определить типы данных автоматически, а не считать всё строками (string)."],"metadata":{"id":"i11-M74wJGXQ"}},{"cell_type":"code","source":["# без inferSchema\n","\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName(\"InferSchemaExample\").getOrCreate()\n","\n","# Создаём пример CSV\n","data = \"\"\"name,age,salary\n","John,30,5000\n","Jane,35,6000\n","Mark,40,7000\n","\"\"\"\n","with open(\"/content/people.csv\", \"w\") as f:\n","    f.write(data)\n","\n","# Читаем CSV без инференса типов\n","df_no_schema = spark.read.csv(\"/content/people.csv\", header=True, inferSchema=False)\n","df_no_schema.printSchema()\n","df_no_schema.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mt5P4zcXJFVI","executionInfo":{"status":"ok","timestamp":1760200869173,"user_tz":-180,"elapsed":895,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"7d0c0818-f837-498d-f999-c11a4fdd2faa"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- name: string (nullable = true)\n"," |-- age: string (nullable = true)\n"," |-- salary: string (nullable = true)\n","\n","+----+---+------+\n","|name|age|salary|\n","+----+---+------+\n","|John| 30|  5000|\n","|Jane| 35|  6000|\n","|Mark| 40|  7000|\n","+----+---+------+\n","\n"]}]},{"cell_type":"code","source":["# с inferSchema=True\n","df_with_schema = spark.read.csv(\"people.csv\", header=True, inferSchema=True)\n","df_with_schema.printSchema()\n","df_with_schema.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7MoOQYQcJgGw","executionInfo":{"status":"ok","timestamp":1760200904950,"user_tz":-180,"elapsed":759,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"d63bcb2d-3ca5-47ee-ec1b-fe2bed23e885"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- name: string (nullable = true)\n"," |-- age: integer (nullable = true)\n"," |-- salary: integer (nullable = true)\n","\n","+----+---+------+\n","|name|age|salary|\n","+----+---+------+\n","|John| 30|  5000|\n","|Jane| 35|  6000|\n","|Mark| 40|  7000|\n","+----+---+------+\n","\n"]}]},{"cell_type":"markdown","source":["## **Spark SQL в PySpark**"],"metadata":{"id":"Qir_FLli0jxu"}},{"cell_type":"markdown","source":["**Пример использования DataFrame API**\n","\n","Предположим, есть JSON файл с информацией о людях. Нужно прочитать эти данные, выполнить различные операции, такие как фильтрация, группировка и агрегация, а затем сохранить результат в формате CSV.\n","\n","Таким образом, как такового SQL нету. То есть есть SQL-подобные функции, но их предоставляет сам Spark."],"metadata":{"id":"Z5y84nkt0w1u"}},{"cell_type":"code","source":["people =[\n","        {\"name\": \"John\", \"age\": 30, \"department\": \"HR\"},\n","        {\"name\": \"Doe\", \"age\": 25, \"department\": \"Finance\"},\n","        {\"name\": \"Jane\", \"age\": 35, \"department\": \"HR\"},\n","        {\"name\": \"Mark\", \"age\": 40, \"department\": \"Finance\"},\n","        {\"name\": \"Smith\", \"age\": 23, \"department\": \"Engineering\"}\n","        ]"],"metadata":{"id":"m25Xf1e40nrV","executionInfo":{"status":"ok","timestamp":1760195626027,"user_tz":-180,"elapsed":4,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["type(people)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dREfBxv81dAm","executionInfo":{"status":"ok","timestamp":1760195660408,"user_tz":-180,"elapsed":13,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"c1d0d8a7-ba8d-47c9-8f4b-c769d0f0e8d3"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["list"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["import json\n","\n","# Данные\n","people = [\n","    {\"name\": \"John\", \"age\": 30, \"department\": \"HR\"},\n","    {\"name\": \"Doe\", \"age\": 25, \"department\": \"Finance\"},\n","    {\"name\": \"Jane\", \"age\": 35, \"department\": \"HR\"},\n","    {\"name\": \"Mark\", \"age\": 40, \"department\": \"Finance\"},\n","    {\"name\": \"Smith\", \"age\": 23, \"department\": \"Engineering\"}\n","]\n","\n","with open(\"/content/people.json\", \"w\", encoding=\"utf-8\") as f:\n","    for person in people:\n","        f.write(json.dumps(person, ensure_ascii=False) + \"\\n\")"],"metadata":{"id":"BbCMXcbh-dgc","executionInfo":{"status":"ok","timestamp":1760198587000,"user_tz":-180,"elapsed":8,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","\n","# Создание SparkSession\n","spark = SparkSession.builder.appName(\"DataFrameAPIExample\").getOrCreate()\n","\n","# Чтение данных из JSON файла\n","df = spark.read.json(\"/content/people.json\")\n","\n","# Фильтрация данных\n","filtered_df = df.filter(col(\"age\") > 30)\n","\n","# Группировка и агрегация данных\n","grouped_df = df.groupBy(\"department\").agg({\"age\": \"avg\", \"name\": \"count\"}).withColumnRenamed(\"avg(age)\", \"avg_age\").withColumnRenamed(\"count(name)\", \"count\")\n","\n","# Сортировка данных\n","sorted_df = grouped_df.orderBy(col(\"count\").desc())\n","\n","# Показ результатов\n","filtered_df.show()\n","sorted_df.show()\n","\n","# Сохранение результирующего DataFrame в CSV файл\n","sorted_df.write.csv(\"/content/output.csv\", header=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KKZ0kh9h1EKZ","executionInfo":{"status":"ok","timestamp":1760198601195,"user_tz":-180,"elapsed":7069,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"cee1f481-b30b-40cd-ab78-aae9f62a29db"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+----------+----+\n","|age|department|name|\n","+---+----------+----+\n","| 35|        HR|Jane|\n","| 40|   Finance|Mark|\n","+---+----------+----+\n","\n","+-----------+-----+-------+\n","| department|count|avg_age|\n","+-----------+-----+-------+\n","|         HR|    2|   32.5|\n","|    Finance|    2|   32.5|\n","|Engineering|    1|   23.0|\n","+-----------+-----+-------+\n","\n"]}]},{"cell_type":"code","source":["spark.stop()"],"metadata":{"id":"VY6oaLVV1Ilu","executionInfo":{"status":"ok","timestamp":1760199657520,"user_tz":-180,"elapsed":112,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["**Как работает \"оригинальный SQL\" в PySpark.**\n","\n","**Описание задачи**\n","\n","Предположим, есть два JSON файла: один с информацией о людях, а другой с информацией о департаментах. Нам нужно прочитать эти данные, зарегистрировать их как временные таблицы, выполнить JOIN-запрос и сохранить результат в формате CSV.\n","\n","\n","\n","Что мы делаем.\n","\n","- Создаем объект SparkSession.\n","\n","- Читаем JSON файлы в DataFrame.\n","\n","- Регистрируем DataFrame как временные таблицы для выполнения SQL-запросов.\n","\n","- Выполняем SQL-запрос для соединения таблиц по идентификатору департамента.\n","\n","- Сохраняем результат в CSV файл.\n","\n","\n","Чтобы все время не сохранять промежуточные таблицы в hive, чтобы обратится к ним с spark.sql.\n","\n","Можно использовать `createOrReplaceTempView` - создать временную вьюху"],"metadata":{"id":"jGtan5SbCoCK"}},{"cell_type":"code","source":["# Пример данных (people.json)\n","\n","people = [\n","          {\"name\": \"John\", \"age\": 30, \"department_id\": 1},\n","          {\"name\": \"Doe\", \"age\": 25, \"department_id\": 2},\n","          {\"name\": \"Jane\", \"age\": 35, \"department_id\": 1},\n","          {\"name\": \"Mark\", \"age\": 40, \"department_id\": 2},\n","          {\"name\": \"Smith\", \"age\": 23, \"department_id\": 3}\n","          ]\n","\n","#Пример данных (departments.json)\n","\n","departments = [\n","              {\"id\": 1, \"department_name\": \"HR\"},\n","              {\"id\": 2, \"department_name\": \"Finance\"},\n","              {\"id\": 3, \"department_name\": \"Engineering\"}\n","              ]\n","\n","with open(\"/content/people.json\", \"w\", encoding=\"utf-8\") as f:\n","    for person in people:\n","        f.write(json.dumps(person, ensure_ascii=False) + \"\\n\")\n","\n","\n","with open(\"/content/departments.json\", \"w\", encoding=\"utf-8\") as f:\n","    for dep in departments:\n","        f.write(json.dumps(dep, ensure_ascii=False) + \"\\n\")\n","\n"],"metadata":{"id":"6xVkSJKYC3-y","executionInfo":{"status":"ok","timestamp":1760199287330,"user_tz":-180,"elapsed":9,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# Создание SparkSession\n","spark = SparkSession.builder.appName(\"SQLAPIExample\").getOrCreate()\n","\n","# Чтение данных из JSON файлов\n","people_df = spark.read.json(\"/content/people.json\")\n","departments_df = spark.read.json(\"/content/departments.json\")\n","\n","# Регистрация DataFrame как временные вьюхи\n","people_df.createOrReplaceTempView(\"people\")\n","departments_df.createOrReplaceTempView(\"departments\")\n","\n","# Выполнение JOIN-запроса с использованием SQL\n","join_df = spark.sql(\"\"\"\n","SELECT p.name, p.age, d.department_name\n","FROM people p\n","JOIN departments d\n","ON p.department_id = d.id\n","\"\"\")\n","\n","# Показ результатов\n","join_df.show()\n","\n","# Сохранение результирующего DataFrame в CSV файл\n","join_df.write.csv(\"/content/output_2.csv\", header=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W_HVEA0pDD9C","executionInfo":{"status":"ok","timestamp":1760199883792,"user_tz":-180,"elapsed":1588,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"a458b218-26c6-4669-e322-a2125c071104"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+---+---------------+\n","| name|age|department_name|\n","+-----+---+---------------+\n","| John| 30|             HR|\n","|  Doe| 25|        Finance|\n","| Jane| 35|             HR|\n","| Mark| 40|        Finance|\n","|Smith| 23|    Engineering|\n","+-----+---+---------------+\n","\n"]}]},{"cell_type":"markdown","source":["**PySpark соединения**\n"],"metadata":{"id":"usDCK30dHM8B"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# Создание SparkSession\n","spark = SparkSession.builder.appName(\"JoinExamples\").getOrCreate()\n","\n","# Пример данных для DataFrame people\n","people_data = [\n","    (\"John\", 30, 1),\n","    (\"Doe\", 25, 2),\n","    (\"Jane\", 35, 1),\n","    (\"Mark\", 40, 2),\n","    (\"Smith\", 23, 3)\n","]\n","people_columns = [\"name\", \"age\", \"department_id\"]\n","people_df = spark.createDataFrame(data=people_data, schema=people_columns)\n","\n","# Пример данных для DataFrame departments\n","departments_data = [\n","    (1, \"HR\"),\n","    (2, \"Finance\"),\n","    (3, \"Engineering\"),\n","    (4, \"Marketing\")\n","]\n","departments_columns = [\"id\", \"department_name\"]\n","departments_df = spark.createDataFrame(data=departments_data, schema=departments_columns)\n","\n","# Показ данных\n","people_df.show()\n","departments_df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2LnoDkW-HWqY","executionInfo":{"status":"ok","timestamp":1760200339238,"user_tz":-180,"elapsed":2882,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"0561a066-420b-42dc-f4a1-5911a2c1959d"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+---+-------------+\n","| name|age|department_id|\n","+-----+---+-------------+\n","| John| 30|            1|\n","|  Doe| 25|            2|\n","| Jane| 35|            1|\n","| Mark| 40|            2|\n","|Smith| 23|            3|\n","+-----+---+-------------+\n","\n","+---+---------------+\n","| id|department_name|\n","+---+---------------+\n","|  1|             HR|\n","|  2|        Finance|\n","|  3|    Engineering|\n","|  4|      Marketing|\n","+---+---------------+\n","\n"]}]},{"cell_type":"markdown","source":["**Внутреннее соединение (Inner Join)**\n","\n","Внутреннее соединение возвращает только те строки, которые имеют совпадающие значения в обеих таблицах."],"metadata":{"id":"4G_tQpX0HiZ_"}},{"cell_type":"code","source":["# Внутреннее соединение\n","inner_join_df = people_df.join(departments_df, people_df.department_id == departments_df.id, \"inner\")\n","inner_join_df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zIxAV_u6HkZA","executionInfo":{"status":"ok","timestamp":1760200388698,"user_tz":-180,"elapsed":3017,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"854c3e07-7cb6-447b-a0b3-f5d940d3ffe5"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+---+-------------+---+---------------+\n","| name|age|department_id| id|department_name|\n","+-----+---+-------------+---+---------------+\n","| John| 30|            1|  1|             HR|\n","| Jane| 35|            1|  1|             HR|\n","|  Doe| 25|            2|  2|        Finance|\n","| Mark| 40|            2|  2|        Finance|\n","|Smith| 23|            3|  3|    Engineering|\n","+-----+---+-------------+---+---------------+\n","\n"]}]},{"cell_type":"markdown","source":["**Левое внешнее соединение (Left Outer Join)**\n","\n","Левое внешнее соединение возвращает все строки из левой таблицы и соответствующие строки из правой таблицы. Если соответствия нет, возвращаются NULL значения для столбцов правой таблицы."],"metadata":{"id":"rUsE5HjqHpmI"}},{"cell_type":"code","source":["# Левое внешнее соединение\n","left_outer_join_df = people_df.join(departments_df, people_df.department_id == departments_df.id, \"left_outer\")\n","left_outer_join_df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aF1aByqQHsQv","executionInfo":{"status":"ok","timestamp":1760200416187,"user_tz":-180,"elapsed":1068,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"89630c8c-933b-4730-8d22-adf21f83df29"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+---+-------------+---+---------------+\n","| name|age|department_id| id|department_name|\n","+-----+---+-------------+---+---------------+\n","| John| 30|            1|  1|             HR|\n","|  Doe| 25|            2|  2|        Finance|\n","| Jane| 35|            1|  1|             HR|\n","|Smith| 23|            3|  3|    Engineering|\n","| Mark| 40|            2|  2|        Finance|\n","+-----+---+-------------+---+---------------+\n","\n"]}]},{"cell_type":"markdown","source":["**Правое внешнее соединение (Right Outer Join)**\n","\n","Правое внешнее соединение возвращает все строки из правой таблицы и соответствующие строки из левой таблицы. Если соответствия нет, возвращаются NULL значения для столбцов левой таблицы."],"metadata":{"id":"mNpUj6vBHwro"}},{"cell_type":"code","source":["# Правое внешнее соединение\n","right_outer_join_df = people_df.join(departments_df, people_df.department_id == departments_df.id, \"right_outer\")\n","right_outer_join_df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BjKygpiIHyNX","executionInfo":{"status":"ok","timestamp":1760200439890,"user_tz":-180,"elapsed":1088,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"5bbfdcea-355b-44a7-cf50-833f8666a011"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+----+-------------+---+---------------+\n","| name| age|department_id| id|department_name|\n","+-----+----+-------------+---+---------------+\n","| Jane|  35|            1|  1|             HR|\n","| John|  30|            1|  1|             HR|\n","| Mark|  40|            2|  2|        Finance|\n","|  Doe|  25|            2|  2|        Finance|\n","|Smith|  23|            3|  3|    Engineering|\n","| NULL|NULL|         NULL|  4|      Marketing|\n","+-----+----+-------------+---+---------------+\n","\n"]}]},{"cell_type":"markdown","source":["**Полное внешнее соединение (Full Outer Join)**\n","\n","Полное внешнее соединение возвращает все строки, когда есть совпадение в одной из таблиц. Строки без совпадений в обеих таблицах будут иметь NULL значения для столбцов из другой таблицы."],"metadata":{"id":"4gT9oKadH104"}},{"cell_type":"code","source":["full_outer_join_df = people_df.join(departments_df, people_df.department_id == departments_df.id, \"outer\")\n","full_outer_join_df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AvF9F3ZSH4fQ","executionInfo":{"status":"ok","timestamp":1760200467045,"user_tz":-180,"elapsed":1140,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"963b8f07-0461-4062-bd1f-a79640ab8970"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+----+-------------+---+---------------+\n","| name| age|department_id| id|department_name|\n","+-----+----+-------------+---+---------------+\n","| John|  30|            1|  1|             HR|\n","| Jane|  35|            1|  1|             HR|\n","|  Doe|  25|            2|  2|        Finance|\n","| Mark|  40|            2|  2|        Finance|\n","|Smith|  23|            3|  3|    Engineering|\n","| NULL|NULL|         NULL|  4|      Marketing|\n","+-----+----+-------------+---+---------------+\n","\n"]}]},{"cell_type":"markdown","source":["**Полное перекрестное соединение (Cross Join)**\n","\n","Полное перекрестное соединение возвращает декартово произведение строк обеих таблиц."],"metadata":{"id":"I2_tS0BHH8S4"}},{"cell_type":"code","source":["cross_join_df = people_df.crossJoin(departments_df)\n","\n","cross_join_df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bXJqQSK1H96w","executionInfo":{"status":"ok","timestamp":1760200492897,"user_tz":-180,"elapsed":1324,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"f9bf8b73-8b12-41c7-ce3c-58d7cd7b603c"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+---+-------------+---+---------------+\n","| name|age|department_id| id|department_name|\n","+-----+---+-------------+---+---------------+\n","| John| 30|            1|  1|             HR|\n","| John| 30|            1|  2|        Finance|\n","|  Doe| 25|            2|  1|             HR|\n","|  Doe| 25|            2|  2|        Finance|\n","| John| 30|            1|  3|    Engineering|\n","| John| 30|            1|  4|      Marketing|\n","|  Doe| 25|            2|  3|    Engineering|\n","|  Doe| 25|            2|  4|      Marketing|\n","| Jane| 35|            1|  1|             HR|\n","| Jane| 35|            1|  2|        Finance|\n","| Mark| 40|            2|  1|             HR|\n","| Mark| 40|            2|  2|        Finance|\n","|Smith| 23|            3|  1|             HR|\n","|Smith| 23|            3|  2|        Finance|\n","| Jane| 35|            1|  3|    Engineering|\n","| Jane| 35|            1|  4|      Marketing|\n","| Mark| 40|            2|  3|    Engineering|\n","| Mark| 40|            2|  4|      Marketing|\n","|Smith| 23|            3|  3|    Engineering|\n","|Smith| 23|            3|  4|      Marketing|\n","+-----+---+-------------+---+---------------+\n","\n"]}]},{"cell_type":"markdown","source":["**Соединение с использованием условия (Join with Condition)**\n","\n","Соединение с использованием условия позволяет задать более сложные условия соединения."],"metadata":{"id":"eBTs1rhrIA9w"}},{"cell_type":"code","source":["condition_join_df = people_df.join(departments_df, (people_df.department_id == departments_df.id) & (people_df.age > 30), \"inner\")\n","condition_join_df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-AO7n1qgIDG4","executionInfo":{"status":"ok","timestamp":1760200513402,"user_tz":-180,"elapsed":923,"user":{"displayName":"Евгений Ерохин","userId":"02627938050949049213"}},"outputId":"f1e97afd-78ab-4add-d540-480e8d1516f5"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["+----+---+-------------+---+---------------+\n","|name|age|department_id| id|department_name|\n","+----+---+-------------+---+---------------+\n","|Jane| 35|            1|  1|             HR|\n","|Mark| 40|            2|  2|        Finance|\n","+----+---+-------------+---+---------------+\n","\n"]}]}]}