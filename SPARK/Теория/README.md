## Spark

Spark имеет способность работать в памяти, что обеспечивает значительное ускорение обработки данных. 
Это стало возможным благодаря концепции Resilient Distributed Dataset (RDD), которая позволяет эффективно распределять данные и выполнять операции над ними в памяти.

**Конкуренты Spark**

**Cloudera Impala:** Impala - это распределенный SQL-движок, разработанный для выполнения интерактивных запросов на основе структурированных данных, хранящихся в Apache Hadoop. 
Impala обеспечивает высокую скорость выполнения SQL-запросов и позволяет взаимодействовать с данными в реальном времени.

**Основные ядра Spark**

Apache Spark состоит из нескольких основных компонентов, которые выполняют различные функции в распределенной обработке данных. Основные ядра Apache Spark включают:

**Spark Core:** Spark Core является основным компонентом Apache Spark. 
Он предоставляет основные функциональности и API для распределенной обработки данных, включая управление памятью, планирование задач, ввод-вывод данных и взаимодействие с распределенной файловой системой.

**Spark SQL:** Spark SQL предоставляет возможности работы с данными в структурированном формате, поддерживая SQL-запросы и операции со структурами данных, такими как таблицы, представления и датасеты. 
Spark SQL обеспечивает интеграцию с различными источниками данных, включая Apache Hive, JDBC и другие.

**Spark Streaming:** Spark Streaming предоставляет возможности обработки потоковых данных в реальном времени. 
Он позволяет разрабатывать и запускать аналитические приложения для обработки непрерывных потоков данных, таких как данные из очередей сообщений, систем мониторинга и т.д.

**MLlib:** MLlib (Machine Learning Library) является библиотекой машинного обучения в Apache Spark. 
Она предоставляет набор алгоритмов и утилит для обработки данных, классификации, регрессии, кластеризации, рекомендательных систем и других задач машинного обучения.

**GraphX:** GraphX - это библиотека для работы с графами в Apache Spark. 
Она предоставляет API и инструменты для анализа и обработки графовых структур, таких как социальные сети, сети связей и другие.

**SparkR:** SparkR - это пакет для языка программирования R, который позволяет использовать возможности Apache Spark в среде R. 
Он предоставляет R-разработчикам удобный способ взаимодействия с данными и выполнения распределенных вычислений в Spark.

Каждое ядро Apache Spark предлагает свои возможности и API для обработки и анализа данных в различных сценариях. 
Вместе эти компоненты обеспечивают мощную и гибкую платформу для обработки больших объемов данных и выполнения различных задач анализа и машинного обучения.

## Создание Spark Session и сравнение со Spark Context

В Apache Spark создание `Spark Session` является более предпочтительным подходом по сравнению с использованием `Spark Context`. Вот почему:

**Spark Context** был основной входной точкой для программирования на Spark в более ранних версиях фреймворка. Он предоставлял основные функции и возможности Spark, такие как создание `RDD (Resilient Distributed Dataset)` и выполнение операций над ними. Однако, с появлением Spark 2.0 и выше, введение `Spark Session` стало рекомендованным способом работы с Spark.

**Spark Session** является более высокоуровневым интерфейсом, который объединяет функциональность `Spark Context`, `SQL Context` и `Hive Context` в одном объекте. Он предоставляет доступ к функциям `Spark Core`, `Spark SQL`, `Spark Streaming`, `MLlib` и `GraphX`. `Spark Session` позволяет разработчикам работать с различными типами данных, включая `RDD`, `DataFrames` и `Datasets`, а также выполнять запросы на языке SQL, манипулировать структурированными данными и выполнять аналитику в реальном времени.

**Преимущества Spark Session по сравнению с Spark Context:**

Удобство использования: Spark Session предоставляет более удобный и единый интерфейс для работы с различными модулями Spark. Он упрощает кодирование и повышает производительность разработки.

Поддержка различных типов данных: Spark Session поддерживает работу с RDD, DataFrames и Datasets, что обеспечивает более гибкую обработку и анализ данных в Spark.

Интеграция со сторонними инструментами: Spark Session обеспечивает интеграцию с различными инструментами и библиотеками, такими как Hive, JDBC, Parquet, Avro и другими.

Улучшенная оптимизация: Spark Session имеет лучшую оптимизацию выполнения запросов, что приводит к улучшенной производительности.

Поддержка различных источников данных: Spark Session позволяет работать с различными источниками данных, включая файловые системы (HDFS, S3 и т.д.), базы данных (MySQL, PostgreSQL и др.), Apache Kafka и другие.

В итоге, Spark Session предоставляет более современный и мощный способ работы с Apache Spark, объединяя различные модули и обеспечивая удобство использования, гибкость и лучшую производительность. Он является рекомендованным подходом для разработки на Spark, особенно начиная с версии Spark 2.0 и выше.
