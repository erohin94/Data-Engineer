# Содержание

- [Ключевые преимущества Spark](#Ключевые-преимущества-Spark)
- [Как именно Spark работает в памяти](#Как-именно-Spark-работает-в-памяти)
- [Ключевое различие между Spark и Hadoop MapReduce](#Ключевое-различие-между-Spark-и-Hadoop-MapReduce)
- [Конкуренты Spark](#Конкуренты-Spark)
- [Основные ядра Spark](#Основные-ядра-Spark)
- [Создание Spark Session и сравнение со Spark Context](#Создание-Spark-Session-и-сравнение-со-Spark-Context)
- [Различные параметры Spark Session](#Различные-параметры-Spark-Session)
- [Как происходит выполнение задачи в PySpark](#Как-происходит-выполнение-задачи-в-PySpark)
- [Workflow приложения Spark](#Workflow-приложения-Spark)
- [RDD vs Dataset vs Dataframe](#RDD-vs-Dataset-vs-Dataframe)
- [Ленивые вычисления действия и преобразования](#Ленивые-вычисления-действия-и-преобразования)
- [Форматы хранения данных](#Форматы-хранения-данных)
     
## Ключевые преимущества Spark

Spark имеет способность работать в памяти, что обеспечивает значительное ускорение обработки данных. 
Это стало возможным благодаря концепции `Resilient Distributed Dataset (RDD)`, которая позволяет эффективно распределять данные и выполнять операции над ними в памяти.

**"Работать в памяти" (In-Memory Processing)** означает, что Spark старается хранить промежуточные данные и результаты вычислений не на жестком диске `(HDD)` или `SSD`, 
а в оперативной памяти `(RAM)` компьютеров, входящих в кластер.

**Простая аналогия**

Представьте, что вы повар, готовящий сложное блюдо:

**Работа с диском (как в Hadoop MapReduce - промежуточные результаты хранятся на диске):** Вам нужно каждый раз ходить в кладовку (диск) за каждым ингредиентом, потом относить туда же готовый полуфабрикат, чтобы потом снова пойти в кладовку за ним для следующего шага. Это очень медленно из-за постоянных "ходок".

**Работа в памяти (как в Spark):** Вы кладете все нужные ингредиенты и промежуточные результаты прямо на свой рабочий стол (оперативную память). Все операции (нарезка, смешивание) происходят очень быстро, так как всё под рукой. На диск вы идете только в самом конце, чтобы убрать готовое блюдо на хранение.

**Почему это так важно?**

**Скорость:** Оперативная память на порядки быстрее любого диска (даже `SSD`). Скорость доступа к данным в `RAM` в сотни тысяч раз выше, чем к данным на `HDD`. Это главная причина, по которой `Spark` может быть в 100 раз быстрее `Hadoop MapReduce` для определенных задач.

**Итеративность:** Многие алгоритмы (особенно машинное обучение и графовые алгоритмы) требуют многократного повторения (итераций) одних и тех же вычислений над одними и теми же данными. `Spark` загружает данные в память один раз и может повторять вычисления, не обращаясь к диску снова и снова.

**Интерактивность:** Благодаря скорости становится возможным интерактивный анализ данных. Вы можете запускать запросы к большим данным и получать ответ за секунды, а не минуты или часы, что позволяет исследовать данные "на лету".

## Как именно Spark работает в памяти

В основе Spark лежит концепция `RDD (Resilient Distributed Dataset)` — устойчивый распределенный набор данных.

**1.Распределенный:** Данные разбиваются на части `(partitions)` и распределяются по оперативной памяти разных узлов (компьютеров) кластера.

**2.Устойчивый (Resilient):** `Spark` автоматически отслеживает, как был создан каждый `RDD` (его "родословную" или `lineage`). Если данные на одном из узлов теряются (например, упал сервер), `Spark` может пересчитать эти данные заново на других узлах, используя сохраненную информацию об их происхождении. Это обеспечивает отказоустойчивость без необходимости постоянной записи на медленный диск.

**3.Набор данных в памяти:** `RDD` по умолчанию хранится в `RAM`. Только если памяти не хватает, `Spark` "вытесняет" `(spills)` часть данных на диск, но старается делать это как можно реже.

**Важное уточнение: Не все данные всегда в памяти**

Важно понимать, что `Spark` — не волшебная палочка. Есть нюансы:

**1.Входные данные сначала читаются с диска.** `Spark` не создает данные из воздуха. Исходные данные обычно лежат в распределенной файловой системе (например, `HDFS`) или базе данных. `Spark` сначала считывает их с диска в память.

**2.Кэширование (Caching/Persistence)** — это явное действие. Чтобы данные оставались в памяти для повторного использования, программист должен явно указать это с помощью методов `.cache()` или `.persist()`. Без этого `Spark` может выгрузить данные из памяти после выполнения операции.

**3.Если памяти не хватает**, Spark начинает использовать диск, и производительность закономерно падает. Поэтому размер кластера и объем памяти нужно адекватно планировать под задачу.

**Пример на псевдокоде**

Допустим, нам нужно посчитать количество слов в документе и найти самое частое слово.

*Без работы в памяти (как в старом подходе):*

1.Прочитать данные с диска -> посчитать слова -> записать промежуточный результат на диск.

2.Прочитать промежуточный результат с диска -> отсортировать по количеству -> записать результат на диск.

3.Прочитать результат с диска -> взять первое значение -> вывести ответ.

*С Spark и работой в памяти:*

1.Прочитать данные с диска и загрузить в память.

2.Посчитать слова (результат остается в памяти).

3.Отсортировать результат (операции происходят в памяти).

4.Взять первое значение и вывести ответ. На диск пишется только конечный результат.

**Итог**

"Работать в памяти" — это основная архитектурная идея Spark, которая позволяет ему достигать высокой скорости обработки данных за счет минимизации медленных операций ввода-вывода с диском. Данные хранятся и обрабатываются в оперативной памяти распределенного кластера, что особенно эффективно для итеративных и интерактивных задач.

## Ключевое различие между Spark и Hadoop MapReduce

Ключевое различие между `Spark` и `Hadoop MapReduce` заключается в модели управления данными между этапами вычислений.

**Hadoop MapReduce: Жесткая модель "диск-память-диск"**

В MapReduce каждый этап работы с данными следует строгому шаблону:

**1. Read (Чтение):** Данные считываются с диска `(HDFS)` в оперативную память.

**2. Map/Reduce (Вычисления):** В оперативной памяти выполняются функции `map` или `reduce`.

**3. Write (Запись):** Промежуточный результат вычислений (например, вывод этапа `map`, который является входом для этапа `reduce`) обязательно записывается на локальный диск каждого `worker`-узла.

**4. Shuffle (Перемешивание):** Следующий этап (например, `reduce`) должен считать эти промежуточные данные с дисков других узлов по сети.

**Проблема:** Эта модель создает огромное количество операций ввода-вывода `(I/O)` на диск. Для сложных заданий, требующих множественных этапов (например, несколько последовательных `MapReduce`-задач), эти промежуточные данные многократно пишутся и читаются с диска, что становится "бутылочным горлышком" и сильно замедляет всю обработку.

**Apache Spark: Гибкая модель "память-диск-память"**

`Spark` использует более гибкий подход, отдавая приоритет памяти:

**1. Read (Чтение):** Данные считываются с диска (`HDFS`, `S3` и т.д.) в оперативную память.

**2. Transformations (Вычисления):** Над данными в оперативной памяти выполняются все преобразования (`map`, `filter`, `reduce`, `join` и т.д.).

**3. Persistence (Сохранение в памяти):** Промежуточные результаты можно явно сохранить (закэшировать) в оперативной памяти с помощью методов '.cache()' или '.persist()'. Это позволяет последующим операциям использовать эти данные с огромной скоростью, не читая их заново с диска.

**4. Shuffle (Перемешивание):** Хотя `Spark` и старается минимизировать лишние перемещения, этап `shuffle` всё же происходит при перераспределении данных (например, перед `reduceByKey`). Но ключевое отличие: даже данные для `shuffle` сначала стараются буферизоваться в памяти, и только при нехватке места сбрасываются на локальный диск узла.

**Преимущество:** Многократные операции над одними и теми же данными происходят в памяти на порядок быстрее. Диск используется экономно, в основном как резерв или для финального сохранения результата.

## Конкуренты Spark

**Cloudera Impala:** Impala - это распределенный SQL-движок, разработанный для выполнения интерактивных запросов на основе структурированных данных, хранящихся в Apache Hadoop. 
Impala обеспечивает высокую скорость выполнения SQL-запросов и позволяет взаимодействовать с данными в реальном времени.

## Основные ядра Spark

Apache Spark состоит из нескольких основных компонентов, которые выполняют различные функции в распределенной обработке данных. Основные ядра Apache Spark включают:

**Spark Core:** Spark Core является основным компонентом Apache Spark. 
Он предоставляет основные функциональности и API для распределенной обработки данных, включая управление памятью, планирование задач, ввод-вывод данных и взаимодействие с распределенной файловой системой.

**Spark SQL:** Spark SQL предоставляет возможности работы с данными в структурированном формате, поддерживая SQL-запросы и операции со структурами данных, такими как таблицы, представления и датасеты. 
Spark SQL обеспечивает интеграцию с различными источниками данных, включая Apache Hive, JDBC и другие.

**Spark Streaming:** Spark Streaming предоставляет возможности обработки потоковых данных в реальном времени. 
Он позволяет разрабатывать и запускать аналитические приложения для обработки непрерывных потоков данных, таких как данные из очередей сообщений, систем мониторинга и т.д.

**MLlib:** MLlib (Machine Learning Library) является библиотекой машинного обучения в Apache Spark. 
Она предоставляет набор алгоритмов и утилит для обработки данных, классификации, регрессии, кластеризации, рекомендательных систем и других задач машинного обучения.

**GraphX:** GraphX - это библиотека для работы с графами в Apache Spark. 
Она предоставляет API и инструменты для анализа и обработки графовых структур, таких как социальные сети, сети связей и другие.

**SparkR:** SparkR - это пакет для языка программирования R, который позволяет использовать возможности Apache Spark в среде R. 
Он предоставляет R-разработчикам удобный способ взаимодействия с данными и выполнения распределенных вычислений в Spark.

Каждое ядро Apache Spark предлагает свои возможности и API для обработки и анализа данных в различных сценариях. 
Вместе эти компоненты обеспечивают мощную и гибкую платформу для обработки больших объемов данных и выполнения различных задач анализа и машинного обучения.

## Создание Spark Session и сравнение со Spark Context

В Apache Spark создание `Spark Session` является более предпочтительным подходом по сравнению с использованием `Spark Context`. Вот почему:

**Spark Context** был основной входной точкой для программирования на Spark в более ранних версиях фреймворка. Он предоставлял основные функции и возможности Spark, такие как создание `RDD (Resilient Distributed Dataset)` и выполнение операций над ними. Однако, с появлением Spark 2.0 и выше, введение `Spark Session` стало рекомендованным способом работы с Spark.

**Spark Session** является более высокоуровневым интерфейсом, который объединяет функциональность `Spark Context`, `SQL Context` и `Hive Context` в одном объекте. Он предоставляет доступ к функциям `Spark Core`, `Spark SQL`, `Spark Streaming`, `MLlib` и `GraphX`. `Spark Session` позволяет разработчикам работать с различными типами данных, включая `RDD`, `DataFrames` и `Datasets`, а также выполнять запросы на языке SQL, манипулировать структурированными данными и выполнять аналитику в реальном времени.

**Преимущества Spark Session по сравнению с Spark Context:**

Удобство использования: Spark Session предоставляет более удобный и единый интерфейс для работы с различными модулями Spark. Он упрощает кодирование и повышает производительность разработки.

Поддержка различных типов данных: Spark Session поддерживает работу с RDD, DataFrames и Datasets, что обеспечивает более гибкую обработку и анализ данных в Spark.

Интеграция со сторонними инструментами: Spark Session обеспечивает интеграцию с различными инструментами и библиотеками, такими как Hive, JDBC, Parquet, Avro и другими.

Улучшенная оптимизация: Spark Session имеет лучшую оптимизацию выполнения запросов, что приводит к улучшенной производительности.

Поддержка различных источников данных: Spark Session позволяет работать с различными источниками данных, включая файловые системы (HDFS, S3 и т.д.), базы данных (MySQL, PostgreSQL и др.), Apache Kafka и другие.

В итоге, Spark Session предоставляет более современный и мощный способ работы с Apache Spark, объединяя различные модули и обеспечивая удобство использования, гибкость и лучшую производительность. Он является рекомендованным подходом для разработки на Spark, особенно начиная с версии Spark 2.0 и выше.

**Пример создания Spark Context на Python:**

```
from pyspark import SparkContext

# Создание объекта SparkContext
sc = SparkContext(appName="MySparkApp")

# Здесь можно выполнять операции с RDD

# Закрытие SparkContext
sc.stop()
```
                  
**Пример создания Spark Session на Python:**

```
from pyspark.sql import SparkSession

# Создание объекта SparkSession
spark = SparkSession.builder \
    .appName("MySparkApp") \
    .getOrCreate()

# Здесь можно выполнять операции с DataFrames
# Закрытие SparkSession
spark.stop()
```
                  
В обоих примерах мы используем PySpark для работы с Spark в языке Python.

В примере создания `Spark Context` мы импортируем модуль `SparkContext` из библиотеки `PySpark` и создаем объект `sc` с указанием имени приложения. Затем мы можем выполнять операции с `RDD`, используя `sc`. По завершении работы необходимо вызвать метод `stop()` для закрытия `Spark Context`.

В примере создания `Spark Session` мы импортируем модуль `SparkSession` и используем `builder` для настройки параметров `Spark Session`, таких как имя приложения. Затем мы вызываем метод `getOrCreate()`, который создает новую `Spark Session` или возвращает существующую, если она уже создана. После этого мы можем выполнять операции с `DataFrames`, используя `spark`. Наконец, мы вызываем метод `stop()` для закрытия `Spark Session`.

## Различные параметры Spark Session

**App Name (spark.app.name)**

- Описание: Имя Spark-приложения.
- Пример: spark = SparkSession.builder.appName("MyApp").getOrCreate()

**Master (spark.master)**

- Описание: Указывает кластерный менеджер для выполнения приложения (например, локально или в кластере).
- Пример: spark = SparkSession.builder.master("local[*]").getOrCreate()

**Spark SQL Shuffle Partitions (spark.sql.shuffle.partitions)**

- Описание: Количество разделов для операций shuffle в Spark SQL.
- Пример: spark = SparkSession.builder.config("spark.sql.shuffle.partitions", "200").getOrCreate()

**Executor Memory (spark.executor.memory)**

- Описание: Количество памяти, выделяемой каждому исполнителю.
- Пример: spark = SparkSession.builder.config("spark.executor.memory", "4g").getOrCreate()

**Driver Memory (spark.driver.memory)**

- Описание: Количество памяти, выделяемой драйверу Spark-приложения.
- Пример: spark = SparkSession.builder.config("spark.driver.memory", "2g").getOrCreate()

**Executor Cores (spark.executor.cores)**

- Описание: Количество ядер, выделяемых каждому исполнителю.
- Пример: spark = SparkSession.builder.config("spark.executor.cores", "4").getOrCreate()

**Dynamic Allocation (spark.dynamicAllocation.enabled)**

- Описание: Включает динамическое выделение исполнителей для приложения.
- Пример: spark = SparkSession.builder.config("spark.dynamicAllocation.enabled", "true").getOrCreate()

**Checkpoint Directory (spark.checkpoint.dir)**

- Описание: Указывает директорию для сохранения контрольных точек RDD.
- Пример: spark = SparkSession.builder.config("spark.checkpoint.dir", "/path/to/checkpoint/dir").getOrCreate()

**SQL Warehouse Directory (spark.sql.warehouse.dir)**

- Описание: Указывает директорию для хранения данных Spark SQL.
- Пример: spark = SparkSession.builder.config("spark.sql.warehouse.dir", "/path/to/warehouse/dir").getOrCreate()

**Hive Support (spark.sql.catalogImplementation)**

- Описание: Включает поддержку Apache Hive в Spark SQL.
- Пример: spark = SparkSession.builder.config("spark.sql.catalogImplementation", "hive").enableHiveSupport().getOrCreate()

**Пример создания Spark Session с параметрами конфигурации:**

```
from pyspark.sql import SparkSession

# Создание Spark Session с различными параметрами конфигурации
spark = SparkSession.builder \
    .appName("MyApp") \
    .master("local[*]") \
    .config("spark.sql.shuffle.partitions", "200") \
    .config("spark.executor.memory", "4g") \
    .config("spark.driver.memory", "2g") \
    .config("spark.executor.cores", "4") \
    .config("spark.dynamicAllocation.enabled", "true") \
    .config("spark.checkpoint.dir", "/path/to/checkpoint/dir") \
    .config("spark.sql.warehouse.dir", "/path/to/warehouse/dir") \
    .config("spark.sql.catalogImplementation", "hive") \
    .enableHiveSupport() \
    .getOrCreate()

# Пример использования Spark Session
df = spark.read.json("/path/to/json/file")
df.show()
```

## Как происходит выполнение задачи в PySpark

Что происходит внутри Spark. Если Spark Session это точка входа в программу, то далее будет писаться высокоуровневый код, который будет оптимизироваться внутренним устройством Spark. Разбиремся, что будет происходить, после того, как мы запустим программу.

Любое Spark приложение состоит из драйвера(driver) и исполнителей(executors). Как раз таки driver - это и есть наша Spark Session. Именно в ней, задается конфигурация запуска нашего приложения и многое другое. 

Теперь разберемся с экзекьюторами. Экзекьюторы нужны для выполнения задач Spark. При чем здесь важно понять, что одно приложение в Spark - это несколько задач (но, конечно же, может быть и одна). Именно в исполнителях (экзекьюторах) происходит обработка данных. Стоит также отметить, что экзекьюторы работают параллельно и потом отдают результат драйверу (тут должна вспомниться похожая картинка про HDFS, только там обработки нету).

<p align="center"><img width="500" height="490" alt="Без названия" src="https://github.com/user-attachments/assets/e54a48db-a099-4efd-95e8-d3b0716afcf2" /></p>

Ситуация вырисовывается следующая - есть worker node, которые задаются cluster manager динамически. И уже исходя из этих worker node и доступной там памяти мы запускаем приложение Spark. На одной worker node может находится несколько executors, если будет хватать ресурсов. И также в executors может находиться несколько задач.


**Пример расчета ресурсов**

Допустим, у нас есть worker node с 16 ядрами и 64 ГБ памяти. Если укажем, что каждому исполнителю выделяется 2 ядра и 4 ГБ памяти, кластерный менеджер может запустить до 8 исполнителей на одном worker node.

**Расчет ресурсов:**

Доступные ядра на worker node: 16

Ядра на одного исполнителя: 2

Доступная память на worker node: 64 ГБ

Память на одного исполнителя: 4 ГБ

**Максимальное количество исполнителей на одном worker node:**

Ядра: 16 / 2 = 8 исполнителей

Память: 64 ГБ / 4 ГБ = 16 исполнителей

Здесь ограничивающим фактором является количество ядер, поэтому на этом worker node можно запустить до 8 исполнителей.

**Ограничения и рекомендации**

Ресурсы узла: Количество исполнителей, которые могут быть запущены на одном worker node, ограничивается доступными ресурсами (ядрами и памятью) на этом узле.

Изоляция ресурсов: Рекомендуется выделять ресурсы таким образом, чтобы избежать конфликтов и обеспечить достаточную изоляцию для каждого исполнителя.

Конфигурация кластера: Кластерный менеджер автоматически распределяет исполнителей по узлам на основе конфигурации и доступных ресурсов.

**Заметка**

- Driver всегда один на весь кластер.

- Worker nodes выполняют задачи, но они получают команды от единого Driver.

- Executors на Worker nodes выполняют свои части задач, но Driver управляет процессом выполнения.

Driver является единственной точкой координации всего кластера. Он:

- Отправляет задачи Worker узлам.

- Следит за их выполнением.

- Агрегирует результаты.

Worker nodes получают задачи от Driver и выполняют их независимо друг от друга, но по заданиям, спущенным от Driver.

## Workflow приложения Spark

Workflow - последовательность шагов для выполнения задачи

**Запуск автономного приложения и инициализация SparkContext**

```
from pyspark.sql import SparkSession

# Создание SparkSession
spark = SparkSession.builder.appName("Spark Workflow Example").getOrCreate()

# Получение SparkContext
sc = spark.sparkContext
```

-Приложение, написанное на Python, Scala, Java или R, запускается автономно.

-В этом приложении создается объект SparkSession (который включает SparkContext), инициализирующий необходимые компоненты для работы Spark.

-Только при наличии SparkContext приложение называется драйвером (Driver)

**Запрос ресурсов у менеджера кластеров**

-Драйвер запрашивает у кластерного менеджера (например, YARN, Mesos, Kubernetes или Spark Standalone) ресурсы для выполнения задачи.

-Драйвер сообщает кластерному менеджеру, сколько ресурсов ему требуется (например, количество исполнителей, объем памяти и процессорные ядра).

```
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 10 \
  --executor-memory 4g \
  --executor-cores 2 \
  path/to/your_script.py
```

Эквивалент в коде Python:

```
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("MyApp") \
    .config("spark.master", "yarn") \
    .config("spark.submit.deployMode", "cluster") \
    .config("spark.executor.instances", "10") \
    .config("spark.executor.memory", "4g") \
    .config("spark.executor.cores", "2") \
    .getOrCreate()

# Здесь пишем код
df = spark.read.parquet("hdfs://path/to/data")
df.show()

spark.stop()
```

Пример полного процесса:
Команду нужно выполнять на сервере, где установлен Hadoop/Spark, обычно это:

```
bash
# 1. Подключиться к edge-узлу
ssh data_engineer@hadoop-edge-01.prod.company.com

# 2. Перейти в рабочую директорию
cd /home/data_engineer/my_spark_project

# 3. Проверить доступность кластера
hdfs dfs -ls /data/input

# 4. Запустить Spark-приложение
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 10 \
  --executor-memory 4g \
  --executor-cores 2 \
  --name "My Data Processing Job" \
  data_processing.py

# 5. Мониторить выполнение
yarn application -list
```
              
**Запуск исполнителей менеджером кластеров**

-Кластерный менеджер распределяет ресурсы и запускает процессы исполнителей (executors) на рабочих узлах (worker nodes).

-Каждый исполнитель запускает отдельный процесс, который будет выполнять задачи, переданные драйвером.

**Запуск кода Spark драйвером**

-Драйвер начинает выполнение кода Spark, который включает чтение данных, выполнение преобразований (transformations) и действий (actions).

-Драйвер разделяет задачи на этапы (stages) и назначает их исполнителям.

```
# Чтение данных
df = spark.read.csv("path/to/input.csv", header=True, inferSchema=True)

# Выполнение преобразований
df_filtered = df.filter(df["age"] > 30)
df_grouped = df_filtered.groupBy("city").count()
```
                  
**Выполнение заданий исполнителями и отправка результатов драйверу**

-Исполнители получают задания от драйвера и начинают их выполнение.

-Каждый исполнитель выполняет часть работы, такой как вычисления на данных и возвращает результаты драйверу.

```
# Действие, инициирующее выполнение заданий
result = df_grouped.collect()
```
                  
**Остановка SparkContext и освобождение ресурсов**

-После завершения выполнения всех задач, драйвер останавливает SparkContext.

-Исполнители закрываются и освобождают выделенные им ресурсы, возвращая их обратно в кластер.

```
# Остановка SparkSession и SparkContext
spark.stop()
```

**Spark Workflow кратко:**

🚗 Driver-программа - ваш Python-скрипт с SparkSession

📋 Запрос ресурсов - через spark-submit говорим сколько нужно исполнителей

👥 Cluster Manager (YARN) - находит свободные серверы и запускает Executors

🛠️ Executors - рабочие процессы на разных серверах

📊 Разделение данных - Spark автоматически дробит данные на части

⚡ Параллельная обработка - каждый Executor обрабатывает свою часть данных

📦 Сбор результатов - Executors отправляют результаты обратно Driver

🧹 Очистка - spark.stop() освобождает ресурсы

Проще: Ваш код → Spark разбивает на задачи → Множество серверов работают параллельно → Собирает результаты → Отдает вам.

**⚡ Что на самом деле происходит:**

| Роль | В Spark | В аналогии |
|------|---------|------------|
| Driver | Ваш Python-скрипт | Прораб |
| Cluster Manager | YARN/K8s/Mesos | Директор по персоналу |
| Executors | Процессы на серверах | Рабочие бригады |
| Tasks | Единицы работы | Задания рабочим |
| Data | RDD/DataFrame | Стройматериалы |

Ключевой момент: Вы пишете код так, будто работаете с одним компьютером, а Spark автоматически распределяет эту работу на сотни серверов!

## RDD vs Dataset vs Dataframe

Хранить огромный массив информации (большие данные) в списке или кортеже или словаре Python занятие не очень интересное, да и неблагодарное. Поэтому введем в наш оборот еще несколько структур, применимых только в рамках PySpark. Здесь приводятся три структуры, хотя в PySpark доступны будут только 2 (RDD, Dataframe). Это связано с тем, что dataset используется только в Spark под управлением Java и Scala. 

**RDD** – это распределенная коллекция данных, расположенных по нескольким узлам кластера, набор объектов Java или Scala, представляющих данные. RDD работает со структурированными и с неструктурированные данными. Также, как DataFrame и DataSet, RDD не выводит схему загруженных данных и требует от пользователя ее указания.

<img width="756" height="326" alt="image" src="https://github.com/user-attachments/assets/9f6ab153-14df-4de6-8d8f-1bfa138ba0e4" />

**DataFrame** – это распределенная коллекция данных в виде именованных столбцов, аналогично таблице в реляционной базе данных. DataFrame работает только со структурированными и полуструктурированными данными, организуя информацию по столбцам, как в реляционных таблицах. Это позволяет Spark управлять схемой данных.

**DataSet**– это расширение API DataFrame, обеспечивающее функциональность объектно-ориентированного RDD-API (строгая типизация, лямбда-функции), производительность оптимизатора запросов Catalyst и механизм хранения вне кучи. DataSet эффективно обрабатывает структурированные и неструктурированные данные, представляя их в виде строки JVM-объектов или коллекции. Для представления табличных форм используется кодировщик (encoder).

RDD, DataFrame и DataSet по-разному используют схемы данных :

- в API RDD проекции схемы используются явно, поэтому требуется определять схему данных вручную;

- датафрейм автоматически определяет схему данных из файлов и представляет их в виде таблиц, например, через мета-хранилище Apache Благодаря этому возможно подключение стандартных SQL-клиентов к Apache Spark.

- датасет также, благодаря встроенному движку Spark SQL (engine) автоматически обнаружит схему данных.

В Apache Spark схема данных (schema) описывает структуру данных и их типы. Понимание схемы данных важно для работы с RDD, DataFrame и Dataset, поскольку каждый из этих типов абстракций имеет свои собственные методы и подходы для работы со схемой.

**RDD (Resilient Distributed Dataset)**

Схема данных:

- В RDD схема данных неявно определяется структурой объектов, хранящихся в RDD.

- RDD является типизированной коллекцией, где каждый элемент может быть объектом любого типа, но Spark не хранит информацию о структуре этих данных.

```
from pyspark import SparkContext

sc = SparkContext("local", "RDD Example")
data = [("Alice", 1), ("Bob", 2), ("Cathy", 3)]
rdd = sc.parallelize(data)

# Схема данных неявно определяется структурой кортежей
print(rdd.collect())  # [('Alice', 1), ('Bob', 2), ('Cathy', 3)]
```
                  
**DataFrame**

Схема данных:

- В DataFrame схема данных явная и включает имена столбцов и типы данных.

- Схема может быть автоматически определена при создании DataFrame из источника данных (например, CSV, JSON) или явно задана при создании DataFrame.

```
from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

spark = SparkSession.builder.appName("DataFrame Example").getOrCreate()
data = [("Alice", 1), ("Bob", 2), ("Cathy", 3)]

# Явное определение схемы
schema = StructType([
    StructField("Name", StringType(), True),
    StructField("Value", IntegerType(), True)
])

# Создание DataFrame с явной схемой
df = spark.createDataFrame(data, schema)
df.printSchema()

# Автоматическое определение схемы при чтении данных из CSV
df_auto = spark.read.csv("/path/to/csv/file", header=True, inferSchema=True)
df_auto.printSchema()
```
                  
**Dataset (доступен только в Scala и Java)**

Схема данных:

- В Dataset схема данных явная и определяется структурой типизированных объектов (классов), хранящихся в Dataset.

- Dataset сочетает в себе преимущества RDD (типизированный API) и DataFrame (явная схема и оптимизация).

```
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types._

val spark = SparkSession.builder.appName("Dataset Example").getOrCreate()
import spark.implicits._

case class Person(name: String, age: Int)
val data = Seq(Person("Alice", 1), Person("Bob", 2), Person("Cathy", 3))
val ds = data.toDS()

// Схема данных определяется структурой класса Person
ds.printSchema()
```

**Сравнение RDD, DataFrame и Dataset по схеме данных**

**RDD:**

- Схема данных неявно определяется структурой объектов.

- Нет встроенной поддержки явного описания схемы.

- Подходит для работы с неструктурированными или слабо структурированными данными.

**DataFrame:**

- Схема данных явная и включает имена столбцов и типы данных.

- Поддерживает автоматическое определение схемы при чтении данных из различных источников.

- Удобен для работы с структурированными данными и предоставляет мощный API для обработки данных на уровне SQL.

**Dataset (Scala/Java):**

- Схема данных явная и определяется структурой типизированных объектов (классов).

- Сочетает в себе преимущества RDD (типизированный API) и DataFrame (явная схема и оптимизация).

- Подходит для работы с типизированными данными и обеспечивает статическую типизацию и оптимизацию через Catalyst Optimizer. 

## Ленивые вычисления действия и преобразования

В Apache Spark концепции ленивых вычислений (lazy evaluation), преобразований (transformations) и действий (actions) играют ключевую роль в оптимизации и выполнении распределенных вычислений. Разберемся, что означают эти термины и как они работают в Spark.

**1. Ленивые вычисления (Lazy Evaluation)**

**Описание:**

Ленивые вычисления означают, что Spark не выполняет вычисления сразу при вызове преобразований. Вместо этого Spark откладывает выполнение до тех пор, пока не встретит действие, требующее получения конечного результата.
Это позволяет Spark оптимизировать план выполнения вычислений, объединяя несколько преобразований в одно и минимизируя количество операций ввода-вывода (I/O).

**Плюсы:**

🔸 Оптимизация: Spark может оптимизировать выполнение, уменьшив количество операций и объем данных, передаваемых между узлами.

🔸 Эффективность: Позволяет избежать ненужных вычислений, так как операции выполняются только при необходимости.

**Минусы:**

⚠️ Сложность отладки: Так как вычисления не выполняются немедленно, становится сложнее отслеживать ошибки и проблемы, которые могут возникнуть в процессе выполнения. Ошибки могут проявиться только при выполнении действия (action), что затрудняет диагностику.

⚠️ Задержка выполнения: Поскольку все преобразования откладываются до момента выполнения действия, время отклика может быть значительным, особенно если цепочка преобразований длинная. Это может привести к неожиданным задержкам при первом выполнении действия.

⚠️ Память и ресурсы: В некоторых случаях отложенные вычисления могут привести к накоплению больших объемов промежуточных данных в памяти. Это может увеличить нагрузку на память и ресурсы кластера, что может привести к сбоям или снижению производительности.

**2. Преобразования (Transformations)**

**Описание**

Преобразования создают новый RDD или DataFrame на основе существующего, но не выполняют вычислений немедленно. Они возвращают новый RDD или DataFrame, который описывает результат трансформации.
Преобразования являются ленивыми и только определяют, как должны быть преобразованы данные, без фактического выполнения операций.

**Примеры:**

`map()` — Применяет функцию к каждому элементу RDD и возвращает новый RDD.

`filter()` — Возвращает новый RDD, содержащий только те элементы, которые удовлетворяют условию.

`flatMap()` — Похож на map, но каждая входная строка может быть сопоставлена с несколькими выходными.

`distinct()` - Убирает дублирующиеся элементы из RDD. 

`union()` -  Объединяет два RDD в один.

```
from pyspark import SparkContext

sc = SparkContext("local", "Transformations Example")
data = [1, 2, 3, 4, 5]
rdd = sc.parallelize(data)

# Применение преобразования map
squared_rdd = rdd.map(lambda x: x * x)
```

**3. Действия (Actions)**

**Описание:**

Действия инициируют выполнение вычислений, описанных преобразованиями, и возвращают результат на драйвер или записывают его во внешний источник.
Действия заставляют Spark выполнить все отложенные преобразования, необходимые для получения результата.

**Типы действий:**

`collect`: Возвращает все элементы RDD или DataFrame в виде списка на драйвер.

`count`: Возвращает количество элементов в RDD или DataFrame.

`first`: Возвращает первый элемент RDD или DataFrame.

`take`: Возвращает первые n элементов RDD или DataFrame.

`reduce`: Применяет коммутативную и ассоциативную бинарную функцию для агрегации элементов.

`saveAsTextFile`: Сохраняет RDD как текстовый файл в файловой системе.

```
# Применение действия collect для выполнения вычислений и получения результата
result = squared_rdd.collect()
print(result)  # [1, 4, 9, 16, 25]
```

## Форматы хранения данных

**Разница между строчным и колоночным хранением**

Пример таблицы:

| id | name  | age | city   |
| -- | ----- | --- | ------ |
| 1  | Anna  | 25  | Moscow |
| 2  | Ivan  | 30  | London |
| 3  | Maria | 22  | Berlin |

**Строчное хранение (Row-based storage)** - данные хранятся построчно, то есть все значения одной строки записаны рядом друг с другом в памяти или на диске.

В памяти или на диске это будет записано примерно так:

```
[1, Anna, 25, Moscow]
[2, Ivan, 30, London]
[3, Maria, 22, Berlin]
```

Плюс: Быстро читать всю строку целиком. Быстро выполняются операции вставки, обновления, транзакции. Отлично подходит для OLTP-систем (операционные базы, например PostgreSQL, MySQL).

📘 Пример: `SELECT * FROM users WHERE id = 1;` Здесь важно быстро получить всю строку пользователя, поэтому строчное хранение эффективнее.

Минус: Медленно читать только один столбец (например, все возрасты), т.к. нужно просканировать все данные. При аналитике (агрегациях, фильтрации по колонкам) производительность падает.

**Колоночное хранение (Column-based storage)** - данные хранятся по колонкам, все значения одного столбца идут подряд.

В памяти или на диске это будет записано примерно так:

```
id:   [1, 2, 3]
name: [Anna, Ivan, Maria]
age:  [25, 30, 22]
city: [Moscow, London, Berlin]
```

Плюс: Очень быстро выполнять агрегации и читать несколько столбцов (т.к. читается только нужный столбец). Лучшее сжатие (поскольку данные в одном столбце однотипны).
Отлично подходит для OLAP-систем (аналитика, отчёты, витрины данных).

📘 Пример: `SELECT AVG(age) FROM users WHERE city = 'Moscow';` Здесь читаются только колонки age и city, поэтому колоночное хранение работает быстрее и экономнее по памяти.

Минус: Медленно читать/записывать одну целую строку. Неэффективно при частых обновлениях и вставках — приходится перезаписывать блоки колонок. Неудобно для транзакций и потоковых систем.
