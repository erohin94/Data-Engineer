# Содержание

- [Ключевые преимущества Spark](#Ключевые-преимущества-Spark)
- [Как именно Spark работает в памяти](#Как-именно-Spark-работает-в-памяти)
- [Ключевое различие между Spark и Hadoop MapReduce](#Ключевое-различие-между-Spark-и-Hadoop-MapReduce)
- [Конкуренты Spark](#Конкуренты-Spark)
- [Основные ядра Spark](#Основные-ядра-Spark)
- [Создание Spark Session и сравнение со Spark Context](#Создание-Spark-Session-и-сравнение-со-Spark-Context)
- [Различные параметры Spark Session](#Различные-параметры-Spark-Session)
- [Как происходит выполнение задачи в PySpark](#Как-происходит-выполнение-задачи-в-PySpark)
- [Workflow приложения Spark](#Workflow-приложения-Spark)
     
## Ключевые преимущества Spark

Spark имеет способность работать в памяти, что обеспечивает значительное ускорение обработки данных. 
Это стало возможным благодаря концепции `Resilient Distributed Dataset (RDD)`, которая позволяет эффективно распределять данные и выполнять операции над ними в памяти.

**"Работать в памяти" (In-Memory Processing)** означает, что Spark старается хранить промежуточные данные и результаты вычислений не на жестком диске `(HDD)` или `SSD`, 
а в оперативной памяти `(RAM)` компьютеров, входящих в кластер.

**Простая аналогия**

Представьте, что вы повар, готовящий сложное блюдо:

**Работа с диском (как в Hadoop MapReduce - промежуточные результаты хранятся на диске):** Вам нужно каждый раз ходить в кладовку (диск) за каждым ингредиентом, потом относить туда же готовый полуфабрикат, чтобы потом снова пойти в кладовку за ним для следующего шага. Это очень медленно из-за постоянных "ходок".

**Работа в памяти (как в Spark):** Вы кладете все нужные ингредиенты и промежуточные результаты прямо на свой рабочий стол (оперативную память). Все операции (нарезка, смешивание) происходят очень быстро, так как всё под рукой. На диск вы идете только в самом конце, чтобы убрать готовое блюдо на хранение.

**Почему это так важно?**

**Скорость:** Оперативная память на порядки быстрее любого диска (даже `SSD`). Скорость доступа к данным в `RAM` в сотни тысяч раз выше, чем к данным на `HDD`. Это главная причина, по которой `Spark` может быть в 100 раз быстрее `Hadoop MapReduce` для определенных задач.

**Итеративность:** Многие алгоритмы (особенно машинное обучение и графовые алгоритмы) требуют многократного повторения (итераций) одних и тех же вычислений над одними и теми же данными. `Spark` загружает данные в память один раз и может повторять вычисления, не обращаясь к диску снова и снова.

**Интерактивность:** Благодаря скорости становится возможным интерактивный анализ данных. Вы можете запускать запросы к большим данным и получать ответ за секунды, а не минуты или часы, что позволяет исследовать данные "на лету".

## Как именно Spark работает в памяти

В основе Spark лежит концепция `RDD (Resilient Distributed Dataset)` — устойчивый распределенный набор данных.

**1.Распределенный:** Данные разбиваются на части `(partitions)` и распределяются по оперативной памяти разных узлов (компьютеров) кластера.

**2.Устойчивый (Resilient):** `Spark` автоматически отслеживает, как был создан каждый `RDD` (его "родословную" или `lineage`). Если данные на одном из узлов теряются (например, упал сервер), `Spark` может пересчитать эти данные заново на других узлах, используя сохраненную информацию об их происхождении. Это обеспечивает отказоустойчивость без необходимости постоянной записи на медленный диск.

**3.Набор данных в памяти:** `RDD` по умолчанию хранится в `RAM`. Только если памяти не хватает, `Spark` "вытесняет" `(spills)` часть данных на диск, но старается делать это как можно реже.

**Важное уточнение: Не все данные всегда в памяти**

Важно понимать, что `Spark` — не волшебная палочка. Есть нюансы:

**1.Входные данные сначала читаются с диска.** `Spark` не создает данные из воздуха. Исходные данные обычно лежат в распределенной файловой системе (например, `HDFS`) или базе данных. `Spark` сначала считывает их с диска в память.

**2.Кэширование (Caching/Persistence)** — это явное действие. Чтобы данные оставались в памяти для повторного использования, программист должен явно указать это с помощью методов `.cache()` или `.persist()`. Без этого `Spark` может выгрузить данные из памяти после выполнения операции.

**3.Если памяти не хватает**, Spark начинает использовать диск, и производительность закономерно падает. Поэтому размер кластера и объем памяти нужно адекватно планировать под задачу.

**Пример на псевдокоде**

Допустим, нам нужно посчитать количество слов в документе и найти самое частое слово.

*Без работы в памяти (как в старом подходе):*

1.Прочитать данные с диска -> посчитать слова -> записать промежуточный результат на диск.

2.Прочитать промежуточный результат с диска -> отсортировать по количеству -> записать результат на диск.

3.Прочитать результат с диска -> взять первое значение -> вывести ответ.

*С Spark и работой в памяти:*

1.Прочитать данные с диска и загрузить в память.

2.Посчитать слова (результат остается в памяти).

3.Отсортировать результат (операции происходят в памяти).

4.Взять первое значение и вывести ответ. На диск пишется только конечный результат.

**Итог**

"Работать в памяти" — это основная архитектурная идея Spark, которая позволяет ему достигать высокой скорости обработки данных за счет минимизации медленных операций ввода-вывода с диском. Данные хранятся и обрабатываются в оперативной памяти распределенного кластера, что особенно эффективно для итеративных и интерактивных задач.

## Ключевое различие между Spark и Hadoop MapReduce

Ключевое различие между `Spark` и `Hadoop MapReduce` заключается в модели управления данными между этапами вычислений.

**Hadoop MapReduce: Жесткая модель "диск-память-диск"**

В MapReduce каждый этап работы с данными следует строгому шаблону:

**1. Read (Чтение):** Данные считываются с диска `(HDFS)` в оперативную память.

**2. Map/Reduce (Вычисления):** В оперативной памяти выполняются функции `map` или `reduce`.

**3. Write (Запись):** Промежуточный результат вычислений (например, вывод этапа `map`, который является входом для этапа `reduce`) обязательно записывается на локальный диск каждого `worker`-узла.

**4. Shuffle (Перемешивание):** Следующий этап (например, `reduce`) должен считать эти промежуточные данные с дисков других узлов по сети.

**Проблема:** Эта модель создает огромное количество операций ввода-вывода `(I/O)` на диск. Для сложных заданий, требующих множественных этапов (например, несколько последовательных `MapReduce`-задач), эти промежуточные данные многократно пишутся и читаются с диска, что становится "бутылочным горлышком" и сильно замедляет всю обработку.

**Apache Spark: Гибкая модель "память-диск-память"**

`Spark` использует более гибкий подход, отдавая приоритет памяти:

**1. Read (Чтение):** Данные считываются с диска (`HDFS`, `S3` и т.д.) в оперативную память.

**2. Transformations (Вычисления):** Над данными в оперативной памяти выполняются все преобразования (`map`, `filter`, `reduce`, `join` и т.д.).

**3. Persistence (Сохранение в памяти):** Промежуточные результаты можно явно сохранить (закэшировать) в оперативной памяти с помощью методов '.cache()' или '.persist()'. Это позволяет последующим операциям использовать эти данные с огромной скоростью, не читая их заново с диска.

**4. Shuffle (Перемешивание):** Хотя `Spark` и старается минимизировать лишние перемещения, этап `shuffle` всё же происходит при перераспределении данных (например, перед `reduceByKey`). Но ключевое отличие: даже данные для `shuffle` сначала стараются буферизоваться в памяти, и только при нехватке места сбрасываются на локальный диск узла.

**Преимущество:** Многократные операции над одними и теми же данными происходят в памяти на порядок быстрее. Диск используется экономно, в основном как резерв или для финального сохранения результата.

## Конкуренты Spark

**Cloudera Impala:** Impala - это распределенный SQL-движок, разработанный для выполнения интерактивных запросов на основе структурированных данных, хранящихся в Apache Hadoop. 
Impala обеспечивает высокую скорость выполнения SQL-запросов и позволяет взаимодействовать с данными в реальном времени.

## Основные ядра Spark

Apache Spark состоит из нескольких основных компонентов, которые выполняют различные функции в распределенной обработке данных. Основные ядра Apache Spark включают:

**Spark Core:** Spark Core является основным компонентом Apache Spark. 
Он предоставляет основные функциональности и API для распределенной обработки данных, включая управление памятью, планирование задач, ввод-вывод данных и взаимодействие с распределенной файловой системой.

**Spark SQL:** Spark SQL предоставляет возможности работы с данными в структурированном формате, поддерживая SQL-запросы и операции со структурами данных, такими как таблицы, представления и датасеты. 
Spark SQL обеспечивает интеграцию с различными источниками данных, включая Apache Hive, JDBC и другие.

**Spark Streaming:** Spark Streaming предоставляет возможности обработки потоковых данных в реальном времени. 
Он позволяет разрабатывать и запускать аналитические приложения для обработки непрерывных потоков данных, таких как данные из очередей сообщений, систем мониторинга и т.д.

**MLlib:** MLlib (Machine Learning Library) является библиотекой машинного обучения в Apache Spark. 
Она предоставляет набор алгоритмов и утилит для обработки данных, классификации, регрессии, кластеризации, рекомендательных систем и других задач машинного обучения.

**GraphX:** GraphX - это библиотека для работы с графами в Apache Spark. 
Она предоставляет API и инструменты для анализа и обработки графовых структур, таких как социальные сети, сети связей и другие.

**SparkR:** SparkR - это пакет для языка программирования R, который позволяет использовать возможности Apache Spark в среде R. 
Он предоставляет R-разработчикам удобный способ взаимодействия с данными и выполнения распределенных вычислений в Spark.

Каждое ядро Apache Spark предлагает свои возможности и API для обработки и анализа данных в различных сценариях. 
Вместе эти компоненты обеспечивают мощную и гибкую платформу для обработки больших объемов данных и выполнения различных задач анализа и машинного обучения.

## Создание Spark Session и сравнение со Spark Context

В Apache Spark создание `Spark Session` является более предпочтительным подходом по сравнению с использованием `Spark Context`. Вот почему:

**Spark Context** был основной входной точкой для программирования на Spark в более ранних версиях фреймворка. Он предоставлял основные функции и возможности Spark, такие как создание `RDD (Resilient Distributed Dataset)` и выполнение операций над ними. Однако, с появлением Spark 2.0 и выше, введение `Spark Session` стало рекомендованным способом работы с Spark.

**Spark Session** является более высокоуровневым интерфейсом, который объединяет функциональность `Spark Context`, `SQL Context` и `Hive Context` в одном объекте. Он предоставляет доступ к функциям `Spark Core`, `Spark SQL`, `Spark Streaming`, `MLlib` и `GraphX`. `Spark Session` позволяет разработчикам работать с различными типами данных, включая `RDD`, `DataFrames` и `Datasets`, а также выполнять запросы на языке SQL, манипулировать структурированными данными и выполнять аналитику в реальном времени.

**Преимущества Spark Session по сравнению с Spark Context:**

Удобство использования: Spark Session предоставляет более удобный и единый интерфейс для работы с различными модулями Spark. Он упрощает кодирование и повышает производительность разработки.

Поддержка различных типов данных: Spark Session поддерживает работу с RDD, DataFrames и Datasets, что обеспечивает более гибкую обработку и анализ данных в Spark.

Интеграция со сторонними инструментами: Spark Session обеспечивает интеграцию с различными инструментами и библиотеками, такими как Hive, JDBC, Parquet, Avro и другими.

Улучшенная оптимизация: Spark Session имеет лучшую оптимизацию выполнения запросов, что приводит к улучшенной производительности.

Поддержка различных источников данных: Spark Session позволяет работать с различными источниками данных, включая файловые системы (HDFS, S3 и т.д.), базы данных (MySQL, PostgreSQL и др.), Apache Kafka и другие.

В итоге, Spark Session предоставляет более современный и мощный способ работы с Apache Spark, объединяя различные модули и обеспечивая удобство использования, гибкость и лучшую производительность. Он является рекомендованным подходом для разработки на Spark, особенно начиная с версии Spark 2.0 и выше.

**Пример создания Spark Context на Python:**

```
from pyspark import SparkContext

# Создание объекта SparkContext
sc = SparkContext(appName="MySparkApp")

# Здесь можно выполнять операции с RDD

# Закрытие SparkContext
sc.stop()
```
                  
**Пример создания Spark Session на Python:**

```
from pyspark.sql import SparkSession

# Создание объекта SparkSession
spark = SparkSession.builder \
    .appName("MySparkApp") \
    .getOrCreate()

# Здесь можно выполнять операции с DataFrames
# Закрытие SparkSession
spark.stop()
```
                  
В обоих примерах мы используем PySpark для работы с Spark в языке Python.

В примере создания `Spark Context` мы импортируем модуль `SparkContext` из библиотеки `PySpark` и создаем объект `sc` с указанием имени приложения. Затем мы можем выполнять операции с `RDD`, используя `sc`. По завершении работы необходимо вызвать метод `stop()` для закрытия `Spark Context`.

В примере создания `Spark Session` мы импортируем модуль `SparkSession` и используем `builder` для настройки параметров `Spark Session`, таких как имя приложения. Затем мы вызываем метод `getOrCreate()`, который создает новую `Spark Session` или возвращает существующую, если она уже создана. После этого мы можем выполнять операции с `DataFrames`, используя `spark`. Наконец, мы вызываем метод `stop()` для закрытия `Spark Session`.

## Различные параметры Spark Session

**App Name (spark.app.name)**

- Описание: Имя Spark-приложения.
- Пример: spark = SparkSession.builder.appName("MyApp").getOrCreate()

**Master (spark.master)**

- Описание: Указывает кластерный менеджер для выполнения приложения (например, локально или в кластере).
- Пример: spark = SparkSession.builder.master("local[*]").getOrCreate()

**Spark SQL Shuffle Partitions (spark.sql.shuffle.partitions)**

- Описание: Количество разделов для операций shuffle в Spark SQL.
- Пример: spark = SparkSession.builder.config("spark.sql.shuffle.partitions", "200").getOrCreate()

**Executor Memory (spark.executor.memory)**

- Описание: Количество памяти, выделяемой каждому исполнителю.
- Пример: spark = SparkSession.builder.config("spark.executor.memory", "4g").getOrCreate()

**Driver Memory (spark.driver.memory)**

- Описание: Количество памяти, выделяемой драйверу Spark-приложения.
- Пример: spark = SparkSession.builder.config("spark.driver.memory", "2g").getOrCreate()

**Executor Cores (spark.executor.cores)**

- Описание: Количество ядер, выделяемых каждому исполнителю.
- Пример: spark = SparkSession.builder.config("spark.executor.cores", "4").getOrCreate()

**Dynamic Allocation (spark.dynamicAllocation.enabled)**

- Описание: Включает динамическое выделение исполнителей для приложения.
- Пример: spark = SparkSession.builder.config("spark.dynamicAllocation.enabled", "true").getOrCreate()

**Checkpoint Directory (spark.checkpoint.dir)**

- Описание: Указывает директорию для сохранения контрольных точек RDD.
- Пример: spark = SparkSession.builder.config("spark.checkpoint.dir", "/path/to/checkpoint/dir").getOrCreate()

**SQL Warehouse Directory (spark.sql.warehouse.dir)**

- Описание: Указывает директорию для хранения данных Spark SQL.
- Пример: spark = SparkSession.builder.config("spark.sql.warehouse.dir", "/path/to/warehouse/dir").getOrCreate()

**Hive Support (spark.sql.catalogImplementation)**

- Описание: Включает поддержку Apache Hive в Spark SQL.
- Пример: spark = SparkSession.builder.config("spark.sql.catalogImplementation", "hive").enableHiveSupport().getOrCreate()

**Пример создания Spark Session с параметрами конфигурации:**

```
from pyspark.sql import SparkSession

# Создание Spark Session с различными параметрами конфигурации
spark = SparkSession.builder \
    .appName("MyApp") \
    .master("local[*]") \
    .config("spark.sql.shuffle.partitions", "200") \
    .config("spark.executor.memory", "4g") \
    .config("spark.driver.memory", "2g") \
    .config("spark.executor.cores", "4") \
    .config("spark.dynamicAllocation.enabled", "true") \
    .config("spark.checkpoint.dir", "/path/to/checkpoint/dir") \
    .config("spark.sql.warehouse.dir", "/path/to/warehouse/dir") \
    .config("spark.sql.catalogImplementation", "hive") \
    .enableHiveSupport() \
    .getOrCreate()

# Пример использования Spark Session
df = spark.read.json("/path/to/json/file")
df.show()
```

## Как происходит выполнение задачи в PySpark

Что происходит внутри Spark. Если Spark Session это точка входа в программу, то далее будет писаться высокоуровневый код, который будет оптимизироваться внутренним устройством Spark. Разбиремся, что будет происходить, после того, как мы запустим программу.

Любое Spark приложение состоит из драйвера(driver) и исполнителей(executors). Как раз таки driver - это и есть наша Spark Session. Именно в ней, задается конфигурация запуска нашего приложения и многое другое. 

Теперь разберемся с экзекьюторами. Экзекьюторы нужны для выполнения задач Spark. При чем здесь важно понять, что одно приложение в Spark - это несколько задач (но, конечно же, может быть и одна). Именно в исполнителях (экзекьюторах) происходит обработка данных. Стоит также отметить, что экзекьюторы работают параллельно и потом отдают результат драйверу (тут должна вспомниться похожая картинка про HDFS, только там обработки нету).

<p align="center"><img width="500" height="490" alt="Без названия" src="https://github.com/user-attachments/assets/e54a48db-a099-4efd-95e8-d3b0716afcf2" /></p>

Ситуация вырисовывается следующая - есть worker node, которые задаются cluster manager динамически. И уже исходя из этих worker node и доступной там памяти мы запускаем приложение Spark. На одной worker node может находится несколько executors, если будет хватать ресурсов. И также в executors может находиться несколько задач.


**Пример расчета ресурсов**

Допустим, у нас есть worker node с 16 ядрами и 64 ГБ памяти. Если укажем, что каждому исполнителю выделяется 2 ядра и 4 ГБ памяти, кластерный менеджер может запустить до 8 исполнителей на одном worker node.

**Расчет ресурсов:**

Доступные ядра на worker node: 16

Ядра на одного исполнителя: 2

Доступная память на worker node: 64 ГБ

Память на одного исполнителя: 4 ГБ

**Максимальное количество исполнителей на одном worker node:**

Ядра: 16 / 2 = 8 исполнителей

Память: 64 ГБ / 4 ГБ = 16 исполнителей

Здесь ограничивающим фактором является количество ядер, поэтому на этом worker node можно запустить до 8 исполнителей.

**Ограничения и рекомендации**

Ресурсы узла: Количество исполнителей, которые могут быть запущены на одном worker node, ограничивается доступными ресурсами (ядрами и памятью) на этом узле.

Изоляция ресурсов: Рекомендуется выделять ресурсы таким образом, чтобы избежать конфликтов и обеспечить достаточную изоляцию для каждого исполнителя.

Конфигурация кластера: Кластерный менеджер автоматически распределяет исполнителей по узлам на основе конфигурации и доступных ресурсов.

**Заметка**

- Driver всегда один на весь кластер.

- Worker nodes выполняют задачи, но они получают команды от единого Driver.

- Executors на Worker nodes выполняют свои части задач, но Driver управляет процессом выполнения.

Driver является единственной точкой координации всего кластера. Он:

- Отправляет задачи Worker узлам.

- Следит за их выполнением.

- Агрегирует результаты.

Worker nodes получают задачи от Driver и выполняют их независимо друг от друга, но по заданиям, спущенным от Driver.

## Workflow приложения Spark

**Запуск автономного приложения и инициализация SparkContext**

```
from pyspark.sql import SparkSession

# Создание SparkSession
spark = SparkSession.builder.appName("Spark Workflow Example").getOrCreate()

# Получение SparkContext
sc = spark.sparkContext
```

-Приложение, написанное на Python, Scala, Java или R, запускается автономно.

-В этом приложении создается объект SparkSession (который включает SparkContext), инициализирующий необходимые компоненты для работы Spark.

-Только при наличии SparkContext приложение называется драйвером (Driver)

**Запрос ресурсов у менеджера кластеров**

-Драйвер запрашивает у кластерного менеджера (например, YARN, Mesos, Kubernetes или Spark Standalone) ресурсы для выполнения задачи.

-Драйвер сообщает кластерному менеджеру, сколько ресурсов ему требуется (например, количество исполнителей, объем памяти и процессорные ядра).

```
spark-submit \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 10 \
  --executor-memory 4g \
  --executor-cores 2 \
  path/to/your_script.py
```
                  
**Запуск исполнителей менеджером кластеров**

-Кластерный менеджер распределяет ресурсы и запускает процессы исполнителей (executors) на рабочих узлах (worker nodes).

-Каждый исполнитель запускает отдельный процесс, который будет выполнять задачи, переданные драйвером.

**Запуск кода Spark драйвером**

-Драйвер начинает выполнение кода Spark, который включает чтение данных, выполнение преобразований (transformations) и действий (actions).

-Драйвер разделяет задачи на этапы (stages) и назначает их исполнителям.

```
# Чтение данных
df = spark.read.csv("path/to/input.csv", header=True, inferSchema=True)

# Выполнение преобразований
df_filtered = df.filter(df["age"] > 30)
df_grouped = df_filtered.groupBy("city").count()
```
                  
**Выполнение заданий исполнителями и отправка результатов драйверу**

-Исполнители получают задания от драйвера и начинают их выполнение.

-Каждый исполнитель выполняет часть работы, такой как вычисления на данных и возвращает результаты драйверу.

```
# Действие, инициирующее выполнение заданий
result = df_grouped.collect()
```
                  
**Остановка SparkContext и освобождение ресурсов**

-После завершения выполнения всех задач, драйвер останавливает SparkContext.

-Исполнители закрываются и освобождают выделенные им ресурсы, возвращая их обратно в кластер.

```
# Остановка SparkSession и SparkContext
spark.stop()
```
